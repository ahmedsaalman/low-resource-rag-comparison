{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedsaalman/low-resource-rag-comparison/blob/main/Generator_Model_Dependencies_mBart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXyV4KQyCcmC"
      },
      "outputs": [],
      "source": [
        "print(\"Installing dependencies... (This takes ~1 minute)\")\n",
        "!pip install -q transformers datasets evaluate sentencepiece accelerate sacrebleu rouge_score nltk\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    MBartForConditionalGeneration,\n",
        "    MBart50TokenizerFast,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "# Setup device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "QQKWB48nIWTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "FILES = {\n",
        "    \"corpus\": \"urdu_covid_corpus_clean.jsonl\",\n",
        "    \"synthetic\": \"synthetic_qa_pairs.jsonl\",\n",
        "    \"eval\": \"eval_queries.jsonl\"\n",
        "}\n",
        "\n",
        "def clean_wiki_text(text):\n",
        "    if not text: return \"\"\n",
        "\n",
        "    text = re.sub(r'\\(\\s*ÿßŸÜ⁄Øÿ±€åÿ≤€å\\s*:.*?\\)', '', text)\n",
        "\n",
        "    text = re.sub(r'\\/.*\\/', '', text)\n",
        "\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def load_jsonl(filename):\n",
        "    data = []\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "print(f\"Loading and Cleaning Corpus from {FILES['corpus']}...\")\n",
        "corpus_data = load_jsonl(FILES['corpus'])\n",
        "\n",
        "corpus_lookup = {}\n",
        "for item in corpus_data:\n",
        "    cleaned_text = clean_wiki_text(item.get('text', ''))\n",
        "    if len(cleaned_text) > 20: # Skip empty/too short lines\n",
        "        corpus_lookup[item['id']] = cleaned_text\n",
        "\n",
        "print(f\"   ‚úÖ Corpus loaded. {len(corpus_lookup)} clean passages ready.\")\n",
        "\n",
        "print(f\"Loading Synthetic Data...\")\n",
        "synthetic_data = load_jsonl(FILES['synthetic'])\n",
        "training_pairs = []\n",
        "\n",
        "for item in synthetic_data:\n",
        "    p_id = item.get('positive_id') or (item.get('positive_ids')[0] if item.get('positive_ids') else None)\n",
        "\n",
        "    if p_id and p_id in corpus_lookup:\n",
        "        training_pairs.append({\n",
        "            \"question\": item['query'],\n",
        "            \"answer\": corpus_lookup[p_id]\n",
        "        })\n",
        "\n",
        "print(f\"   ‚úÖ Mapped {len(training_pairs)} Primary QA pairs.\")\n",
        "\n",
        "eval_raw = load_jsonl(FILES['eval'])\n",
        "eval_pairs = [{\"question\": i['query'], \"answer\": i['gold_answer']} for i in eval_raw]\n",
        "df_eval = pd.DataFrame(eval_pairs)"
      ],
      "metadata": {
        "id": "dbHyerRTIzpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "print(\"Performing Smart Data Augmentation...\")\n",
        "\n",
        "templates = [\n",
        "    \"{title} ⁄©€åÿß €Å€íÿü\",                         # What is {title}?\n",
        "    \"{title} ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ŸÖÿπŸÑŸàŸÖÿßÿ™\",             # Information about {title}\n",
        "    \"{title} ⁄©€å ÿ™ŸÅÿµ€åŸÑ ÿ®€åÿßŸÜ ⁄©ÿ±€å⁄∫\",              # Describe {title}\n",
        "    \"{title} ÿ≥€í ⁄©€åÿß ŸÖÿ±ÿßÿØ €Å€íÿü\",                 # What is meant by {title}?\n",
        "    \"ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ {title} ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿ®ÿ™ÿßÿ¶€å⁄∫\"      # Please tell me about {title}\n",
        "]\n",
        "\n",
        "augmented_samples = []\n",
        "target_count = 600\n",
        "\n",
        "shuffled_ids = list(corpus_lookup.keys())\n",
        "random.shuffle(shuffled_ids)\n",
        "\n",
        "for pid in shuffled_ids:\n",
        "    if len(augmented_samples) >= target_count: break\n",
        "\n",
        "    meta = next((item for item in corpus_data if item[\"id\"] == pid), None)\n",
        "    text = corpus_lookup[pid]\n",
        "\n",
        "    if meta and meta.get('title'):\n",
        "        title = meta['title']\n",
        "\n",
        "        if len(title) > 3:\n",
        "            tmpl = random.choice(templates)\n",
        "            question = tmpl.format(title=title)\n",
        "\n",
        "            augmented_samples.append({\n",
        "                \"question\": question,\n",
        "                \"answer\": text\n",
        "            })\n",
        "\n",
        "df_aug = pd.DataFrame(augmented_samples)\n",
        "df_train_primary = pd.DataFrame(training_pairs)\n",
        "\n",
        "df_total_train = pd.concat([df_train_primary, df_aug]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"üìä Final Training Set: {len(df_total_train)} samples\")\n",
        "print(f\"   - {len(df_train_primary)} Real QA pairs\")\n",
        "print(f\"   - {len(df_aug)} Augmented pairs\")\n",
        "\n",
        " train_dataset = Dataset.from_pandas(df_total_train)\n",
        "eval_dataset = Dataset.from_pandas(df_eval)"
      ],
      "metadata": {
        "id": "NlixdDVEI9Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Model Initialization\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "tokenizer.src_lang = \"ur_PK\"\n",
        "tokenizer.tgt_lang = \"ur_PK\"\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"ur_PK\"]\n",
        "\n",
        "# MEMORY HACK: Enable Gradient Checkpointing\n",
        "# This trades a little speed for MASSIVE memory savings\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"‚úÖ Model loaded.\")"
      ],
      "metadata": {
        "id": "On71wnpxJNra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Preprocessing & Config\n",
        "\n",
        "max_input = 128\n",
        "max_target = 256\n",
        "\n",
        "def preprocess_fn(examples):\n",
        "    inputs = [f\"ÿ≥ŸàÿßŸÑ: {q}\" for q in examples[\"question\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"answer\"], max_length=max_target, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "print(\"Tokenizing data...\")\n",
        "tokenized_train = train_dataset.map(preprocess_fn, batched=True)\n",
        "tokenized_eval = eval_dataset.map(preprocess_fn, batched=True)\n",
        "\n",
        " args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./mbart-covid-urdu\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4, # Effective batch = 16\n",
        "    num_train_epochs=8,            # Increased epochs for small data\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    logging_steps=25,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "print(\"‚úÖ Configuration ready.\")"
      ],
      "metadata": {
        "id": "VOHoQHbpJPtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Training Loop\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting Training...\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ Training finished.\")"
      ],
      "metadata": {
        "id": "a0kCCcT4KXgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Comprehensive Evaluation\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "metric_bleu = evaluate.load(\"sacrebleu\")\n",
        "metric_rouge = evaluate.load(\"rouge\")\n",
        "metric_meteor = evaluate.load(\"meteor\")\n",
        "metric_chrf = evaluate.load(\"chrf\")\n",
        "\n",
        "def evaluate_model():\n",
        "    print(\"‚è≥ Generating predictions for Eval set... (This might take a minute)\")\n",
        "\n",
        "    results = trainer.predict(tokenized_eval)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(results.predictions, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(results.label_ids != -100, results.label_ids, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "\n",
        "\n",
        "    # A. BLEU (Requires list of lists for references)\n",
        "    # Good for exact phrase matching\n",
        "    bleu_refs = [[l] for l in decoded_labels]\n",
        "    score_bleu = metric_bleu.compute(predictions=decoded_preds, references=bleu_refs)\n",
        "\n",
        "    # B. ROUGE (Recall - Did we capture the main points?)\n",
        "    # ROUGE-L is best for sentence-level structure\n",
        "    score_rouge = metric_rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # C. METEOR (Semantic matching/Synonyms)\n",
        "    score_meteor = metric_meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # D. chrF (Character overlap - BEST for Urdu morphology)\n",
        "    score_chrf = metric_chrf.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # --- DISPLAY RESULTS ---\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üìä MODEL PERFORMANCE REPORT\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"üîπ BLEU Score:   {score_bleu['score']:.2f}  (Higher is better, >15 is decent for Urdu)\")\n",
        "    print(f\"üîπ chrF Score:   {score_chrf['score']:.2f}  (Best metric for Urdu, aim for >40)\")\n",
        "    print(f\"üîπ ROUGE-L:      {score_rouge['rougeL'] * 100:.2f}  (Sentence structure match)\")\n",
        "    print(f\"üîπ METEOR:       {score_meteor['meteor'] * 100:.2f}  (Synonym/Meaning match)\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    print(\"\\n--- üîç Qualitative Analysis (First 3 Samples) ---\")\n",
        "    for i in range(min(3, len(df_eval))):\n",
        "        print(f\" Question: {df_eval.iloc[i]['question']}\")\n",
        "        print(f\" Gold Ans: {df_eval.iloc[i]['answer']}\")\n",
        "        print(f\"Model Ans: {decoded_preds[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "evaluate_model()"
      ],
      "metadata": {
        "id": "cLtQ_TdlKcCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Interactive Test (Improved Generation Parameters)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import torch\n",
        "\n",
        "print(\"üí¨ Urdu COVID QA Interface: \")\n",
        "model.eval()\n",
        "\n",
        "def ask_mbart(question):\n",
        "    input_str = f\"ÿ≥ŸàÿßŸÑ: {question}\"\n",
        "    inputs = tokenizer(input_str, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=150,\n",
        "            min_length=10,\n",
        "            num_beams=5,\n",
        "\n",
        "            # --- CRITICAL FIXES FOR REPETITION ---\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=2,\n",
        "\n",
        "            # --- FIXES FOR CREATIVITY/LOGIC ---\n",
        "            do_sample=True,           # Allows \"temperature\" to work\n",
        "            temperature=0.6,          # Lower (0.6) = More factual/Focused. Higher (1.0) = Creative/Random\n",
        "            top_p=0.9                 # Nucleus sampling (Keeps top 90% probable words)\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# UI Setup\n",
        "txt_in = widgets.Text(placeholder='€å€Åÿß⁄∫ ÿ≥ŸàÿßŸÑ ŸÑ⁄©⁄æ€å⁄∫...', description='Question:', layout=widgets.Layout(width='80%'))\n",
        "out_area = widgets.Output()\n",
        "\n",
        "def on_change(change):\n",
        "    with out_area:\n",
        "        out_area.clear_output()\n",
        "        if change.new:\n",
        "            print(f\"Thinking... (Model is analyzing '{change.new}')\")\n",
        "            ans = ask_mbart(change.new)\n",
        "            print(f\"\\nüí° ÿ¨Ÿàÿßÿ®:\\n{ans}\")\n",
        "\n",
        "txt_in.observe(on_change, names='value')\n",
        "display(txt_in, out_area)"
      ],
      "metadata": {
        "id": "_8f9ZZWfKeVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Save Model\n",
        "output_path = \"./fine_tuned_mbart_urdu\"\n",
        "model.save_pretrained(output_path)\n",
        "tokenizer.save_pretrained(output_path)\n",
        "\n",
        "print(f\"Model saved to {output_path}\")\n",
        "\n",
        " !zip -r mbart_urdu_covid.zip {output_path}\n",
        "from google.colab import files\n",
        "try:\n",
        "    files.download('mbart_urdu_covid.zip')\n",
        "except:\n",
        "    print(\"Download failed automatically. Please check the file browser on the left.\")"
      ],
      "metadata": {
        "id": "81FADzf2KgWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp mbart_urdu_covid.zip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "LnwOIPH7scyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATION ARGUMENTS (Fixes the \"rhinitis\" loop)\n",
        "generation_config=GenerationConfig(\n",
        "    max_new_tokens=128,\n",
        "    repetition_penalty=1.2,   # Penalizes repeating words\n",
        "    no_repeat_ngram_size=3,   # Prevents 3-word phrase repeats\n",
        ")"
      ],
      "metadata": {
        "id": "i4dGoPLjh5Dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}