{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedsaalman/low-resource-rag-comparison/blob/main/NLP_Project_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgyLTdh72rhW"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install required libraries (run this cell first and one by one all required libraries will be installed)\n",
        "# - transformers: model + generation\n",
        "# - sentence-transformers: dense embeddings / fine-tuning helpers\n",
        "# - faiss-cpu (or faiss-gpu if GPU available)\n",
        "# - rank_bm25: BM25 baseline\n",
        "# - datasets: convenient JSONL loading\n",
        "# - evaluate / sacrebleu: BLEU/chrF metrics\n",
        "# - tqdm: progress bars\n",
        "# - accelerate (optional) for distributed/faster training\n",
        "!pip install -q transformers sentence-transformers faiss-cpu rank_bm25 datasets evaluate sacrebleu tqdm accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM4J5YcY3WU5"
      },
      "outputs": [],
      "source": [
        "!pip list # Optional to run this cell: To check which of the libraries/packages have been installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_VNjxHI3YMd"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Imports and GPU check: Run this cell after the first cell\n",
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
        "import sentence_transformers # Import the package itself to access __version__\n",
        "\n",
        "# FAISS and BM25\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Datasets and metrics\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "import sacrebleu\n",
        "\n",
        "# Print versions and GPU info\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"torch not available:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViZ8XWh530oS"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "DATA_DIR = Path(\"drive/MyDrive/data\")  # change if files are elsewhere\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "with open(DATA_DIR / \"urdu_covid_passages.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Use split(None, 1) to split on the first occurrence of any whitespace\n",
        "        # This handles cases where the delimiter might be spaces instead of a tab.\n",
        "        if line.strip(): # Ensure line is not empty after stripping whitespace\n",
        "            parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "            if len(parts) == 2:\n",
        "                pid, text = parts\n",
        "                passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "            else:\n",
        "                print(f\"Skipping malformed line in urdu_covid_passages.tsv: {line.strip()}\")\n",
        "\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "print(\"Loaded:\", len(corpus_clean), \"corpus_clean; \", len(passages_min), \"passages_min; \", len(eval_queries), \"eval queries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlEd0kMX4uGm"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Validate IDs referenced in eval/synthetic/hard_negatives exist in corpus\n",
        "# Run this after Cell 3.\n",
        "passage_ids = {p[\"id\"] for p in passages_min}\n",
        "missing = []\n",
        "for q in eval_queries:\n",
        "    for pid in q.get(\"positive_ids\", []):\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"eval\", q[\"query_id\"], pid))\n",
        "for s in synthetic_pairs:\n",
        "    if s[\"positive_id\"] not in passage_ids:\n",
        "        missing.append((\"synthetic\", s[\"synthetic_id\"], s[\"positive_id\"]))\n",
        "for h in hard_negatives:\n",
        "    for pid in h[\"hard_negatives\"]:\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"hardneg\", h[\"query_id\"], pid))\n",
        "print(\"Missing references (should be zero):\", len(missing))\n",
        "if missing:\n",
        "    print(missing[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJkfVV8T_GR5"
      },
      "outputs": [],
      "source": [
        "# Cell 5 (Run after Cell 4): BM25 baseline index (tokenize with simple whitespace; for Urdu this is OK as baseline)\n",
        "# We'll store tokenized corpus and BM25 object for retrieval.\n",
        "from nltk.tokenize import word_tokenize\n",
        "# If nltk not installed, use simple split\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Added to resolve LookupError for 'punkt_tab'\n",
        "    tokenizer = lambda s: word_tokenize(s)\n",
        "except Exception:\n",
        "    tokenizer = lambda s: s.split()\n",
        "\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "corpus_ids = [p[\"id\"] for p in passages_min]\n",
        "tokenized_corpus = [tokenizer(t) for t in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Example retrieval function\n",
        "def bm25_retrieve(query, k=5):\n",
        "    q_tokens = tokenizer(query)\n",
        "    scores = bm25.get_scores(q_tokens)\n",
        "    topk = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in topk]\n",
        "\n",
        "# Quick test\n",
        "print(\"BM25 top-3 for sample:\", bm25_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5b: BM25-only retriever evaluation tool (run after Cell 5)\n",
        "# Purpose: standalone evaluation harness for the independent BM25 retriever (bm25_retrieve)\n",
        "# Metrics included (applicable to a retriever-only evaluation):\n",
        "#   - Recall@1, Recall@5\n",
        "#   - MRR (Mean Reciprocal Rank)\n",
        "#   - Precision@k (k=1,5)\n",
        "#   - Average / median retrieval latency\n",
        "#   - Optional: match by gold_passage_id or by substring match of gold_answer\n",
        "# Output:\n",
        "#   - Per-query JSONL saved to bm25_eval_results.jsonl\n",
        "#   - Printed summary with all metrics\n",
        "#\n",
        "# Requirements (must be available in the session):\n",
        "#   - bm25_retrieve(query, k) -> list of (passage_id, passage_text, score)\n",
        "#   - eval_queries: list of dicts with at least a query field and optionally:\n",
        "#       * \"question\" or \"query\" or \"q\"  (the query text)\n",
        "#       * \"gold_passage_id\" (optional) OR \"answer\"/\"gold\" (gold text to match)\n",
        "#\n",
        "# Usage:\n",
        "#   - Run this cell after you build the BM25 index (Cell 5).\n",
        "#   - Optionally pass a different eval list or k values to evaluate subsets.\n",
        "\n",
        "# Use this evaluator if your eval_queries items contain \"positive_ids\" and \"gold_answer\"\n",
        "import json, time, re, statistics\n",
        "from typing import List, Dict\n",
        "\n",
        "OUT_JSONL = \"bm25_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def get_query_text(item: Dict) -> str:\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_bm25_with_positive_ids(eval_items: List[Dict],\n",
        "                                    out_jsonl: str = OUT_JSONL,\n",
        "                                    k: int = DEFAULT_K,\n",
        "                                    recall_ks = RECALL_KS,\n",
        "                                    precision_ks = PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        # normalize to list of strings\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or item.get(\"answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = bm25_retrieve(q, k=k)   # (id, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] bm25_retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k (multiple positives supported)\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            # precision@k = (# positives in top-k) / k\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [t[:300] for t in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run it\n",
        "if 'eval_queries' not in globals():\n",
        "    # try to load from file if not in memory\n",
        "    eval_queries = []\n",
        "    with open(\"eval_queries.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            eval_queries.append(json.loads(line))\n",
        "\n",
        "summary, records = evaluate_bm25_with_positive_ids(eval_queries, out_jsonl=OUT_JSONL, k=DEFAULT_K)\n",
        "print(\"BM25 retrieval evaluation summary:\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# show a few examples where retrieval missed positives\n",
        "misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "print(f\"\\nTotal misses: {len(misses)} / {len(records)}. Showing up to 5 misses:\")\n",
        "for r in misses[:5]:\n",
        "    print(\"Query id:\", r.get(\"query_id\"), \"Query:\", r[\"query\"][:80])\n",
        "    print(\" Positives:\", r[\"positive_ids\"])\n",
        "    print(\" Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "JqDyRpA-924o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw6ei6j6_P97"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Dense embeddings with a multilingual model (use a compact model for Colab)\n",
        "# We use a multilingual SBERT model that supports Urdu reasonably (e.g., 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embed_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "# Compute embeddings for passages_min (batching)\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS index (cosine similarity via normalized vectors)\n",
        "d = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)  # inner product\n",
        "# normalize embeddings for cosine\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# Map index positions to ids\n",
        "# retrieval function\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "# Quick test\n",
        "print(\"Dense top-3:\", dense_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6b: Evaluation of dense retriever (run after Cell 6)\n",
        "# Purpose: measure Recall@1, Recall@5, MRR, Precision@k, latency for dense_retrieve\n",
        "# Uses eval_queries with \"positive_ids\" and \"gold_answer\" fields\n",
        "\n",
        "import json, time, re, statistics\n",
        "\n",
        "OUT_JSONL_DENSE = \"dense_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s):\n",
        "    if s is None: return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "\n",
        "def get_query_text(item):\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_dense(eval_items, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K,\n",
        "                   recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies, rr_list = [], []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        pos_ids = item.get(\"positive_ids\") or []\n",
        "        if isinstance(pos_ids, str): pos_ids = [pos_ids]\n",
        "        pos_ids = [str(x) for x in pos_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        hits = dense_retrieve(q, k=k)  # (id, text, score)\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in pos_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in pos_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in pos_ids)\n",
        "            precision_sums[pk] += num_pos_in_topk / pk\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": pos_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [txt[:300] for txt in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": sum(rr_list)/n,\n",
        "        **{f\"Recall@{rk}\": recall_counts[rk]/n for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_sums[pk]/n for pk in precision_ks},\n",
        "        \"latency_mean_s\": statistics.mean(latencies) if latencies else 0.0,\n",
        "        \"latency_median_s\": statistics.median(latencies) if latencies else 0.0\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run evaluation\n",
        "print(\"[dense_eval] Running dense retriever evaluation...\")\n",
        "summary_dense, records_dense = evaluate_dense(eval_queries, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K)\n",
        "print(\"\\nDense retriever evaluation summary:\")\n",
        "for k,v in summary_dense.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Show a few examples\n",
        "print(\"\\nExamples (first 5):\")\n",
        "for r in records_dense[:5]:\n",
        "    print(\" - Query:\", r[\"query\"][:80])\n",
        "    print(\"   Retrieved ids:\", r[\"retrieved_ids\"][:6])\n",
        "    print(\"   Reciprocal rank:\", r[\"reciprocal_rank\"], \"Latency(s):\", round(r[\"latency\"], 4))\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "0kE-KPlxENAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Prepare InputExamples for sentence-transformers fine-tuning\n",
        "# Now with an 80/20 train/validation split\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "import random\n",
        "\n",
        "pid2text = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "examples = []\n",
        "for s in synthetic_pairs:\n",
        "    q = s[\"query\"]\n",
        "    pos = pid2text.get(s[\"positive_id\"])\n",
        "    neg = None\n",
        "    # Find hard negatives if available\n",
        "    hn = next((h for h in hard_negatives if h[\"query_id\"] == s.get(\"synthetic_id\", s.get(\"query_id\"))), None)\n",
        "    if hn:\n",
        "        for nid in hn[\"hard_negatives\"]:\n",
        "            if nid != s[\"positive_id\"]:\n",
        "                neg = pid2text.get(nid)\n",
        "                break\n",
        "    if neg is None:\n",
        "        # fallback: random negative\n",
        "        neg_id = random.choice([pid for pid in corpus_ids if pid != s[\"positive_id\"]])\n",
        "        neg = pid2text[neg_id]\n",
        "    if pos and neg:\n",
        "        examples.append(InputExample(texts=[q, pos, neg]))\n",
        "\n",
        "print(\"Prepared\", len(examples), \"triplet examples.\")\n",
        "\n",
        "# --- Split into train/validation (80/20) ---\n",
        "random.shuffle(examples)\n",
        "split_idx = int(0.8 * len(examples))\n",
        "train_examples = examples[:split_idx]\n",
        "val_examples = examples[split_idx:]\n",
        "\n",
        "print(\"Train examples:\", len(train_examples))\n",
        "print(\"Validation examples:\", len(val_examples))\n"
      ],
      "metadata": {
        "id": "SjfNbXQ7Jboz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 (use in-memory model; do NOT reload): Fine-tune SBERT with triplet loss and IR validation on passages_min\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, losses, evaluation\n",
        "import faiss\n",
        "\n",
        "# Sanity checks\n",
        "assert isinstance(train_examples, list) and len(train_examples) > 0, \"train_examples must be a non-empty list\"\n",
        "assert 'passages_min' in globals(), \"passages_min must be loaded\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "\n",
        "# Build validation split against real corpus & labels\n",
        "eval_val = eval_queries_val if 'eval_queries_val' in globals() else eval_queries[int(0.8*len(eval_queries)):]\n",
        "val_queries_dict = {it[\"query_id\"]: it[\"query\"] for it in eval_val}\n",
        "val_relevant_dict = {it[\"query_id\"]: set(it[\"positive_ids\"]) for it in eval_val}\n",
        "val_corpus_dict = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "# Warn if labels reference missing ids\n",
        "missing = []\n",
        "for qid, rels in val_relevant_dict.items():\n",
        "    for pid in rels:\n",
        "        if pid not in val_corpus_dict:\n",
        "            missing.append((qid, pid))\n",
        "if missing:\n",
        "    print(f\"Warning: {len(missing)} relevant ids not found in corpus. Example:\", missing[:3])\n",
        "\n",
        "# Construct evaluator (defaults to cosine similarity)\n",
        "retrieval_evaluator = evaluation.InformationRetrievalEvaluator(\n",
        "    queries=val_queries_dict,\n",
        "    corpus=val_corpus_dict,\n",
        "    relevant_docs=val_relevant_dict,\n",
        "    name=\"val_ir_passages\"\n",
        ")\n",
        "\n",
        "# Start from baseline multilingual MiniLM\n",
        "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\").to(\"cuda\")\n",
        "\n",
        "# Triplet loss with conservative settings\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.TripletLoss(\n",
        "    model=embedder,\n",
        "    distance_metric=losses.TripletDistanceMetric.COSINE,\n",
        "    triplet_margin=0.3\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
        "optimizer_params = {'lr': 2e-5}\n",
        "\n",
        "# Train with IR evaluator\n",
        "embedder.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=retrieval_evaluator,\n",
        "    epochs=num_epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    optimizer_params=optimizer_params,\n",
        "    show_progress_bar=True,\n",
        "    output_path=\"fine_tuned_sbert_urdu_passages\"\n",
        ")\n",
        "\n",
        "print(\"✅ Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\")\n"
      ],
      "metadata": {
        "id": "R8-GRissKrda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8b: Rebuild FAISS with fine-tuned in-memory embedder\n",
        "\n",
        "import faiss\n",
        "\n",
        "# Use the current in-memory fine-tuned 'embedder'\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index = faiss.IndexFlatIP(passage_embeddings.shape[1])\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "print(\"✅ FAISS rebuilt with fine-tuned embeddings (in-memory model).\")\n"
      ],
      "metadata": {
        "id": "x9LZDA7kMWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We skipped Cell 7 and 8 which were for fine tuning our dense-retriever but required GPU. Then, after not obtaining proper\n",
        "# results, we enabled GPU on collab and have now run cell 7 and 8, then cell 9, then cell 8b as well.\n",
        "# Cell 9 (final): Retriever wrapper with true fusion modes (non-destructive)\n",
        "# - Creates bm25_new only if not present\n",
        "# - Supports modes: 'bm25', 'dense', 'hybrid_interleave' (legacy), 'hybrid_score', 'hybrid_rrf'\n",
        "# - Returns list of (pid, text, score) tuples\n",
        "# - Does NOT rebuild or overwrite dense/FAISS objects\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ---------- Config ----------\n",
        "# Tune these later on a small validation set\n",
        "DEFAULT_RETRIEVE_POOL = 50\n",
        "SCORE_FUSION_ALPHA = 0.6   # alpha in [0,1] for score fusion: alpha * dense + (1-alpha) * bm25\n",
        "RRF_K = 60                 # reciprocal rank fusion constant\n",
        "\n",
        "# ---------- Sanity checks for canonical corpus ----------\n",
        "assert 'passages_min' in globals() and isinstance(passages_min, list) and len(passages_min) > 0, \"passages_min must be loaded\"\n",
        "assert 'pid2text' in globals() and isinstance(pid2text, dict) and len(pid2text) > 0, \"pid2text must be available\"\n",
        "assert 'dense_retrieve' in globals(), \"dense_retrieve wrapper must be defined (fine-tuned dense retriever)\"\n",
        "\n",
        "# ---------- Build or reuse BM25 index (non-destructive) ----------\n",
        "try:\n",
        "    # If bm25_new already exists from a previous run, reuse it\n",
        "    bm25_new  # noqa: F821\n",
        "except Exception:\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank_bm25\"], check=True)\n",
        "        from rank_bm25 import BM25Okapi\n",
        "\n",
        "    # Build tokenized corpus from passages_min (light normalization)\n",
        "    def _normalize_for_bm25(s: str) -> str:\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = s.replace(\"\\u200c\", \" \")  # zero-width non-joiner\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s.lower()\n",
        "\n",
        "    bm25_new_ids = [p[\"id\"] for p in passages_min]\n",
        "    bm25_new_texts = [p[\"text\"] for p in passages_min]\n",
        "    bm25_new_tokenized = [_normalize_for_bm25(t).split() for t in bm25_new_texts]\n",
        "    bm25_new = BM25Okapi(bm25_new_tokenized)\n",
        "\n",
        "# Safe wrapper for BM25 that returns (pid, text, score)\n",
        "def bm25_new_retrieve(query: str, k: int = 5):\n",
        "    q_tok = _normalize_for_bm25(query).split()\n",
        "    scores = bm25_new.get_scores(q_tok)\n",
        "    top_idx = np.argsort(scores)[::-1][:k]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        i = int(i)\n",
        "        pid = bm25_new_ids[i]\n",
        "        text = bm25_new_texts[i]\n",
        "        score = float(scores[i])\n",
        "        results.append((pid, text, score))\n",
        "    return results\n",
        "\n",
        "# ---------- Fusion utilities ----------\n",
        "def normalize_scores(score_map):\n",
        "    \"\"\"Min-max normalize a dict of scores to [0,1].\"\"\"\n",
        "    if not score_map:\n",
        "        return {}\n",
        "    vals = list(score_map.values())\n",
        "    lo, hi = min(vals), max(vals)\n",
        "    if hi == lo:\n",
        "        return {k: 1.0 for k in score_map}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in score_map.items()}\n",
        "\n",
        "def rrf_rank(dense_list, bm25_list, k_rrf=RRF_K):\n",
        "    \"\"\"Reciprocal Rank Fusion: returns sorted list of pids by RRF score.\"\"\"\n",
        "    score = {}\n",
        "    for rank, (pid, _, _) in enumerate(dense_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    for rank, (pid, _, _) in enumerate(bm25_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    sorted_pids = sorted(score.keys(), key=lambda p: score[p], reverse=True)\n",
        "    return sorted_pids, score\n",
        "\n",
        "# ---------- Metadata filter helper (unchanged semantics) ----------\n",
        "# If you have meta_map from corpus_clean, it will be used; otherwise fallback to passages_min metadata\n",
        "if 'corpus_clean' in globals():\n",
        "    meta_map = {p[\"id\"]: p for p in corpus_clean}\n",
        "else:\n",
        "    meta_map = {p[\"id\"]: p for p in passages_min}\n",
        "\n",
        "def filter_by_metadata(candidate_ids, min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    out = []\n",
        "    for pid in candidate_ids:\n",
        "        m = meta_map.get(pid, {})\n",
        "        ok = True\n",
        "        if min_date or max_date:\n",
        "            dt = None\n",
        "            if \"retrieved_at\" in m:\n",
        "                try:\n",
        "                    dt = datetime.fromisoformat(m[\"retrieved_at\"])\n",
        "                except Exception:\n",
        "                    dt = None\n",
        "            if dt:\n",
        "                if min_date and dt < min_date: ok = False\n",
        "                if max_date and dt > max_date: ok = False\n",
        "        if allowed_sources and m.get(\"source\") not in allowed_sources:\n",
        "            ok = False\n",
        "        if exclude_time_sensitive is not None and m.get(\"time_sensitive\") == exclude_time_sensitive:\n",
        "            ok = False\n",
        "        if ok:\n",
        "            out.append(pid)\n",
        "    return out\n",
        "\n",
        "# ---------- Main retrieve wrapper with fusion modes ----------\n",
        "def retrieve(query: str, k: int = 5, mode: str = \"hybrid_score\", min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    \"\"\"\n",
        "    retrieve(query, k, mode)\n",
        "    Modes:\n",
        "      - 'bm25' : BM25-only (bm25_new_retrieve)\n",
        "      - 'dense' : dense-only (dense_retrieve)\n",
        "      - 'hybrid_interleave' : legacy interleave (dense first, then bm25)\n",
        "      - 'hybrid_score' : score fusion (normalized dense + bm25)\n",
        "      - 'hybrid_rrf' : reciprocal rank fusion (RRF)\n",
        "    Returns: list of (pid, text, score)\n",
        "    \"\"\"\n",
        "    # Get candidate pools (pool size configurable)\n",
        "    pool = max(DEFAULT_RETRIEVE_POOL, k)\n",
        "    dense_hits = dense_retrieve(query, k=pool)   # expected (pid, text, score)\n",
        "    bm25_hits = bm25_new_retrieve(query, k=pool) # (pid, text, score)\n",
        "\n",
        "    # Mode-specific behavior\n",
        "    if mode == \"bm25\":\n",
        "        results = bm25_hits[:k]\n",
        "    elif mode == \"dense\":\n",
        "        results = dense_hits[:k]\n",
        "    elif mode == \"hybrid_interleave\":\n",
        "        # preserve dense-first interleaving (legacy behavior)\n",
        "        seen = set()\n",
        "        cands = []\n",
        "        for lst in (dense_hits, bm25_hits):\n",
        "            for pid, text, score in lst:\n",
        "                if pid not in seen:\n",
        "                    seen.add(pid)\n",
        "                    cands.append((pid, text, float(score)))\n",
        "        results = cands[:k]\n",
        "    elif mode == \"hybrid_score\":\n",
        "        # Score fusion: normalize and combine\n",
        "        dense_scores = {pid: sc for pid, _, sc in dense_hits}\n",
        "        bm25_scores = {pid: sc for pid, _, sc in bm25_hits}\n",
        "        d_norm = normalize_scores(dense_scores)\n",
        "        b_norm = normalize_scores(bm25_scores)\n",
        "        alpha = SCORE_FUSION_ALPHA\n",
        "        combined = {}\n",
        "        for pid in set(list(d_norm.keys()) + list(b_norm.keys())):\n",
        "            combined[pid] = alpha * d_norm.get(pid, 0.0) + (1 - alpha) * b_norm.get(pid, 0.0)\n",
        "        # sort by combined score\n",
        "        sorted_pids = sorted(combined.keys(), key=lambda p: combined[p], reverse=True)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(combined[pid])))\n",
        "    elif mode == \"hybrid_rrf\":\n",
        "        sorted_pids, score_map = rrf_rank(dense_hits, bm25_hits, k_rrf=RRF_K)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(score_map.get(pid, 0.0))))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieve mode: {mode}\")\n",
        "\n",
        "    # Apply metadata filters if requested (filter by pid only)\n",
        "    if any([min_date, max_date, allowed_sources, exclude_time_sensitive is not None]):\n",
        "        filtered_ids = filter_by_metadata([pid for pid,_,_ in results], min_date, max_date, allowed_sources, exclude_time_sensitive)\n",
        "        results = [(pid, pid2text.get(pid, \"\"), score) for pid,_,score in results if pid in filtered_ids]\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------- Quick sample test (safe) ----------\n",
        "q = eval_queries[0][\"query\"] if 'eval_queries' in globals() and len(eval_queries)>0 else \"کووڈ-19 کی عام علامات کیا ہیں؟\"\n",
        "print(\"Sample dense top-5 ids:\", [r[0] for r in dense_retrieve(q, k=5)])\n",
        "print(\"Sample bm25_new top-5 ids:\", [r[0] for r in bm25_new_retrieve(q, k=5)])\n",
        "print(\"Sample hybrid_score top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_score')])\n",
        "print(\"Sample hybrid_rrf top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_rrf')])\n"
      ],
      "metadata": {
        "id": "GyXpE1l7DFn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9a: Run after above cell 9. Cell 9 creates B2M5 + Dense hybrid and below\n",
        "# cell evaluates its performance:\n",
        "# Cell 9c: Validation diagnostics — Recall@1, Recall@5, MRR, Precision@k for retrievers\n",
        "# - Works with any mode supported by your Cell 9 wrapper: 'bm25', 'dense', 'hybrid_interleave', 'hybrid_score', 'hybrid_rrf'\n",
        "# - Calls retrieve(...) and computes retrieval metrics\n",
        "# - Outputs summary metrics and a few examples of misses\n",
        "\n",
        "import time, statistics\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def evaluate_retriever(eval_items, mode=\"hybrid_score\", k=DEFAULT_K,\n",
        "                       recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in tqdm(eval_items, desc=f\"Evaluating {mode} retriever\"):\n",
        "        total += 1\n",
        "        q = item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = retrieve(q, k=k, mode=mode)   # (pid, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [r[0] for r in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# ---------- Run evaluation ----------\n",
        "# Use eval_queries_val if defined, else fall back to eval_queries\n",
        "eval_items = eval_queries_val if 'eval_queries_val' in globals() else eval_queries\n",
        "\n",
        "# Evaluate all retriever modes\n",
        "modes = [\"bm25\", \"dense\", \"hybrid_interleave\", \"hybrid_score\", \"hybrid_rrf\"]\n",
        "results = {}\n",
        "for m in modes:\n",
        "    summary, records = evaluate_retriever(eval_items, mode=m, k=DEFAULT_K)\n",
        "    results[m] = summary\n",
        "    print(f\"\\n{m} retriever evaluation summary:\")\n",
        "    for k,v in summary.items():\n",
        "        print(f\"  {k}: {v:.3f}\" if isinstance(v,float) else f\"  {k}: {v}\")\n",
        "\n",
        "    # Show a few misses\n",
        "    misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "    print(f\"  Total misses: {len(misses)} / {len(records)}. Showing up to 3 misses:\")\n",
        "    for r in misses[:3]:\n",
        "        print(\"   Query:\", r[\"query\"][:80])\n",
        "        print(\"    Positives:\", r[\"positive_ids\"])\n",
        "        print(\"    Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n"
      ],
      "metadata": {
        "id": "eDG0oLDaswu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6c: Report top-3 from current fine-tuned dense retriever (no re-init)\n",
        "print(\"Dense top-3 (fine-tuned):\", dense_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ],
      "metadata": {
        "id": "pE9Ao8BDO9ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9b (fixed): Validation diagnostics — Recall@1 and Recall@5 on validation examples for Dense Model only\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Map passage text back to IDs\n",
        "text2pid = {p[\"text\"]: p[\"id\"] for p in passages_min}\n",
        "\n",
        "# Build validation pairs (query, gold passage ID)\n",
        "val_pairs = []\n",
        "for ex in val_examples[:200]:  # cap to 200 for speed; remove cap for full set\n",
        "    q = ex.texts[0]\n",
        "    pos_text = ex.texts[1]\n",
        "    gold_pid = text2pid.get(pos_text)\n",
        "    if gold_pid:\n",
        "        val_pairs.append((q, gold_pid))\n",
        "\n",
        "def recall_at_k_pairs(pairs, k=1, mode=\"dense\"):\n",
        "    hits = 0\n",
        "    for q, gold_pid in tqdm(pairs):\n",
        "        # retrieve(...) returns (pid, text, score) tuples; take the first element as pid\n",
        "        retrieved = [r[0] for r in retrieve(q, k=k, mode=mode)]\n",
        "        if gold_pid in retrieved:\n",
        "            hits += 1\n",
        "    return hits / len(pairs) if pairs else 0.0\n",
        "\n",
        "r1 = recall_at_k_pairs(val_pairs, k=1, mode=\"dense\")\n",
        "r5 = recall_at_k_pairs(val_pairs, k=5, mode=\"dense\")\n",
        "print(f\"Validation Recall@1 (dense): {r1:.3f}\")\n",
        "print(f\"Validation Recall@5 (dense): {r5:.3f}\")\n"
      ],
      "metadata": {
        "id": "a0mlSvToMGTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional Cell:\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedder = SentenceTransformer(\"fine_tuned_sbert_urdu\").to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "68qlwuw-Pxff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional cell - only need to be run if generator model not present as unzipped in drive\n",
        "\n",
        "# 1. Mount Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Paths — update to your actual Drive path\n",
        "DRIVE_ZIP = \"/content/drive/MyDrive/mbart_urdu_covid.zip\"   # path to the zip in Drive\n",
        "UNZIP_TARGET = \"/content/drive/MyDrive/fine_tuned_mbart_urdu\"  # folder to create in Drive\n",
        "\n",
        "# 3. Make sure target folder does not already exist (optional safety)\n",
        "import os, shutil\n",
        "if os.path.exists(UNZIP_TARGET):\n",
        "    print(\"Warning: target folder already exists:\", UNZIP_TARGET)\n",
        "else:\n",
        "    # 4. Copy zip into runtime (optional) and unzip directly into Drive\n",
        "    !unzip -q \"{DRIVE_ZIP}\" -d \"{UNZIP_TARGET}\"\n",
        "    print(\"Unzipped to:\", UNZIP_TARGET)\n",
        "\n",
        "# 5. List files to confirm\n",
        "for root, dirs, files in os.walk(UNZIP_TARGET):\n",
        "    print(root)\n",
        "    print(\"  dirs:\", dirs)\n",
        "    print(\"  files:\", files[:10])\n",
        "    break\n"
      ],
      "metadata": {
        "id": "LY9ZXMpRXjQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGSNY-smGM7j"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Load fine-tuned MBART generator from Drive (or local path)\n",
        "# - Paste this cell into your RAG notebook before the RAG inference cell.\n",
        "# - Assumes you have mounted Google Drive earlier in the notebook:\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive')\n",
        "# - Replace `MBART_ARCHIVE_PATH` with the folder path that contains the saved model\n",
        "#   (the folder should contain config.json, model.safetensors, tokenizer files, etc.)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import MBartForConditionalGeneration, MBart50Tokenizer # Changed to MBart50Tokenizer\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "# Path to the unzipped fine-tuned model folder (update to your Drive path)\n",
        "# Corrected path to the nested directory where the model files are actually located\n",
        "MBART_ARCHIVE_PATH = \"/content/drive/MyDrive/fine_tuned_mbart_urdu/fine_tuned_mbart_urdu\"  # <- Corrected path\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------- Load tokenizer and model ----------\n",
        "print(f\"Loading MBART generator from: {MBART_ARCHIVE_PATH}\")\n",
        "tokenizer = MBart50Tokenizer.from_pretrained(MBART_ARCHIVE_PATH) # Changed to MBart50Tokenizer\n",
        "# Ensure tokenizer uses Urdu language code for both source and target\n",
        "tokenizer.src_lang = \"ur_PK\"\n",
        "tokenizer.tgt_lang = \"ur_PK\"\n",
        "\n",
        "gen_model = MBartForConditionalGeneration.from_pretrained(MBART_ARCHIVE_PATH).to(device)\n",
        "gen_model.eval()\n",
        "\n",
        "# Optional: set forced BOS token to ensure Urdu generation if not already set\n",
        "if \"ur_PK\" in tokenizer.lang_code_to_id:\n",
        "    gen_model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"ur_PK\"]\n",
        "\n",
        "print(f\"✅ MBART loaded on {device}. Tokenizer and model ready for generation.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10b (memory-optimized): Fine-tune MBART generator for RAG-style inputs\n",
        "# - Uses eval_queries.jsonl as training data\n",
        "# - Saves fine-tuned model to OUTPUT_DIR on Drive\n",
        "# - Adjusted for Colab GPU memory limits\n",
        "\n",
        "import os, json, torch, gc\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "# Clear GPU cache before training\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ---------- Config ----------\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/data\")\n",
        "TRAIN_JSONL = DATA_DIR / \"eval_queries.jsonl\"\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/models/mbart_rag_finetuned\")\n",
        "\n",
        "# Smaller lengths to reduce memory\n",
        "MAX_INPUT_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 64\n",
        "BATCH_SIZE = 1              # keep tiny batch size\n",
        "EPOCHS = 2                  # fewer epochs to fit memory\n",
        "LEARNING_RATE = 2e-5\n",
        "GRAD_ACCUM_STEPS = 8        # simulate larger batch via accumulation\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'tokenizer' in globals() and 'gen_model' in globals(), \"tokenizer and gen_model must be loaded (Cell 10)\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Training device:\", device)\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "gen_model.gradient_checkpointing_enable()\n",
        "\n",
        "# ---------- Load training examples ----------\n",
        "train_examples = []\n",
        "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            train_examples.append(json.loads(line))\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "assert len(train_examples) > 0, \"No training examples found in eval_queries.jsonl\"\n",
        "\n",
        "# ---------- Helper to build RAG-style input text ----------\n",
        "def build_rag_input(example, top_k=3, mode=\"hybrid_score\"):\n",
        "    q = example.get(\"query\") or example.get(\"question\") or example.get(\"q\")\n",
        "    hits = retrieve(q, k=top_k, mode=mode)\n",
        "    passages = [f\"[حوالہ] {ptext}\" for _, ptext, _ in hits]\n",
        "    context = \"\\n\\n\".join(passages)\n",
        "    instruction = \"حوالہ شدہ معلومات کی بنیاد پر مختصر اور درست جواب لکھیں۔\"\n",
        "    return f\"سوال: {q}\\n\\nحوالہ شدہ معلومات:\\n{context}\\n\\n{instruction}\"\n",
        "\n",
        "# ---------- Build dataset ----------\n",
        "inputs, targets = [], []\n",
        "for ex in train_examples:\n",
        "    inp = build_rag_input(ex, top_k=3, mode=\"hybrid_score\")\n",
        "    tgt = ex.get(\"gold_answer\") or \"\"\n",
        "    if not tgt:\n",
        "        continue\n",
        "    inputs.append(inp)\n",
        "    targets.append(tgt)\n",
        "\n",
        "assert len(inputs) > 0, \"No valid (input,target) pairs constructed\"\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    model_inputs = tokenizer(batch[\"input\"], max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(batch[\"target\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\", text_target=batch[\"target\"])\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "hf_ds = Dataset.from_dict({\"input\": inputs, \"target\": targets})\n",
        "hf_ds = hf_ds.map(tokenize_batch, batched=True, remove_columns=[\"input\", \"target\"])\n",
        "\n",
        "# ---------- Training arguments ----------\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    predict_with_generate=False,\n",
        "    remove_unused_columns=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=gen_model, label_pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# ---------- Trainer ----------\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=gen_model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting fine-tuning (memory-optimized).\")\n",
        "trainer.train()\n",
        "\n",
        "# ---------- Save fine-tuned model ----------\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "trainer.save_model(str(OUTPUT_DIR))\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "print(f\"Fine-tuned generator saved to {OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "id": "o9KPBjPbvN20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10c: Evaluate fine-tuned generator alone (no retrieval context)\n",
        "# - Loads eval queries and gold answers\n",
        "# - Generates answers directly from the fine-tuned MBART model\n",
        "# - Computes BLEU, chrF, token-level F1, latency\n",
        "# - Provides summary metrics for comparison with RAG and retrievers\n",
        "\n",
        "import time, statistics\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'tokenizer' in globals() and 'gen_model' in globals(), \"Fine-tuned generator must be loaded (Cell 10b)\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "\n",
        "# ---------- Helper: token-level F1 ----------\n",
        "def token_f1(pred, refs):\n",
        "    def toks(s):\n",
        "        return [t for t in s.strip().split() if t]\n",
        "    pred_t = set(toks(pred))\n",
        "    if not refs:\n",
        "        return 0.0\n",
        "    best = 0.0\n",
        "    for r in refs:\n",
        "        ref_t = set(toks(r))\n",
        "        if not ref_t:\n",
        "            continue\n",
        "        tp = len(pred_t & ref_t)\n",
        "        prec = tp / max(1, len(pred_t))\n",
        "        rec = tp / max(1, len(ref_t))\n",
        "        f1 = 0.0 if (prec + rec) == 0 else (2 * prec * rec) / (prec + rec)\n",
        "        best = max(best, f1)\n",
        "    return best\n",
        "\n",
        "# ---------- Build references ----------\n",
        "def get_references_for_item(item):\n",
        "    refs = []\n",
        "    if \"gold_answer\" in item and item[\"gold_answer\"]:\n",
        "        refs = [item[\"gold_answer\"]]\n",
        "    elif \"answers\" in item and item[\"answers\"]:\n",
        "        val = item[\"answers\"]\n",
        "        if isinstance(val, str):\n",
        "            refs = [val]\n",
        "        elif isinstance(val, list):\n",
        "            refs = [x for x in val if isinstance(x, str) and x.strip()]\n",
        "    return [r.strip() for r in refs if r and r.strip()]\n",
        "\n",
        "# ---------- Evaluation loop ----------\n",
        "preds = []\n",
        "refs_all = []\n",
        "f1s = []\n",
        "latencies = []\n",
        "\n",
        "for it in eval_queries:\n",
        "    q = it[\"query\"]\n",
        "    refs = get_references_for_item(it)\n",
        "\n",
        "    # Build simple prompt: question only (no retrieval context)\n",
        "    input_text = f\"سوال: {q}\\n\\nبراہ کرم مختصر اور درست جواب دیں۔\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=256, truncation=True).to(gen_model.device)\n",
        "\n",
        "    t0 = time.time()\n",
        "    outputs = gen_model.generate(**inputs, max_length=64, num_beams=4)\n",
        "    latency = time.time() - t0\n",
        "    latencies.append(latency)\n",
        "\n",
        "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    preds.append(pred)\n",
        "    refs_all.append(refs if refs else [\"\"])\n",
        "    f1s.append(token_f1(pred, refs))\n",
        "\n",
        "# ---------- BLEU / chrF ----------\n",
        "max_refs = max(len(r) for r in refs_all) if refs_all else 1\n",
        "ref_sets = []\n",
        "for i in range(max_refs):\n",
        "    ref_sets.append([ (refs[i] if i < len(refs) else \"\") for refs in refs_all ])\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(preds, ref_sets)\n",
        "chrf = sacrebleu.corpus_chrf(preds, ref_sets)\n",
        "\n",
        "# ---------- Summary ----------\n",
        "summary_gen = {\n",
        "    \"BLEU\": float(bleu.score),\n",
        "    \"chrF\": float(chrf.score),\n",
        "    \"F1_mean\": float(np.mean(f1s)) if f1s else 0.0,\n",
        "    \"F1_median\": float(statistics.median(f1s)) if f1s else 0.0,\n",
        "    \"latency_mean_s\": float(np.mean(latencies)) if latencies else 0.0,\n",
        "    \"latency_median_s\": float(statistics.median(latencies)) if latencies else 0.0,\n",
        "    \"n\": len(eval_queries)\n",
        "}\n",
        "\n",
        "print(\"\\n=== Generator-only Evaluation (no retrieval) ===\")\n",
        "for k,v in summary_gen.items():\n",
        "    print(f\"- {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "1jedq7xUz7dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional to clear GPU cache:\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "o9wns6MjwdVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional cell to just check if our imported generative model above performs fine independantly.\n",
        "# Expectation: Just generate a fluent urdu answer since this is just a smoke test.\n",
        "\n",
        "prompt = \"کووڈ-19 کی عام علامات کیا ہیں؟\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    output_ids = gen_model.generate(\n",
        "        **inputs,\n",
        "        max_length=64,\n",
        "        num_beams=4,\n",
        "        length_penalty=1.0\n",
        "    )\n",
        "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "mKPApZ23P6tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfCZMDk5Ivj_"
      },
      "outputs": [],
      "source": [
        "# Cell 11 (final, fixed): RAG inference — hybrid_score fusion + fine-tuned generator\n",
        "# - Loads tokenizer from base MBART (\"facebook/mbart-large-50\") for robust config\n",
        "# - Loads fine-tuned generator weights from Drive path\n",
        "# - Sets Urdu language codes (src_lang, forced_bos_token_id)\n",
        "# - Uses retrieve(..., mode=\"hybrid_score\") by default\n",
        "# - Strict grounding, intent-aware expansion, corpus rescue, extractive QA fallback\n",
        "\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'retrieve' in globals(), \"retrieve(...) must be defined (Cell 9)\"\n",
        "assert 'pid2text' in globals(), \"pid2text must be loaded\"\n",
        "assert 'passages_min' in globals() and len(passages_min) > 0, \"passages_min must be loaded\"\n",
        "\n",
        "# ---------- Device ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------- Load tokenizer (base) and fine-tuned generator (weights) ----------\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "\n",
        "GEN_PATH = \"/content/drive/MyDrive/models/mbart_rag_finetuned\"\n",
        "BASE_TOKENIZER = \"facebook/mbart-large-50\"\n",
        "print(f\"Loading tokenizer from base: {BASE_TOKENIZER}\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(BASE_TOKENIZER)\n",
        "\n",
        "print(f\"Loading fine-tuned generator weights from: {GEN_PATH}\")\n",
        "gen_model = MBartForConditionalGeneration.from_pretrained(GEN_PATH).to(device)\n",
        "gen_model.eval()\n",
        "\n",
        "# Set Urdu language codes for MBART\n",
        "# Use 'ur_PK' (Urdu, Pakistan) if available; fallback to 'ur_IN' if needed\n",
        "src_lang = \"ur_PK\" if \"ur_PK\" in tokenizer.lang_code_to_id else \"ur_IN\"\n",
        "tokenizer.src_lang = src_lang\n",
        "if hasattr(tokenizer, \"lang_code_to_id\"):\n",
        "    forced_bos_id = tokenizer.lang_code_to_id.get(src_lang)\n",
        "    if forced_bos_id is not None:\n",
        "        # Configure BOS token for generation\n",
        "        if hasattr(gen_model, \"generation_config\"):\n",
        "            gen_model.generation_config.forced_bos_token_id = forced_bos_id\n",
        "        else:\n",
        "            gen_model.config.forced_bos_token_id = forced_bos_id\n",
        "\n",
        "print(f\"Fine-tuned generator loaded. src_lang={src_lang}, forced_bos_token_id={getattr(gen_model.config, 'forced_bos_token_id', None)}\")\n",
        "\n",
        "# ---------- Config ----------\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/data\")\n",
        "CORPUS_JSONL_PATH = DATA_DIR / \"urdu_covid_corpus_clean.jsonl\"\n",
        "DEFAULT_RETRIEVE_K = 50      # larger candidate pool for rescue/rerank\n",
        "FINAL_CONTEXT_K = 5          # passages kept for generation\n",
        "DEFAULT_RETRIEVE_MODE = \"hybrid_score\"  # use true fusion by default\n",
        "\n",
        "SYMPTOM_EXPANSION = \" علامات بخار کھانسی سانس ذائقہ بو تھکن\"\n",
        "SYMPTOM_TOKENS = {t.lower() for t in [\"بخار\",\"کھانسی\",\"سانس\",\"علامات\",\"ذائقہ\",\"بو\",\"تھکن\",\"سانس لینے\",\"سانس پھولنا\",\"گلے\"]}\n",
        "\n",
        "# ---------- Load corpus (keyword rescue; read-only) ----------\n",
        "def load_corpus_jsonl(path: str):\n",
        "    docs = []\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    docs.append(json.loads(line))\n",
        "                except Exception:\n",
        "                    continue\n",
        "    except FileNotFoundError:\n",
        "        docs = []\n",
        "    return docs\n",
        "\n",
        "_CORPUS_DOCS = load_corpus_jsonl(str(CORPUS_JSONL_PATH))\n",
        "print(f\"Loaded _CORPUS_DOCS with {len(_CORPUS_DOCS)} items.\")\n",
        "\n",
        "# ---------- Intent detection ----------\n",
        "INTENT_KEYWORDS = {\n",
        "    \"symptoms\": [\"علامات\", \"بخار\", \"کھانسی\", \"سانس\", \"تھکن\", \"ذائقہ\", \"بو\"],\n",
        "    \"diagnosis\": [\"PCR\", \"RT-PCR\", \"اینٹیجن\", \"سویب\", \"ٹیسٹ\", \"تشخیص\"],\n",
        "    \"prevention\": [\"ماسک\", \"ہاتھ\", \"فاصلے\", \"سینیٹائزر\", \"صفائی\"],\n",
        "    \"vaccination\": [\"ویکسین\", \"ٹیکہ\", \"بوسٹر\", \"ڈوز\"]\n",
        "}\n",
        "\n",
        "def infer_intent(query: str) -> str:\n",
        "    q = query.lower()\n",
        "    if any(k.lower() in q for k in INTENT_KEYWORDS[\"diagnosis\"]): return \"diagnosis\"\n",
        "    if \"علامات\" in q or any(k.lower() in q for k in INTENT_KEYWORDS[\"symptoms\"]): return \"symptoms\"\n",
        "    if any(k.lower() in q for k in INTENT_KEYWORDS[\"vaccination\"]): return \"vaccination\"\n",
        "    if any(k.lower() in q for k in INTENT_KEYWORDS[\"prevention\"]): return \"prevention\"\n",
        "    return \"general\"\n",
        "\n",
        "def expanded_query_for_intent(query: str, intent: str) -> str:\n",
        "    if intent == \"symptoms\":\n",
        "        return query + \" \" + SYMPTOM_EXPANSION\n",
        "    return query\n",
        "\n",
        "# ---------- Retrieval helpers ----------\n",
        "def dedupe_candidates(cands: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Accepts iterable items:\n",
        "      - (pid, text)\n",
        "      - (pid, text, score)\n",
        "      - any sequence with pid at [0] and text at [1]\n",
        "    Returns deduplicated list of (pid, text) preserving first-seen order.\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for item in cands:\n",
        "        try:\n",
        "            pid = item[0]\n",
        "            txt = item[1]\n",
        "        except Exception:\n",
        "            continue\n",
        "        if pid in seen:\n",
        "            continue\n",
        "        seen.add(pid)\n",
        "        out.append((pid, txt))\n",
        "    return out\n",
        "\n",
        "def filter_retrieved_by_intent(retrieved_candidates: List[Tuple[str, str]], intent: str, keep: int) -> List[Tuple[str, str]]:\n",
        "    if intent == \"general\":\n",
        "        return dedupe_candidates(retrieved_candidates)[:keep]\n",
        "    intent_keywords = INTENT_KEYWORDS.get(intent, [])\n",
        "    prioritized, other = [], []\n",
        "    for pid, text in retrieved_candidates:\n",
        "        (prioritized if any(keyword.lower() in text.lower() for keyword in intent_keywords) else other).append((pid, text))\n",
        "    return dedupe_candidates(prioritized + other)[:keep]\n",
        "\n",
        "# ---------- Keyword rescue ----------\n",
        "def corpus_keyword_rescue(docs, tokens, limit=10):\n",
        "    hits = []\n",
        "    for d in docs:\n",
        "        txt = d.get(\"text\", \"\").lower()\n",
        "        if any(tok in txt for tok in tokens):\n",
        "            hits.append((d.get(\"id\"), d.get(\"text\")))\n",
        "            if len(hits) >= limit:\n",
        "                break\n",
        "    return hits\n",
        "\n",
        "# ---------- Grounding utilities ----------\n",
        "INSTRUCTION_TOKENS = [\"براہ کرم\", \"جواب\", \"ہدایات\", \"instruction\", \"Answer:\", \"Response:\", \"حوالہ شدہ معلومات\"]\n",
        "\n",
        "def strip_instruction_echoes(text: str) -> str:\n",
        "    t = text.strip()\n",
        "    for tok in INSTRUCTION_TOKENS:\n",
        "        t = t.replace(tok, \"\")\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def content_words(s: str):\n",
        "    stop = {\"ہے\", \"ہیں\", \"میں\", \"کی\", \"کے\", \"اور\", \"سے\", \"پر\", \"کہ\", \"ہی\", \"بھی\"}\n",
        "    toks = [w for w in re.split(r\"\\W+\", s) if len(w) >= 3 and w not in stop]\n",
        "    return set(toks)\n",
        "\n",
        "def has_overlap_with_context(ans: str, context_texts):\n",
        "    ans_words = content_words(ans.lower())\n",
        "    ctx_words = set()\n",
        "    for txt in context_texts:\n",
        "        ctx_words |= content_words(txt.lower())\n",
        "    return len(ans_words & ctx_words) >= 2\n",
        "\n",
        "def looks_like_echo(query: str, ans: str) -> bool:\n",
        "    q = re.sub(r\"\\s+\", \" \", query).strip()\n",
        "    a = re.sub(r\"\\s+\", \" \", ans).strip()\n",
        "    shared = os.path.commonprefix([q, a])\n",
        "    long_overlap = len(shared) >= max(8, int(0.3 * len(q)))\n",
        "    starts_like_q = a.startswith(q[: max(12, len(q)//2)])\n",
        "    return long_overlap or starts_like_q\n",
        "\n",
        "def is_vague(ans: str) -> bool:\n",
        "    a = ans.strip()\n",
        "    if len(a) < 12:\n",
        "        return True\n",
        "    hedges = [\"منحصر\", \"ممکن\", \"عام طور\", \"ضروری\", \"اہم\", \"کامیابی\"]\n",
        "    return any(h in a for h in hedges) and len(a.split()) < 16\n",
        "\n",
        "# ---------- Generation helper (fine-tuned MBART) ----------\n",
        "def generate_answer_with_mbart(query, retrieved_passages, max_length=160, num_beams=6, intent=\"general\"):\n",
        "    def short(p, limit=350):\n",
        "        p = \" \".join(p.split())\n",
        "        return (p[:limit] + \"…\") if len(p) > limit else p\n",
        "\n",
        "    context = \"\\n\\n\".join([f\"[حوالہ] {short(p)}\" for _, p in retrieved_passages])\n",
        "\n",
        "    if intent == \"symptoms\":\n",
        "        instruction = (\n",
        "            \"صرف انہی حوالہ شدہ معلومات کی بنیاد پر علامات کی فہرست لکھیں۔ \"\n",
        "            \"اضافی مشورے یا عمومی صحت کی معلومات شامل نہ کریں۔ سوال کو دوبارہ نہ لکھیں۔\"\n",
        "        )\n",
        "    else:\n",
        "        instruction = (\n",
        "            \"صرف انہی حوالہ شدہ معلومات کی بنیاد پر مختصر اور درست جواب لکھیں۔ \"\n",
        "            \"اضافی معلومات شامل نہ کریں اور سوال کو دوبارہ نہ لکھیں۔\"\n",
        "        )\n",
        "\n",
        "    prompt = f\"سوال: {query}\\n\\nحوالہ شدہ معلومات:\\n{context}\\n\\n{instruction}\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = gen_model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            min_length=8,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=1.0,\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=False,\n",
        "            early_stopping=True\n",
        "        )\n",
        "    ans = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    ans = strip_instruction_echoes(ans)\n",
        "    if ans and not ans.endswith(\"۔\"):\n",
        "        ans += \"۔\"\n",
        "    return ans, prompt\n",
        "\n",
        "# ---------- Extractive QA fallback (multi-passage) ----------\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "qa_model_name = \"deepset/xlm-roberta-base-squad2\"\n",
        "print(f\"Loading extractive QA model: {qa_model_name}\")\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name).to(device)\n",
        "qa_model.eval()\n",
        "print(\"Extractive QA model loaded successfully.\")\n",
        "\n",
        "def extractive_answer_multi(query, passages, top_k=3, min_span_len=3, intent=\"general\"):\n",
        "    best_ans, best_score = \"\", float(\"-inf\")\n",
        "    for passage in passages[:top_k]:\n",
        "        inputs = qa_tokenizer(query, passage, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = qa_model(**inputs)\n",
        "        start_logits = outputs.start_logits[0]\n",
        "        end_logits = outputs.end_logits[0]\n",
        "        score = float(torch.max(start_logits).item() + torch.max(end_logits).item())\n",
        "        start = int(torch.argmax(start_logits).item())\n",
        "        end = int(torch.argmax(end_logits).item()) + 1\n",
        "        tokens = inputs[\"input_ids\"][0][start:end]\n",
        "        ans = qa_tokenizer.decode(tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        if not ans or len(ans.split()) < min_span_len:\n",
        "            continue\n",
        "        if intent == \"symptoms\":\n",
        "            lower = ans.lower()\n",
        "            if not any(tok in lower for tok in SYMPTOM_TOKENS):\n",
        "                continue\n",
        "\n",
        "        if score > best_score:\n",
        "            best_ans, best_score = ans, score\n",
        "    return best_ans.strip()\n",
        "\n",
        "# ---------- RAG wrapper ----------\n",
        "def rag_answer(query, k=5, mode=DEFAULT_RETRIEVE_MODE, metadata_filters=None, max_length=160, num_beams=6, debug=True):\n",
        "    t0 = time.time()\n",
        "    intent = infer_intent(query)\n",
        "    q_for_retrieval = expanded_query_for_intent(query, intent)\n",
        "\n",
        "    # Start with candidates; prepend p0001 for symptoms if available\n",
        "    initial_candidates = []\n",
        "    if intent == \"symptoms\" and ('pid2text' in globals()) and ('p0001' in pid2text):\n",
        "        p0001_text = pid2text['p0001']\n",
        "        initial_candidates.append(('p0001', p0001_text))\n",
        "        if debug:\n",
        "            print(\"DEBUG: Explicitly pre-pending p0001 for symptom query.\")\n",
        "\n",
        "    # Retrieve via fusion retriever\n",
        "    retrieve_k = DEFAULT_RETRIEVE_K if intent == \"symptoms\" else max(k, DEFAULT_RETRIEVE_K // 5)\n",
        "    retrieved = retrieve(q_for_retrieval, k=retrieve_k, mode=mode, **(metadata_filters or {}))\n",
        "    initial_candidates.extend(retrieved)\n",
        "\n",
        "    # Deduplicate and intent-filter\n",
        "    retrieved_raw = dedupe_candidates(initial_candidates)\n",
        "    retrieved_filtered = filter_retrieved_by_intent(retrieved_raw, intent, keep=FINAL_CONTEXT_K)\n",
        "\n",
        "    # Corpus rescue (symptoms only) if filtered lacks strong symptom signals\n",
        "    rescued_hits = []\n",
        "    if intent == \"symptoms\":\n",
        "        has_signal = any(any(tok in txt.lower() for tok in SYMPTOM_TOKENS) for _, txt in retrieved_filtered)\n",
        "        if not has_signal:\n",
        "            pool_hits = [(pid, txt) for pid, txt in retrieved_raw if any(tok in txt.lower() for tok in SYMPTOM_TOKENS)]\n",
        "            if pool_hits:\n",
        "                retrieved_filtered = dedupe_candidates(pool_hits + retrieved_filtered)[:FINAL_CONTEXT_K]\n",
        "            else:\n",
        "                rescued_hits = corpus_keyword_rescue(_CORPUS_DOCS, SYMPTOM_TOKENS, limit=10)\n",
        "                if rescued_hits:\n",
        "                    retrieved_filtered = dedupe_candidates(rescued_hits + retrieved_filtered)[:FINAL_CONTEXT_K]\n",
        "\n",
        "    # Empty context → safe return; no hallucinations\n",
        "    if not retrieved_filtered:\n",
        "        latency = time.time() - t0\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": \"سیاق میں متعلقہ معلومات دستیاب نہیں۔\",\n",
        "            \"provenance\": [],\n",
        "            \"latency\": latency,\n",
        "            \"used_fallback\": True,\n",
        "            \"generator_failed\": True,\n",
        "            \"intent\": intent,\n",
        "            \"debug\": {\n",
        "                \"retrieved_top\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in retrieved_raw[:k]],\n",
        "                \"retrieved_filtered\": [],\n",
        "                \"rescued_hits\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in rescued_hits],\n",
        "                \"prompt_context_preview\": \"\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Try generator\n",
        "    gen_ans = \"\"\n",
        "    generator_failed = False\n",
        "    used_fallback = False\n",
        "    try:\n",
        "        gen_ans, prompt_used = generate_answer_with_mbart(\n",
        "            query, retrieved_filtered, max_length=max_length, num_beams=num_beams, intent=intent\n",
        "        )\n",
        "    except Exception:\n",
        "        gen_ans = \"\"\n",
        "        generator_failed = True\n",
        "        prompt_used = \"\"\n",
        "\n",
        "    # Grounding heuristics\n",
        "    context_texts = [p for _, p in retrieved_filtered]\n",
        "    if (not gen_ans) or looks_like_echo(query, gen_ans) or is_vague(gen_ans) or not has_overlap_with_context(gen_ans, context_texts):\n",
        "        generator_failed = True\n",
        "\n",
        "    # Fallback to extractive QA\n",
        "    if generator_failed:\n",
        "        used_fallback = True\n",
        "        def rephrase_from_question(q, a): return a\n",
        "        span = extractive_answer_multi(query, context_texts, top_k=min(3, len(context_texts)), intent=intent)\n",
        "        final_ans = rephrase_from_question(query, span) if span else \"کوئی جواب نہیں ملا۔\"\n",
        "    else:\n",
        "        final_ans = gen_ans\n",
        "\n",
        "    latency = time.time() - t0\n",
        "\n",
        "    # Provenance\n",
        "    if 'corpus_clean' in globals():\n",
        "        meta_map = {p[\"id\"]: p for p in corpus_clean}\n",
        "    else:\n",
        "        meta_map = {p[\"id\"]: p for p in passages_min}\n",
        "    provenance = []\n",
        "    for pid, text in retrieved_filtered:\n",
        "        meta = meta_map.get(pid, {})\n",
        "        provenance.append({\"id\": pid, \"source\": meta.get(\"source\"), \"retrieved_at\": meta.get(\"retrieved_at\")})\n",
        "\n",
        "    # Debug\n",
        "    debug_block = None\n",
        "    if debug:\n",
        "        debug_block = {\n",
        "            \"retrieved_top\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in retrieved_raw[:min(10, len(retrieved_raw))]],\n",
        "            \"retrieved_filtered\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in retrieved_filtered],\n",
        "            \"rescued_hits\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in rescued_hits],\n",
        "            \"prompt_context_preview\": context_texts[0][:400] if context_texts else \"\",\n",
        "            \"prompt_used\": prompt_used\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": final_ans,\n",
        "        \"provenance\": provenance,\n",
        "        \"latency\": latency,\n",
        "        \"used_fallback\": used_fallback,\n",
        "        \"generator_failed\": generator_failed,\n",
        "        \"intent\": intent,\n",
        "        \"debug\": debug_block\n",
        "    }\n",
        "\n",
        "# ---------- Quick smoke test ----------\n",
        "res = rag_answer(\"کووڈ-19 کی عام علامات کیا ہیں؟\", k=5, mode=DEFAULT_RETRIEVE_MODE, debug=True)\n",
        "print(\"Answer:\", res[\"answer\"])\n",
        "print(\"Used fallback:\", res[\"used_fallback\"], \"Generator failed:\", res[\"generator_failed\"], \"Intent:\", res[\"intent\"])\n",
        "if res[\"debug\"]:\n",
        "    print(\"\\n--- Debug: Top retrieved (raw) ---\")\n",
        "    for i, it in enumerate(res[\"debug\"][\"retrieved_top\"][:5], 1):\n",
        "        print(f\"{i}. [{it['id']}] {it['text']}\")\n",
        "    print(\"\\n--- Debug: Filtered context used for generation ---\")\n",
        "    for i, it in enumerate(res[\"debug\"][\"retrieved_filtered\"][:5], 1):\n",
        "        print(f\"{i}. [{it['id']}] {it['text']}\")\n",
        "    print(\"\\n--- Debug: Rescued hits (corpus scan) ---\")\n",
        "    for i, it in enumerate(res[\"debug\"][\"rescued_hits\"][:5], 1):\n",
        "        print(f\"{i}. [{it['id']}] {it['text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy cell so that notebook doesn`t reset\n",
        "print(\"Notebook don`t reset!!\")"
      ],
      "metadata": {
        "id": "2CJLtHqe8CCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick smoke-run: 10 eval items to sanity-check Cell 12\n",
        "sample_eval = eval_items[:10]  # eval_items is defined inside Cell 12; if not, use eval_queries[:10]\n",
        "print(\"Running quick RAG eval on 10 items (hybrid) to estimate runtime...\")\n",
        "_ = rag_generation_metrics(sample_eval, mode=\"hybrid\", k=5, max_length=120, num_beams=4)\n",
        "print(\"Quick run complete.\")\n"
      ],
      "metadata": {
        "id": "BZOGCC8KY4KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Comprehensive evaluation — retrieval, RAG generation, ablations, human sampling\n",
        "# - Evaluates bm25, dense, hybrid_interleave, hybrid_score, hybrid_rrf\n",
        "# - Computes Retrieval: Recall@1, Recall@5, MRR\n",
        "# - Computes Generation (RAG): BLEU, chrF, token F1, latency, fallback rates\n",
        "# - Ablation: disable extractive QA fallback; compare retrieval modes\n",
        "# - Exports summary JSON and human-eval samples\n",
        "\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import statistics\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'retrieve' in globals(), \"retrieve(...) must be defined (Cell 9)\"\n",
        "assert 'rag_answer' in globals(), \"rag_answer(...) must be defined (Cell 11)\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "assert isinstance(eval_queries, list) and len(eval_queries) > 0, \"eval_queries must be a non-empty list\"\n",
        "\n",
        "# ---------- Config ----------\n",
        "EVAL_ITEMS = eval_queries_val if 'eval_queries_val' in globals() else eval_queries\n",
        "MODES = [\"bm25\", \"dense\", \"hybrid_interleave\", \"hybrid_score\", \"hybrid_rrf\"]\n",
        "K_RETRIEVAL = 5\n",
        "RAG_K = 5\n",
        "RAG_MAX_LEN = 160\n",
        "RAG_BEAMS = 6\n",
        "\n",
        "# Output paths\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/eval_outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SUMMARY_JSON = OUT_DIR / \"rag_eval_summary.json\"\n",
        "PER_QUERY_JSON = OUT_DIR / \"rag_eval_per_query.jsonl\"\n",
        "HUMAN_SAMPLES_JSON = OUT_DIR / \"rag_human_samples.jsonl\"\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def get_references_for_item(item):\n",
        "    refs = []\n",
        "    if \"gold_answer\" in item and item[\"gold_answer\"]:\n",
        "        refs = [item[\"gold_answer\"]]\n",
        "    elif \"answers\" in item and item[\"answers\"]:\n",
        "        val = item[\"answers\"]\n",
        "        if isinstance(val, str):\n",
        "            refs = [val]\n",
        "        elif isinstance(val, list):\n",
        "            refs = [x for x in val if isinstance(x, str) and x.strip()]\n",
        "    return [r.strip() for r in refs if r and r.strip()]\n",
        "\n",
        "def token_f1(pred, refs):\n",
        "    def toks(s):\n",
        "        return [t for t in s.strip().split() if t]\n",
        "    pred_t = set(toks(pred))\n",
        "    if not refs:\n",
        "        return 0.0\n",
        "    best = 0.0\n",
        "    for r in refs:\n",
        "        ref_t = set(toks(r))\n",
        "        if not ref_t:\n",
        "            continue\n",
        "        tp = len(pred_t & ref_t)\n",
        "        prec = tp / max(1, len(pred_t))\n",
        "        rec = tp / max(1, len(ref_t))\n",
        "        f1 = 0.0 if (prec + rec) == 0 else (2 * prec * rec) / (prec + rec)\n",
        "        best = max(best, f1)\n",
        "    return best\n",
        "\n",
        "# ---------- Retrieval evaluation ----------\n",
        "def retrieval_metrics(eval_items, mode=\"hybrid_score\", k=K_RETRIEVAL):\n",
        "    rr_list = []\n",
        "    r1 = 0\n",
        "    r5 = 0\n",
        "    latencies = []\n",
        "    for it in eval_items:\n",
        "        q = it.get(\"query\") or it.get(\"question\") or it.get(\"q\") or \"\"\n",
        "        pos_ids = set(map(str, it.get(\"positive_ids\", [])))\n",
        "        t0 = time.time()\n",
        "        hits = retrieve(q, k=k, mode=mode)\n",
        "        latencies.append(time.time() - t0)\n",
        "        retrieved_ids = [r[0] for r in hits]\n",
        "        # MRR\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in pos_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "        # Recall@1 / Recall@5\n",
        "        r1 += 1 if (len(retrieved_ids) >= 1 and retrieved_ids[0] in pos_ids) else 0\n",
        "        r5 += 1 if any(pid in pos_ids for pid in retrieved_ids[:5]) else 0\n",
        "    n = len(eval_items)\n",
        "    return {\n",
        "        \"MRR\": float(np.mean(rr_list)) if rr_list else 0.0,\n",
        "        \"Recall@1\": r1 / n if n else 0.0,\n",
        "        \"Recall@5\": r5 / n if n else 0.0,\n",
        "        \"latency_mean_s\": float(np.mean(latencies)) if latencies else 0.0,\n",
        "        \"latency_median_s\": float(statistics.median(latencies)) if latencies else 0.0,\n",
        "        \"n\": n\n",
        "    }\n",
        "\n",
        "# ---------- SacreBLEU setup ----------\n",
        "try:\n",
        "    import sacrebleu\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"sacrebleu\"], check=True)\n",
        "    import sacrebleu\n",
        "\n",
        "# ---------- RAG generation evaluation ----------\n",
        "def rag_generation_metrics(eval_items, mode=\"hybrid_score\", k=RAG_K, max_length=RAG_MAX_LEN, num_beams=RAG_BEAMS, allow_fallback=True):\n",
        "    preds = []\n",
        "    refs_all = []\n",
        "    f1s = []\n",
        "    latencies = []\n",
        "    fallback_count = 0\n",
        "    gen_fail_count = 0\n",
        "    per_query = []\n",
        "\n",
        "    for it in eval_items:\n",
        "        q = it[\"query\"]\n",
        "        refs = get_references_for_item(it)\n",
        "        res = rag_answer(q, k=k, mode=mode, max_length=max_length, num_beams=num_beams, debug=False)\n",
        "        # ablation: disable fallback (use generator answer only; if generator_failed, treat as empty)\n",
        "        if not allow_fallback and res.get(\"generator_failed\"):\n",
        "            pred = \"\"\n",
        "            used_fallback = False\n",
        "            generator_failed = True\n",
        "        else:\n",
        "            pred = res.get(\"answer\", \"\").strip()\n",
        "            used_fallback = res.get(\"used_fallback\", False)\n",
        "            generator_failed = res.get(\"generator_failed\", False)\n",
        "\n",
        "        preds.append(pred)\n",
        "        refs_all.append(refs if refs else [\"\"])\n",
        "        f1s.append(token_f1(pred, refs))\n",
        "        latencies.append(res.get(\"latency\", 0.0))\n",
        "        fallback_count += 1 if used_fallback else 0\n",
        "        gen_fail_count += 1 if generator_failed else 0\n",
        "\n",
        "        per_query.append({\n",
        "            \"query\": q,\n",
        "            \"intent\": res.get(\"intent\"),\n",
        "            \"pred\": pred,\n",
        "            \"refs\": refs,\n",
        "            \"used_fallback\": used_fallback,\n",
        "            \"generator_failed\": generator_failed,\n",
        "            \"latency_s\": res.get(\"latency\", 0.0),\n",
        "            \"provenance\": res.get(\"provenance\", [])\n",
        "        })\n",
        "\n",
        "    # SacreBLEU / chrF formatting: transpose references to list-of-reference-sets\n",
        "    max_refs = max(len(r) for r in refs_all) if refs_all else 1\n",
        "    ref_sets = []\n",
        "    for i in range(max_refs):\n",
        "        ref_sets.append([ (refs[i] if i < len(refs) else \"\") for refs in refs_all ])\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(preds, ref_sets)\n",
        "    chrf = sacrebleu.corpus_chrf(preds, ref_sets)\n",
        "\n",
        "    summary = {\n",
        "        \"BLEU\": float(bleu.score),\n",
        "        \"chrF\": float(chrf.score),\n",
        "        \"F1_mean\": float(np.mean(f1s)) if f1s else 0.0,\n",
        "        \"F1_median\": float(statistics.median(f1s)) if f1s else 0.0,\n",
        "        \"latency_mean_s\": float(np.mean(latencies)) if latencies else 0.0,\n",
        "        \"latency_median_s\": float(statistics.median(latencies)) if latencies else 0.0,\n",
        "        \"fallback_rate\": fallback_count / len(eval_items) if eval_items else 0.0,\n",
        "        \"generator_fail_rate\": gen_fail_count / len(eval_items) if eval_items else 0.0,\n",
        "        \"n\": len(eval_items)\n",
        "    }\n",
        "    return summary, per_query\n",
        "\n",
        "# ---------- Human factuality sampling export ----------\n",
        "def export_human_samples(per_query_records, sample_size=25, path=HUMAN_SAMPLES_JSON):\n",
        "    # Select diverse samples: prioritize no-fallback, then fallback, mix intents\n",
        "    # Simple strategy: take first N no-fallback, then fill with fallback cases\n",
        "    no_fb = [r for r in per_query_records if not r[\"used_fallback\"]]\n",
        "    fb = [r for r in per_query_records if r[\"used_fallback\"]]\n",
        "    sample = (no_fb[:sample_size//2]) + (fb[:sample_size - len(no_fb[:sample_size//2])])\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in sample:\n",
        "            payload = {\n",
        "                \"query\": r[\"query\"],\n",
        "                \"intent\": r.get(\"intent\"),\n",
        "                \"prediction\": r[\"pred\"],\n",
        "                \"references\": r[\"refs\"],\n",
        "                \"provenance\": r.get(\"provenance\", []),\n",
        "                \"human_judgment\": {\n",
        "                    \"factual_consistency\": \"TBD_true/partial/false\",\n",
        "                    \"helpfulness\": \"TBD_1-5\",\n",
        "                    \"notes\": \"\"\n",
        "                }\n",
        "            }\n",
        "            f.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n",
        "    return len(sample)\n",
        "\n",
        "# ---------- Run retrieval evaluation across modes ----------\n",
        "retrieval_results = {}\n",
        "print(\"=== Retrieval Quality (Recall@1 / Recall@5 / MRR) ===\")\n",
        "for m in MODES:\n",
        "    metrics = retrieval_metrics(EVAL_ITEMS, mode=m, k=K_RETRIEVAL)\n",
        "    retrieval_results[m] = metrics\n",
        "    print(f\"- {m}: Recall@1={metrics['Recall@1']:.3f} | Recall@5={metrics['Recall@5']:.3f} | MRR={metrics['MRR']:.3f}\")\n",
        "\n",
        "# ---------- RAG generation & end-to-end evaluation (by mode) ----------\n",
        "rag_results_by_mode = {}\n",
        "per_query_all = {}\n",
        "\n",
        "print(\"\\n=== RAG Generation & End-to-End (with fallback) ===\")\n",
        "for m in MODES:\n",
        "    summary, per_query = rag_generation_metrics(EVAL_ITEMS, mode=m, k=RAG_K, max_length=RAG_MAX_LEN, num_beams=RAG_BEAMS, allow_fallback=True)\n",
        "    rag_results_by_mode[m] = summary\n",
        "    per_query_all[f\"{m}_with_fb\"] = per_query\n",
        "    print(f\"- {m}: BLEU={summary['BLEU']:.2f}, chrF={summary['chrF']:.2f}, F1_mean={summary['F1_mean']:.3f}, latency_mean_s={summary['latency_mean_s']:.3f}, fallback_rate={summary['fallback_rate']:.2f}\")\n",
        "\n",
        "print(\"\\n=== Ablation: RAG without extractive QA fallback (generator-only on retrieved context) ===\")\n",
        "rag_results_no_fb = {}\n",
        "for m in MODES:\n",
        "    summary, per_query = rag_generation_metrics(EVAL_ITEMS, mode=m, k=RAG_K, max_length=RAG_MAX_LEN, num_beams=RAG_BEAMS, allow_fallback=False)\n",
        "    rag_results_no_fb[m] = summary\n",
        "    per_query_all[f\"{m}_no_fb\"] = per_query\n",
        "    print(f\"- {m}: BLEU={summary['BLEU']:.2f}, chrF={summary['chrF']:.2f}, F1_mean={summary['F1_mean']:.3f}, latency_mean_s={summary['latency_mean_s']:.3f}, generator_fail_rate={summary['generator_fail_rate']:.2f}\")\n",
        "\n",
        "# ---------- Compact summary and export ----------\n",
        "summary = {\n",
        "    \"retrieval\": retrieval_results,\n",
        "    \"rag_with_fallback\": rag_results_by_mode,\n",
        "    \"rag_without_fallback\": rag_results_no_fb,\n",
        "    \"config\": {\n",
        "        \"modes\": MODES,\n",
        "        \"retrieval_k\": K_RETRIEVAL,\n",
        "        \"rag_k\": RAG_K,\n",
        "        \"rag_max_length\": RAG_MAX_LEN,\n",
        "        \"rag_num_beams\": RAG_BEAMS,\n",
        "        \"n_eval\": len(EVAL_ITEMS)\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(SUMMARY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(PER_QUERY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    for key, records in per_query_all.items():\n",
        "        for r in records:\n",
        "            r_out = dict(r)\n",
        "            r_out[\"mode\"] = key\n",
        "            f.write(json.dumps(r_out, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# ---------- Human factuality sampling ----------\n",
        "sample_count = export_human_samples(per_query_all.get(\"hybrid_score_with_fb\", []), sample_size=25, path=HUMAN_SAMPLES_JSON)\n",
        "\n",
        "print(\"\\n=== Summary (saved) ===\")\n",
        "print(f\"- Summary JSON: {SUMMARY_JSON}\")\n",
        "print(f\"- Per-query logs: {PER_QUERY_JSON}\")\n",
        "print(f\"- Human samples for factuality (n={sample_count}): {HUMAN_SAMPLES_JSON}\")\n"
      ],
      "metadata": {
        "id": "HZGQpO0zBBoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that bm25 index object and corpus mapping exist\n",
        "print(\"bm25_retrieve exists:\", 'bm25_retrieve' in globals())\n",
        "if 'bm25_index' in globals():\n",
        "    try:\n",
        "        print(\"bm25_index type:\", type(bm25_index))\n",
        "        # If your BM25 implementation exposes doc count:\n",
        "        print(\"bm25_index doc count (if available):\", getattr(bm25_index, 'doc_count', 'unknown'))\n",
        "    except Exception as e:\n",
        "        print(\"bm25_index introspect error:\", e)\n",
        "else:\n",
        "    print(\"bm25_index not found in globals.\")\n"
      ],
      "metadata": {
        "id": "Y9mGYjctVWm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "if 'bm25_retrieve' in globals():\n",
        "    print(inspect.getsource(bm25_retrieve))\n",
        "else:\n",
        "    print(\"bm25_retrieve not defined in this session.\")\n"
      ],
      "metadata": {
        "id": "R-_NuwrVVaCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_qs = [eval_queries[0][\"query\"], eval_queries[1][\"query\"], \"کووڈ-19 کی عام علامات کیا ہیں؟\"]\n",
        "for q in sample_qs:\n",
        "    print(\"\\nQuery:\", q)\n",
        "    try:\n",
        "        hits = bm25_retrieve(q, k=10)  # adjust if your wrapper signature differs\n",
        "        for i, (pid, score, txt) in enumerate(hits[:10], 1):\n",
        "            print(f\"{i}. {pid} score={score} text_preview={txt[:120]}\")\n",
        "    except Exception as e:\n",
        "        print(\"bm25_retrieve call error:\", e)\n"
      ],
      "metadata": {
        "id": "Hx8vabVqVedt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If your BM25 corpus mapping is named bm25_corpus or similar, inspect keys\n",
        "if 'bm25_corpus' in globals():\n",
        "    sample_keys = list(bm25_corpus.keys())[:10]\n",
        "    print(\"bm25_corpus sample keys:\", sample_keys)\n",
        "else:\n",
        "    print(\"No bm25_corpus variable found; check your BM25 build step.\")\n",
        "# Check overlap with pid2text\n",
        "if 'pid2text' in globals() and 'bm25_corpus' in globals():\n",
        "    overlap = set(pid2text.keys()) & set(bm25_corpus.keys())\n",
        "    print(\"Overlap count between pid2text and bm25_corpus:\", len(overlap))\n"
      ],
      "metadata": {
        "id": "2RyITB0ZViQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = eval_queries[0][\"query\"]\n",
        "print(\"Dense top-5:\", [pid for pid,_,_ in dense_retrieve(q, k=5)])\n",
        "print(\"BM25 top-5:\", [pid for pid,_,_ in bm25_retrieve(q, k=5)])\n"
      ],
      "metadata": {
        "id": "mFpFXSk8VlvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Full evaluation run (may take long). Use max_examples to limit.\n",
        "results = evaluate_rag(eval_queries, k=5, mode=\"hybrid\", max_examples=100)  # set None to run all\n",
        "print(\"Results:\", results)\n"
      ],
      "metadata": {
        "id": "C-7YyeTQBKJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Ablation experiments: compare retrieval modes and cross-lingual fallback\n",
        "modes = [\"bm25\", \"dense\", \"hybrid\"]\n",
        "ablation_results = {}\n",
        "for mode in modes:\n",
        "    print(\"Evaluating mode:\", mode)\n",
        "    ablation_results[mode] = evaluate_rag(eval_queries, k=5, mode=mode, max_examples=100)\n",
        "print(\"Ablation summary:\", ablation_results)\n"
      ],
      "metadata": {
        "id": "M2oPkfA9BSba"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UWlbuPkYZc0sXKeM2L5lHP0wSGcZhhCp",
      "authorship_tag": "ABX9TyOrSraxnl5ULKF9iyz5mld0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}