{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedsaalman/low-resource-rag-comparison/blob/main/NLP_Project_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HgyLTdh72rhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a20bbd-6d95-426d-e4f0-17630c386433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install required libraries (run this cell first and one by one all required libraries will be installed)\n",
        "# - transformers: model + generation\n",
        "# - sentence-transformers: dense embeddings / fine-tuning helpers\n",
        "# - faiss-cpu (or faiss-gpu if GPU available)\n",
        "# - rank_bm25: BM25 baseline\n",
        "# - datasets: convenient JSONL loading\n",
        "# - evaluate / sacrebleu: BLEU/chrF metrics\n",
        "# - tqdm: progress bars\n",
        "# - accelerate (optional) for distributed/faster training\n",
        "!pip install -q transformers sentence-transformers faiss-cpu rank_bm25 datasets evaluate sacrebleu tqdm accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM4J5YcY3WU5"
      },
      "outputs": [],
      "source": [
        "!pip list # Optional to run this cell: To check which of the libraries/packages have been installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_VNjxHI3YMd",
        "outputId": "f20122d7-08fd-4f16-ffde-0722b929bf60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers: 4.57.3\n",
            "sentence-transformers: 5.1.2\n",
            "torch: 2.9.0+cu126 cuda: True\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Imports and GPU check: Run this cell after the first cell\n",
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
        "import sentence_transformers # Import the package itself to access __version__\n",
        "\n",
        "# FAISS and BM25\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Datasets and metrics\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "import sacrebleu\n",
        "\n",
        "# Print versions and GPU info\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"torch not available:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViZ8XWh530oS",
        "outputId": "e9a405c9-dc1f-40fb-b5c5-ce30f2d40a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 60 corpus_clean;  60 passages_min;  100 eval queries\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "DATA_DIR = Path(\"drive/MyDrive/data\")  # change if files are elsewhere\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "with open(DATA_DIR / \"urdu_covid_passages.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Use split(None, 1) to split on the first occurrence of any whitespace\n",
        "        # This handles cases where the delimiter might be spaces instead of a tab.\n",
        "        if line.strip(): # Ensure line is not empty after stripping whitespace\n",
        "            parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "            if len(parts) == 2:\n",
        "                pid, text = parts\n",
        "                passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "            else:\n",
        "                print(f\"Skipping malformed line in urdu_covid_passages.tsv: {line.strip()}\")\n",
        "\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "print(\"Loaded:\", len(corpus_clean), \"corpus_clean; \", len(passages_min), \"passages_min; \", len(eval_queries), \"eval queries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlEd0kMX4uGm",
        "outputId": "d9fa47df-256e-4554-8e0c-03c466687517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing references (should be zero): 0\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Validate IDs referenced in eval/synthetic/hard_negatives exist in corpus\n",
        "# Run this after Cell 3.\n",
        "passage_ids = {p[\"id\"] for p in passages_min}\n",
        "missing = []\n",
        "for q in eval_queries:\n",
        "    for pid in q.get(\"positive_ids\", []):\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"eval\", q[\"query_id\"], pid))\n",
        "for s in synthetic_pairs:\n",
        "    if s[\"positive_id\"] not in passage_ids:\n",
        "        missing.append((\"synthetic\", s[\"synthetic_id\"], s[\"positive_id\"]))\n",
        "for h in hard_negatives:\n",
        "    for pid in h[\"hard_negatives\"]:\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"hardneg\", h[\"query_id\"], pid))\n",
        "print(\"Missing references (should be zero):\", len(missing))\n",
        "if missing:\n",
        "    print(missing[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJkfVV8T_GR5",
        "outputId": "4380d7b0-d532-44d6-8f75-8b1c8bb0c178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 top-3 for sample: [('p0001', 'کورونا وائرس مرض 2019 (COVID-19) ایک متعدی بیماری ہے جس کی عام علامات میں بخار، کھانسی اور سانس لینے میں دشواری شامل ہیں۔', 5.810063974702894), ('p0024', 'بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔', 5.103496839739362), ('p0002', 'کووڈ-19 کی تشخیص کے لیے rRT-PCR سویب ٹیسٹ عام طور پر استعمال ہوتے ہیں اور یہ وائرس کی موجودگی کی تصدیق کرتے ہیں۔', 4.589270107579207)]\n"
          ]
        }
      ],
      "source": [
        "# Cell 5 (Run after Cell 4): BM25 baseline index (tokenize with simple whitespace; for Urdu this is OK as baseline)\n",
        "# We'll store tokenized corpus and BM25 object for retrieval.\n",
        "from nltk.tokenize import word_tokenize\n",
        "# If nltk not installed, use simple split\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Added to resolve LookupError for 'punkt_tab'\n",
        "    tokenizer = lambda s: word_tokenize(s)\n",
        "except Exception:\n",
        "    tokenizer = lambda s: s.split()\n",
        "\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "corpus_ids = [p[\"id\"] for p in passages_min]\n",
        "tokenized_corpus = [tokenizer(t) for t in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Example retrieval function\n",
        "def bm25_retrieve(query, k=5):\n",
        "    q_tokens = tokenizer(query)\n",
        "    scores = bm25.get_scores(q_tokens)\n",
        "    topk = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in topk]\n",
        "\n",
        "# Quick test\n",
        "print(\"BM25 top-3 for sample:\", bm25_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5b: BM25-only retriever evaluation tool (run after Cell 5)\n",
        "# Purpose: standalone evaluation harness for the independent BM25 retriever (bm25_retrieve)\n",
        "# Metrics included (applicable to a retriever-only evaluation):\n",
        "#   - Recall@1, Recall@5\n",
        "#   - MRR (Mean Reciprocal Rank)\n",
        "#   - Precision@k (k=1,5)\n",
        "#   - Average / median retrieval latency\n",
        "#   - Optional: match by gold_passage_id or by substring match of gold_answer\n",
        "# Output:\n",
        "#   - Per-query JSONL saved to bm25_eval_results.jsonl\n",
        "#   - Printed summary with all metrics\n",
        "#\n",
        "# Requirements (must be available in the session):\n",
        "#   - bm25_retrieve(query, k) -> list of (passage_id, passage_text, score)\n",
        "#   - eval_queries: list of dicts with at least a query field and optionally:\n",
        "#       * \"question\" or \"query\" or \"q\"  (the query text)\n",
        "#       * \"gold_passage_id\" (optional) OR \"answer\"/\"gold\" (gold text to match)\n",
        "#\n",
        "# Usage:\n",
        "#   - Run this cell after you build the BM25 index (Cell 5).\n",
        "#   - Optionally pass a different eval list or k values to evaluate subsets.\n",
        "\n",
        "# Use this evaluator if your eval_queries items contain \"positive_ids\" and \"gold_answer\"\n",
        "import json, time, re, statistics\n",
        "from typing import List, Dict\n",
        "\n",
        "OUT_JSONL = \"bm25_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def get_query_text(item: Dict) -> str:\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_bm25_with_positive_ids(eval_items: List[Dict],\n",
        "                                    out_jsonl: str = OUT_JSONL,\n",
        "                                    k: int = DEFAULT_K,\n",
        "                                    recall_ks = RECALL_KS,\n",
        "                                    precision_ks = PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        # normalize to list of strings\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or item.get(\"answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = bm25_retrieve(q, k=k)   # (id, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] bm25_retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k (multiple positives supported)\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            # precision@k = (# positives in top-k) / k\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [t[:300] for t in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run it\n",
        "if 'eval_queries' not in globals():\n",
        "    # try to load from file if not in memory\n",
        "    eval_queries = []\n",
        "    with open(\"eval_queries.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            eval_queries.append(json.loads(line))\n",
        "\n",
        "summary, records = evaluate_bm25_with_positive_ids(eval_queries, out_jsonl=OUT_JSONL, k=DEFAULT_K)\n",
        "print(\"BM25 retrieval evaluation summary:\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# show a few examples where retrieval missed positives\n",
        "misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "print(f\"\\nTotal misses: {len(misses)} / {len(records)}. Showing up to 5 misses:\")\n",
        "for r in misses[:5]:\n",
        "    print(\"Query id:\", r.get(\"query_id\"), \"Query:\", r[\"query\"][:80])\n",
        "    print(\" Positives:\", r[\"positive_ids\"])\n",
        "    print(\" Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqDyRpA-924o",
        "outputId": "b4b25727-8787-4824-c2a0-66cbab4b35fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 retrieval evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.8853333333333333\n",
            "  Recall@1: 0.84\n",
            "  Recall@5: 0.95\n",
            "  Precision@1: 0.84\n",
            "  Precision@5: 0.21599999999999964\n",
            "  latency_mean_s: 0.00045699119567871094\n",
            "  latency_median_s: 0.0004508495330810547\n",
            "\n",
            "Total misses: 5 / 100. Showing up to 5 misses:\n",
            "Query id: q007 Query: کووڈ-19 ویکسین کا بنیادی مقصد کیا ہے؟\n",
            " Positives: ['p0007']\n",
            " Retrieved top ids: ['p0028', 'p0050', 'p0051', 'p0027', 'p0039']\n",
            "\n",
            "Query id: q019 Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            " Positives: ['p0020']\n",
            " Retrieved top ids: ['p0017', 'p0060', 'p0031', 'p0048', 'p0027']\n",
            "\n",
            "Query id: q038 Query: ویکسین سائیڈ ایفیکٹس کی نگرانی کیسے کی جاتی ہے؟\n",
            " Positives: ['p0039']\n",
            " Retrieved top ids: ['p0058', 'p0040', 'p0032', 'p0051', 'p0011']\n",
            "\n",
            "Query id: q065 Query: ویکسین کی سائیڈ ایفیکٹس کی رپورٹنگ کیسے ہوتی ہے؟\n",
            " Positives: ['p0039']\n",
            " Retrieved top ids: ['p0058', 'p0047', 'p0032', 'p0022', 'p0025']\n",
            "\n",
            "Query id: q095 Query: وبا کے دوران معاشی بحالی کے لیے کون سے اقدامات کیے جا سکتے ہیں؟\n",
            " Positives: ['p0054']\n",
            " Retrieved top ids: ['p0060', 'p0035', 'p0044', 'p0028', 'p0057']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406,
          "referenced_widgets": [
            "e49bdcad0c234d368ca23a3943b0ea07",
            "89e79cb483fa445188ccf9e46ddc5056",
            "91af2ffc5d86440a8aa1ed29ff201387",
            "1a016364c3d5439698acac6561f3bf48",
            "395e03d26967463fa5ea1f78aa285a52",
            "7b9d3b3ffbd84732bc7f207b9b101e72",
            "068ad0b42bec442bac8626ea174c103c",
            "965b7149283740d98e25249753fc9cc5",
            "c158c633c38a44beaac13c253b08461b",
            "1cfa6ceb38554bb0929e622768da1ad5",
            "41f4cd353c9c4fda912dcea7595101c9",
            "3e76feada5f24b8d992febc9ba95934f",
            "556b7771f1b145099daba8372e279ee4",
            "c9dc6a6ed16e4983b3f7ee95621b03a1",
            "be7b95d0975041a0a781fc4bb8611b53",
            "ba6cf6714a7344839a32dc72a5d04696",
            "710e2a75675c49a1831b9ebd12d1da0c",
            "520086371f214558bff6f03209254e5e",
            "8e69f3e19b0b423fa966a1768641b2cd",
            "a4eb1bcc202a439b91ea9e0c337d33f1",
            "d62627d0d367464ab09d19cd529440a3",
            "c3ac2e0240ab4b3bb49d56d7016f6c92",
            "fd849290691a4219a009dc56a163ca7d",
            "1e2baeccd01844a9a071037a58c352bc",
            "72ad8e15510645beb5eb207019b61198",
            "ade0a52ea9ea4303955e8ec6e37b6064",
            "ff2be32c4b8c45218f4ad3a045b0857c",
            "24586f6d64a24b418fbd7b3cb2a8c842",
            "fd387002cac149899d6a641a1ef77397",
            "e919d137ce154bf283aeeeabc5a1f0c5",
            "11fabb71fe734203af073a732e3eeea3",
            "330c62efe45c4c739ba5d8622fcc1491",
            "c41101c290d94989b1641c7ad2fbd3bc",
            "0ef52de5144f4689b9c2fa62dddcf4cc",
            "4e52b392ee554ece9690a08b537a81cd",
            "5c96407d11b74005a9abbfd232941afa",
            "5739014ec4ba478497b72b8d808f4ee8",
            "a50a7abfa0f44cbda66c51e4ef1f5848",
            "02832f9d470e4e10a11df3b952bbe52b",
            "db65fe40e9c547c1a3140597916f452e",
            "2e2f2860a3c545be8bf1aab0f8ca2ac3",
            "6cce53346a2c4ab6ac4801ef16ae37d7",
            "6032265340904a148238e43f4d807745",
            "3159b8f4d66449469a1e36779d7bd087",
            "017a2b157e1a4384b05cbd8594833ae4",
            "48b068d61ae747c4bea6b52cd3672b60",
            "7b9add69f5cb4197801ec7f512de4bde",
            "b0095a734d0640a68c32204bd9263660",
            "993d650d43c94d6ab62af98b4375a6a4",
            "86cb410119114229b99cfaeaf9757dd0",
            "1304e9fb0e0e446aad82dc6733e15e82",
            "d1336b7fbde444d8bca4f8906942a13a",
            "c2023a2ce8d349009f89005b307bf255",
            "0829b0a8af4c4e5da89c9916cde965eb",
            "ab6b8121c2594514a229da1fe522786c",
            "6a6ffdd4fb7f404fad7e8cd366b030f9",
            "3b3e5a5fb5804eaea5f274e8f806e4ad",
            "6e6974a493234bdfa8b500ed2d03b561",
            "5da6c0c5baeb47f297fd94dba23f761c",
            "0b1a7e779c0b4148b4988db211326b24",
            "d35a5cb5e0ae4676887efe04764833ad",
            "8e41249bbf844df587c0af30f4c20c1f",
            "d2a21999f14e414fa7c961246c1e916c",
            "b8c0c2a64d0042a29b899e4432870dae",
            "a5eeceff389d4da28cbccebb9de190aa",
            "2ecb2b5386c749c189819db2568cbdef",
            "821a40d85a2a4f678fda8c9e9b4e71f6",
            "c7b5cb42f5c741a3b7d288c2ec520e55",
            "e9dd13771d204ed4a15691d3b49f1bd5",
            "83b0d71e2faf4b8ab674bdb3a55228f8",
            "3e941aadc8d6403aaa278f62087a35cb",
            "e6e99d6551e146a092463f5936a35049",
            "9805c21567a04dbab0f65536bf296537",
            "1aa9e4f7c9ec43569a87771f224aeed1",
            "37a0eb12e3b8408abb7448c2f4863cb7",
            "dbb51f4a1f6f4a0d93adefe30a4e1eac",
            "4708bab344d440179b215dd4e9ca0daf",
            "516d6f94099a48229e8bcfe30cf377ae",
            "5ec7700e07c049efbe00cbdf66379893",
            "c9d3f5557a6144cca5eacd8d3b94dfea",
            "593c53988b984ac386318d49cbe8ca3f",
            "cbeb57490c714613b039777327c07efe",
            "826c44eb5a3a4eb7b189f3f17d94c730",
            "4f00e2dc35e24c809b4c9576059145a4",
            "aa96661207fa413d837f253fa7380dc9",
            "85f01f587ebd47a6b9b8c62ee4b6305d",
            "2d5687fa72c14ec9abd5abfee5786954",
            "b6795b1bd689462bba6073ed86ea7eff",
            "e24ebe816f8e4d48b468a02e7132e411",
            "3bc449a98220474ca20f4182f3063974",
            "d93ee0c78da0434caf8ad267e7eed8f7",
            "75549e4ec9b24a0e9b18155e1854a77a",
            "ff457656eaf14d619d107a4ebbca75c5",
            "59eff56dc8964e3f9d79f0487361fe7a",
            "079d4678df644cb4895af879089c70e2",
            "e24e5a3c8a034a289c788fff84ee2523",
            "f5275bffc98a4c50af6f71c01371712a",
            "c5aee29b324c47f9a99db5ef4b151417",
            "de89d42a71b847b58092ca76d79668fc",
            "52af3313ad5d4d3ab8b23cce8daafc7a",
            "03c21077aef4406093a0a41d0de5c672",
            "8267e6ed07134bae8855e4f7e76285a4",
            "a1a52faccdd8431e81662d81ac3e3470",
            "47afe6fe9f0347ddab2cb023979aa5e0",
            "f1fe30cf8c9d41119aaa7ab446421157",
            "1083500446a0415ba5bcfc412548aa71",
            "c5cef0ec18d1488fa89c2b7e60517fcb",
            "71c57763c541474ea7689997f41d93e4",
            "322c3716102b468cb6055f90c329cbec",
            "68df72bce1af4440af04feae1cd43dc3",
            "2610b6bb9c87444b914b4dabac6785bc",
            "2c700bfd679542cba79f2ce9f7406f18",
            "5bf9852e35ce4aafaa36525fd53d34fd",
            "1fd67c46b04a4e268ddf6c38670ba20c",
            "0e097061774646f68ce7a721e3860141",
            "bec2ca3d0fa045bd8f15369674a820a0",
            "22f06a5fb20942e09a8976d1445c91ce",
            "d3846707e7d64649bfc929509f775cd5",
            "60e15b7e436b48b295aa885261c0f3a4",
            "d23fd4951b16463592c6a326a22bd1a7",
            "d79ccf76543b47a3bcd2418f7080cfe9"
          ]
        },
        "id": "uw6ei6j6_P97",
        "outputId": "9b8bfc0b-d2e4-4938-8fed-836a2b515776"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e49bdcad0c234d368ca23a3943b0ea07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e76feada5f24b8d992febc9ba95934f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd849290691a4219a009dc56a163ca7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ef52de5144f4689b9c2fa62dddcf4cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "017a2b157e1a4384b05cbd8594833ae4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a6ffdd4fb7f404fad7e8cd366b030f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "821a40d85a2a4f678fda8c9e9b4e71f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "516d6f94099a48229e8bcfe30cf377ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e24ebe816f8e4d48b468a02e7132e411"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52af3313ad5d4d3ab8b23cce8daafc7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2610b6bb9c87444b914b4dabac6785bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense top-3: [('p0024', 'بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔', 0.7648560404777527), ('p0021', 'کووڈ-19 کے بعد بعض افراد میں طویل مدتی علامات (Long COVID) جیسے تھکن، سانس کی تکلیف اور دماغی دھند برقرار رہ سکتی ہیں؛ ریہیب پروگرامز مدد دیتے ہیں۔', 0.6735703349113464), ('p0036', 'کووڈ-19 کے مریضوں میں خون جمنے کے مسائل اور دیگر پیچیدگیاں بعض اوقات سامنے آئیں، اس لیے طبی نگرانی اور مناسب علاج ضروری ہے۔', 0.647298276424408)]\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Dense embeddings with a multilingual model (use a compact model for Colab)\n",
        "# We use a multilingual SBERT model that supports Urdu reasonably (e.g., 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embed_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "# Compute embeddings for passages_min (batching)\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS index (cosine similarity via normalized vectors)\n",
        "d = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)  # inner product\n",
        "# normalize embeddings for cosine\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# Map index positions to ids\n",
        "# retrieval function\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "# Quick test\n",
        "print(\"Dense top-3:\", dense_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6b: Evaluation of dense retriever (run after Cell 6)\n",
        "# Purpose: measure Recall@1, Recall@5, MRR, Precision@k, latency for dense_retrieve\n",
        "# Uses eval_queries with \"positive_ids\" and \"gold_answer\" fields\n",
        "\n",
        "import json, time, re, statistics\n",
        "\n",
        "OUT_JSONL_DENSE = \"dense_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s):\n",
        "    if s is None: return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "\n",
        "def get_query_text(item):\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_dense(eval_items, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K,\n",
        "                   recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies, rr_list = [], []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        pos_ids = item.get(\"positive_ids\") or []\n",
        "        if isinstance(pos_ids, str): pos_ids = [pos_ids]\n",
        "        pos_ids = [str(x) for x in pos_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        hits = dense_retrieve(q, k=k)  # (id, text, score)\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in pos_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in pos_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in pos_ids)\n",
        "            precision_sums[pk] += num_pos_in_topk / pk\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": pos_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [txt[:300] for txt in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": sum(rr_list)/n,\n",
        "        **{f\"Recall@{rk}\": recall_counts[rk]/n for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_sums[pk]/n for pk in precision_ks},\n",
        "        \"latency_mean_s\": statistics.mean(latencies) if latencies else 0.0,\n",
        "        \"latency_median_s\": statistics.median(latencies) if latencies else 0.0\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run evaluation\n",
        "print(\"[dense_eval] Running dense retriever evaluation...\")\n",
        "summary_dense, records_dense = evaluate_dense(eval_queries, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K)\n",
        "print(\"\\nDense retriever evaluation summary:\")\n",
        "for k,v in summary_dense.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Show a few examples\n",
        "print(\"\\nExamples (first 5):\")\n",
        "for r in records_dense[:5]:\n",
        "    print(\" - Query:\", r[\"query\"][:80])\n",
        "    print(\"   Retrieved ids:\", r[\"retrieved_ids\"][:6])\n",
        "    print(\"   Reciprocal rank:\", r[\"reciprocal_rank\"], \"Latency(s):\", round(r[\"latency\"], 4))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kE-KPlxENAG",
        "outputId": "35862faa-51f5-45e2-ffed-5f2431f7c588"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[dense_eval] Running dense retriever evaluation...\n",
            "\n",
            "Dense retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.7956666666666666\n",
            "  Recall@1: 0.7\n",
            "  Recall@5: 0.92\n",
            "  Precision@1: 0.7\n",
            "  Precision@5: 0.20199999999999968\n",
            "  latency_mean_s: 0.02900353670120239\n",
            "  latency_median_s: 0.022706270217895508\n",
            "\n",
            "Examples (first 5):\n",
            " - Query: کووڈ-19 کی عام علامات کیا ہیں؟\n",
            "   Retrieved ids: ['p0024', 'p0021', 'p0036', 'p0001', 'p0019']\n",
            "   Reciprocal rank: 0.25 Latency(s): 0.0575\n",
            "\n",
            " - Query: کووڈ-19 کی تشخیص کے لیے کون سا ٹیسٹ عام طور پر استعمال ہوتا ہے؟\n",
            "   Retrieved ids: ['p0018', 'p0002', 'p0036', 'p0055', 'p0024']\n",
            "   Reciprocal rank: 0.5 Latency(s): 0.0394\n",
            "\n",
            " - Query: ہاتھوں کی صفائی وبا کے دوران کیوں ضروری ہے؟\n",
            "   Retrieved ids: ['p0030', 'p0003', 'p0042', 'p0046', 'p0041']\n",
            "   Reciprocal rank: 0.5 Latency(s): 0.0494\n",
            "\n",
            " - Query: ماسک پہننے کے کیا فوائد ہیں؟\n",
            "   Retrieved ids: ['p0004', 'p0029', 'p0042', 'p0022', 'p0050']\n",
            "   Reciprocal rank: 1.0 Latency(s): 0.0749\n",
            "\n",
            " - Query: سماجی فاصلہ رکھنے کی اہمیت کیا ہے؟\n",
            "   Retrieved ids: ['p0005', 'p0054', 'p0015', 'p0016', 'p0046']\n",
            "   Reciprocal rank: 1.0 Latency(s): 0.0328\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Prepare InputExamples for sentence-transformers fine-tuning\n",
        "# Now with an 80/20 train/validation split\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "import random\n",
        "\n",
        "pid2text = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "examples = []\n",
        "for s in synthetic_pairs:\n",
        "    q = s[\"query\"]\n",
        "    pos = pid2text.get(s[\"positive_id\"])\n",
        "    neg = None\n",
        "    # Find hard negatives if available\n",
        "    hn = next((h for h in hard_negatives if h[\"query_id\"] == s.get(\"synthetic_id\", s.get(\"query_id\"))), None)\n",
        "    if hn:\n",
        "        for nid in hn[\"hard_negatives\"]:\n",
        "            if nid != s[\"positive_id\"]:\n",
        "                neg = pid2text.get(nid)\n",
        "                break\n",
        "    if neg is None:\n",
        "        # fallback: random negative\n",
        "        neg_id = random.choice([pid for pid in corpus_ids if pid != s[\"positive_id\"]])\n",
        "        neg = pid2text[neg_id]\n",
        "    if pos and neg:\n",
        "        examples.append(InputExample(texts=[q, pos, neg]))\n",
        "\n",
        "print(\"Prepared\", len(examples), \"triplet examples.\")\n",
        "\n",
        "# --- Split into train/validation (80/20) ---\n",
        "random.shuffle(examples)\n",
        "split_idx = int(0.8 * len(examples))\n",
        "train_examples = examples[:split_idx]\n",
        "val_examples = examples[split_idx:]\n",
        "\n",
        "print(\"Train examples:\", len(train_examples))\n",
        "print(\"Validation examples:\", len(val_examples))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjfNbXQ7Jboz",
        "outputId": "28253491-1633-414e-a0da-5673f2f98a7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 500 triplet examples.\n",
            "Train examples: 400\n",
            "Validation examples: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 (use in-memory model; do NOT reload): Fine-tune SBERT with triplet loss and IR validation on passages_min\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, losses, evaluation\n",
        "import faiss\n",
        "\n",
        "# Sanity checks\n",
        "assert isinstance(train_examples, list) and len(train_examples) > 0, \"train_examples must be a non-empty list\"\n",
        "assert 'passages_min' in globals(), \"passages_min must be loaded\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "\n",
        "# Build validation split against real corpus & labels\n",
        "eval_val = eval_queries_val if 'eval_queries_val' in globals() else eval_queries[int(0.8*len(eval_queries)):]\n",
        "val_queries_dict = {it[\"query_id\"]: it[\"query\"] for it in eval_val}\n",
        "val_relevant_dict = {it[\"query_id\"]: set(it[\"positive_ids\"]) for it in eval_val}\n",
        "val_corpus_dict = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "# Warn if labels reference missing ids\n",
        "missing = []\n",
        "for qid, rels in val_relevant_dict.items():\n",
        "    for pid in rels:\n",
        "        if pid not in val_corpus_dict:\n",
        "            missing.append((qid, pid))\n",
        "if missing:\n",
        "    print(f\"Warning: {len(missing)} relevant ids not found in corpus. Example:\", missing[:3])\n",
        "\n",
        "# Construct evaluator (defaults to cosine similarity)\n",
        "retrieval_evaluator = evaluation.InformationRetrievalEvaluator(\n",
        "    queries=val_queries_dict,\n",
        "    corpus=val_corpus_dict,\n",
        "    relevant_docs=val_relevant_dict,\n",
        "    name=\"val_ir_passages\"\n",
        ")\n",
        "\n",
        "# Start from baseline multilingual MiniLM\n",
        "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\").to(\"cuda\")\n",
        "\n",
        "# Triplet loss with conservative settings\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.TripletLoss(\n",
        "    model=embedder,\n",
        "    distance_metric=losses.TripletDistanceMetric.COSINE,\n",
        "    triplet_margin=0.3\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
        "optimizer_params = {'lr': 2e-5}\n",
        "\n",
        "# Train with IR evaluator\n",
        "embedder.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=retrieval_evaluator,\n",
        "    epochs=num_epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    optimizer_params=optimizer_params,\n",
        "    show_progress_bar=True,\n",
        "    output_path=\"fine_tuned_sbert_urdu_passages\"\n",
        ")\n",
        "\n",
        "print(\"✅ Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\")\n"
      ],
      "metadata": {
        "id": "R8-GRissKrda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557,
          "referenced_widgets": [
            "f2971407d4e745a6a1be2b75b6248ed6",
            "7c47573e9b0541bc8b2859c578ad8fb9",
            "471786f95deb47edb67150babd27d09c",
            "dc580f53f75d424e9c8adcaacd28cc72",
            "68ea5ec0dda84af0a5b4bbafa1023c7d",
            "3d0717dfa50e4cc5a6b433ee5a5ff460",
            "b5d5021b645e4015890345f869c43c5c",
            "c0e363738c4e46f1aff170e6bfd6b77f",
            "942dabb05bc045dd8373df043dcbaf19",
            "47356731cad44720b0a73db704927887",
            "42460165781440cf9b5b861695a61365"
          ]
        },
        "outputId": "86764ece-8134-41f3-804c-2f14bfd9a721"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2971407d4e745a6a1be2b75b6248ed6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmwaqarsaleem1\u001b[0m (\u001b[33mmwaqarsaleem1-national-university-of-computing-and-emerg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251215_163312-tjqyo2tw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mwaqarsaleem1-national-university-of-computing-and-emerg/sentence-transformers/runs/tjqyo2tw' target=\"_blank\">scarlet-aardvark-8</a></strong> to <a href='https://wandb.ai/mwaqarsaleem1-national-university-of-computing-and-emerg/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mwaqarsaleem1-national-university-of-computing-and-emerg/sentence-transformers' target=\"_blank\">https://wandb.ai/mwaqarsaleem1-national-university-of-computing-and-emerg/sentence-transformers</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mwaqarsaleem1-national-university-of-computing-and-emerg/sentence-transformers/runs/tjqyo2tw' target=\"_blank\">https://wandb.ai/mwaqarsaleem1-national-university-of-computing-and-emerg/sentence-transformers/runs/tjqyo2tw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:13, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@1</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@3</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@5</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@10</th>\n",
              "      <th>Val Ir Passages Cosine Precision@1</th>\n",
              "      <th>Val Ir Passages Cosine Precision@3</th>\n",
              "      <th>Val Ir Passages Cosine Precision@5</th>\n",
              "      <th>Val Ir Passages Cosine Precision@10</th>\n",
              "      <th>Val Ir Passages Cosine Recall@1</th>\n",
              "      <th>Val Ir Passages Cosine Recall@3</th>\n",
              "      <th>Val Ir Passages Cosine Recall@5</th>\n",
              "      <th>Val Ir Passages Cosine Recall@10</th>\n",
              "      <th>Val Ir Passages Cosine Ndcg@10</th>\n",
              "      <th>Val Ir Passages Cosine Mrr@10</th>\n",
              "      <th>Val Ir Passages Cosine Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.816226</td>\n",
              "      <td>0.862500</td>\n",
              "      <td>0.767717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.821244</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.774146</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8b: Rebuild FAISS with fine-tuned in-memory embedder\n",
        "\n",
        "import faiss\n",
        "\n",
        "# Use the current in-memory fine-tuned 'embedder'\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index = faiss.IndexFlatIP(passage_embeddings.shape[1])\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "print(\"✅ FAISS rebuilt with fine-tuned embeddings (in-memory model).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b22969d73471439aa2896966f1fdbff4",
            "6de05a0b2ffd465fa2d01fec45a7a0e9",
            "c00e808a74ed47e2b800eb8244a12601",
            "91017b4cbbc24440bea91ecf8e3ff3c3",
            "22d81019bf634096882edf8c009e0e9f",
            "1090b5c79c84441baedf1a6100312bf3",
            "a29acf69c8fe4c0d9df3cac3c29ed3d4",
            "50f4f9b7db7a40d6b56ec27ddc4c3fe5",
            "1826f04b6e774b7a9cc1a753b64ac74e",
            "d92a11d4c80c449eab3d02eab97e1835",
            "b85e0205027c4664a84bee9321a3bbb0"
          ]
        },
        "id": "x9LZDA7kMWON",
        "outputId": "3f51aa33-8b95-4511-c65f-e8fa4044327c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b22969d73471439aa2896966f1fdbff4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS rebuilt with fine-tuned embeddings (in-memory model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 (final): Retriever wrapper with true fusion modes (non-destructive)\n",
        "# - Creates bm25_new only if not present\n",
        "# - Supports modes: 'bm25', 'dense', 'hybrid_interleave' (legacy), 'hybrid_score', 'hybrid_rrf'\n",
        "# - Returns list of (pid, text, score) tuples\n",
        "# - Does NOT rebuild or overwrite dense/FAISS objects\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ---------- Config ----------\n",
        "# Tune these later on a small validation set\n",
        "DEFAULT_RETRIEVE_POOL = 50\n",
        "SCORE_FUSION_ALPHA = 0.6   # alpha in [0,1] for score fusion: alpha * dense + (1-alpha) * bm25\n",
        "RRF_K = 60                 # reciprocal rank fusion constant\n",
        "\n",
        "# ---------- Sanity checks for canonical corpus ----------\n",
        "assert 'passages_min' in globals() and isinstance(passages_min, list) and len(passages_min) > 0, \"passages_min must be loaded\"\n",
        "assert 'pid2text' in globals() and isinstance(pid2text, dict) and len(pid2text) > 0, \"pid2text must be available\"\n",
        "assert 'dense_retrieve' in globals(), \"dense_retrieve wrapper must be defined (fine-tuned dense retriever)\"\n",
        "\n",
        "# ---------- Build or reuse BM25 index (non-destructive) ----------\n",
        "try:\n",
        "    # If bm25_new already exists from a previous run, reuse it\n",
        "    bm25_new  # noqa: F821\n",
        "except Exception:\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank_bm25\"], check=True)\n",
        "        from rank_bm25 import BM25Okapi\n",
        "\n",
        "    # Build tokenized corpus from passages_min (light normalization)\n",
        "    def _normalize_for_bm25(s: str) -> str:\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = s.replace(\"\\u200c\", \" \")  # zero-width non-joiner\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s.lower()\n",
        "\n",
        "    bm25_new_ids = [p[\"id\"] for p in passages_min]\n",
        "    bm25_new_texts = [p[\"text\"] for p in passages_min]\n",
        "    bm25_new_tokenized = [_normalize_for_bm25(t).split() for t in bm25_new_texts]\n",
        "    bm25_new = BM25Okapi(bm25_new_tokenized)\n",
        "\n",
        "# Safe wrapper for BM25 that returns (pid, text, score)\n",
        "def bm25_new_retrieve(query: str, k: int = 5):\n",
        "    q_tok = _normalize_for_bm25(query).split()\n",
        "    scores = bm25_new.get_scores(q_tok)\n",
        "    top_idx = np.argsort(scores)[::-1][:k]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        i = int(i)\n",
        "        pid = bm25_new_ids[i]\n",
        "        text = bm25_new_texts[i]\n",
        "        score = float(scores[i])\n",
        "        results.append((pid, text, score))\n",
        "    return results\n",
        "\n",
        "# ---------- Fusion utilities ----------\n",
        "def normalize_scores(score_map):\n",
        "    \"\"\"Min-max normalize a dict of scores to [0,1].\"\"\"\n",
        "    if not score_map:\n",
        "        return {}\n",
        "    vals = list(score_map.values())\n",
        "    lo, hi = min(vals), max(vals)\n",
        "    if hi == lo:\n",
        "        return {k: 1.0 for k in score_map}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in score_map.items()}\n",
        "\n",
        "def rrf_rank(dense_list, bm25_list, k_rrf=RRF_K):\n",
        "    \"\"\"Reciprocal Rank Fusion: returns sorted list of pids by RRF score.\"\"\"\n",
        "    score = {}\n",
        "    for rank, (pid, _, _) in enumerate(dense_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    for rank, (pid, _, _) in enumerate(bm25_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    sorted_pids = sorted(score.keys(), key=lambda p: score[p], reverse=True)\n",
        "    return sorted_pids, score\n",
        "\n",
        "# ---------- Metadata filter helper (unchanged semantics) ----------\n",
        "# If you have meta_map from corpus_clean, it will be used; otherwise fallback to passages_min metadata\n",
        "if 'corpus_clean' in globals():\n",
        "    meta_map = {p[\"id\"]: p for p in corpus_clean}\n",
        "else:\n",
        "    meta_map = {p[\"id\"]: p for p in passages_min}\n",
        "\n",
        "def filter_by_metadata(candidate_ids, min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    out = []\n",
        "    for pid in candidate_ids:\n",
        "        m = meta_map.get(pid, {})\n",
        "        ok = True\n",
        "        if min_date or max_date:\n",
        "            dt = None\n",
        "            if \"retrieved_at\" in m:\n",
        "                try:\n",
        "                    dt = datetime.fromisoformat(m[\"retrieved_at\"])\n",
        "                except Exception:\n",
        "                    dt = None\n",
        "            if dt:\n",
        "                if min_date and dt < min_date: ok = False\n",
        "                if max_date and dt > max_date: ok = False\n",
        "        if allowed_sources and m.get(\"source\") not in allowed_sources:\n",
        "            ok = False\n",
        "        if exclude_time_sensitive is not None and m.get(\"time_sensitive\") == exclude_time_sensitive:\n",
        "            ok = False\n",
        "        if ok:\n",
        "            out.append(pid)\n",
        "    return out\n",
        "\n",
        "# ---------- Main retrieve wrapper with fusion modes ----------\n",
        "def retrieve(query: str, k: int = 5, mode: str = \"hybrid_score\", min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    \"\"\"\n",
        "    retrieve(query, k, mode)\n",
        "    Modes:\n",
        "      - 'bm25' : BM25-only (bm25_new_retrieve)\n",
        "      - 'dense' : dense-only (dense_retrieve)\n",
        "      - 'hybrid_interleave' : legacy interleave (dense first, then bm25)\n",
        "      - 'hybrid_score' : score fusion (normalized dense + bm25)\n",
        "      - 'hybrid_rrf' : reciprocal rank fusion (RRF)\n",
        "    Returns: list of (pid, text, score)\n",
        "    \"\"\"\n",
        "    # Get candidate pools (pool size configurable)\n",
        "    pool = max(DEFAULT_RETRIEVE_POOL, k)\n",
        "    dense_hits = dense_retrieve(query, k=pool)   # expected (pid, text, score)\n",
        "    bm25_hits = bm25_new_retrieve(query, k=pool) # (pid, text, score)\n",
        "\n",
        "    # Mode-specific behavior\n",
        "    if mode == \"bm25\":\n",
        "        results = bm25_hits[:k]\n",
        "    elif mode == \"dense\":\n",
        "        results = dense_hits[:k]\n",
        "    elif mode == \"hybrid_interleave\":\n",
        "        # preserve dense-first interleaving (legacy behavior)\n",
        "        seen = set()\n",
        "        cands = []\n",
        "        for lst in (dense_hits, bm25_hits):\n",
        "            for pid, text, score in lst:\n",
        "                if pid not in seen:\n",
        "                    seen.add(pid)\n",
        "                    cands.append((pid, text, float(score)))\n",
        "        results = cands[:k]\n",
        "    elif mode == \"hybrid_score\":\n",
        "        # Score fusion: normalize and combine\n",
        "        dense_scores = {pid: sc for pid, _, sc in dense_hits}\n",
        "        bm25_scores = {pid: sc for pid, _, sc in bm25_hits}\n",
        "        d_norm = normalize_scores(dense_scores)\n",
        "        b_norm = normalize_scores(bm25_scores)\n",
        "        alpha = SCORE_FUSION_ALPHA\n",
        "        combined = {}\n",
        "        for pid in set(list(d_norm.keys()) + list(b_norm.keys())):\n",
        "            combined[pid] = alpha * d_norm.get(pid, 0.0) + (1 - alpha) * b_norm.get(pid, 0.0)\n",
        "        # sort by combined score\n",
        "        sorted_pids = sorted(combined.keys(), key=lambda p: combined[p], reverse=True)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(combined[pid])))\n",
        "    elif mode == \"hybrid_rrf\":\n",
        "        sorted_pids, score_map = rrf_rank(dense_hits, bm25_hits, k_rrf=RRF_K)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(score_map.get(pid, 0.0))))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieve mode: {mode}\")\n",
        "\n",
        "    # Apply metadata filters if requested (filter by pid only)\n",
        "    if any([min_date, max_date, allowed_sources, exclude_time_sensitive is not None]):\n",
        "        filtered_ids = filter_by_metadata([pid for pid,_,_ in results], min_date, max_date, allowed_sources, exclude_time_sensitive)\n",
        "        results = [(pid, pid2text.get(pid, \"\"), score) for pid,_,score in results if pid in filtered_ids]\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------- Quick sample test (safe) ----------\n",
        "q = eval_queries[0][\"query\"] if 'eval_queries' in globals() and len(eval_queries)>0 else \"کووڈ-19 کی عام علامات کیا ہیں؟\"\n",
        "print(\"Sample dense top-5 ids:\", [r[0] for r in dense_retrieve(q, k=5)])\n",
        "print(\"Sample bm25_new top-5 ids:\", [r[0] for r in bm25_new_retrieve(q, k=5)])\n",
        "print(\"Sample hybrid_score top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_score')])\n",
        "print(\"Sample hybrid_rrf top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_rrf')])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyXpE1l7DFn4",
        "outputId": "ac073180-3bfd-4199-bf45-84bf187f3782"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample dense top-5 ids: ['p0024', 'p0001', 'p0021', 'p0044', 'p0008']\n",
            "Sample bm25_new top-5 ids: ['p0001', 'p0024', 'p0002', 'p0044', 'p0021']\n",
            "Sample hybrid_score top-5 ids: ['p0001', 'p0024', 'p0044', 'p0021', 'p0002']\n",
            "Sample hybrid_rrf top-5 ids: ['p0024', 'p0001', 'p0021', 'p0044', 'p0002']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9b: Run after above cell 9. Cell 9 creates B2M5 + Dense hybrid and below\n",
        "# cell evaluates its performance:\n",
        "# Validation diagnostics — Recall@1, Recall@5, MRR, Precision@k for retrievers\n",
        "# - Works with any mode supported by your Cell 9 wrapper: 'bm25', 'dense', 'hybrid_interleave', 'hybrid_score', 'hybrid_rrf'\n",
        "# - Calls retrieve(...) and computes retrieval metrics\n",
        "# - Outputs summary metrics and a few examples of misses\n",
        "\n",
        "import time, statistics\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def evaluate_retriever(eval_items, mode=\"hybrid_score\", k=DEFAULT_K,\n",
        "                       recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in tqdm(eval_items, desc=f\"Evaluating {mode} retriever\"):\n",
        "        total += 1\n",
        "        q = item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = retrieve(q, k=k, mode=mode)   # (pid, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [r[0] for r in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# ---------- Run evaluation ----------\n",
        "# Use eval_queries_val if defined, else fall back to eval_queries\n",
        "eval_items = eval_queries_val if 'eval_queries_val' in globals() else eval_queries\n",
        "\n",
        "# Evaluate all retriever modes\n",
        "modes = [\"bm25\", \"dense\", \"hybrid_interleave\", \"hybrid_score\", \"hybrid_rrf\"]\n",
        "results = {}\n",
        "for m in modes:\n",
        "    summary, records = evaluate_retriever(eval_items, mode=m, k=DEFAULT_K)\n",
        "    results[m] = summary\n",
        "    print(f\"\\n{m} retriever evaluation summary:\")\n",
        "    for k,v in summary.items():\n",
        "        print(f\"  {k}: {v:.3f}\" if isinstance(v,float) else f\"  {k}: {v}\")\n",
        "\n",
        "    # Show a few misses\n",
        "    misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "    print(f\"  Total misses: {len(misses)} / {len(records)}. Showing up to 3 misses:\")\n",
        "    for r in misses[:3]:\n",
        "        print(\"   Query:\", r[\"query\"][:80])\n",
        "        print(\"    Positives:\", r[\"positive_ids\"])\n",
        "        print(\"    Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7f327aa1fdca4907a6a44bea202c8b32",
            "5112415152134b10bd9abe84a9b704c4",
            "c99b302201b240f1938085fcfd229651",
            "3d1e04dee6534975ba518694d72856d6",
            "3c38cf94031f42e18e794b6b72ca1bbb",
            "834c0f360fba496cad14e0c3cdf9687a",
            "92e333088e9a4b24b067a8f36e3a6560",
            "89939202bac541f192e7447a3aeb7a88",
            "ce5e93fa72b045bd98f0ad26d028e7ad",
            "842dbd3dae17421b97d46de529b7d8ff",
            "438575727558466d9fbbaff8fcd48805",
            "8deb66a5eeac40018882b5327b3e066e",
            "93764513aa3643d0b4b8a35e901d74a2",
            "90861d2b87de46f3bc7abc2b9432d2df",
            "35321cdf1c8542a49f270d129fc82307",
            "7076df42a3db4e87b96c363b64bb69dd",
            "942ae86a5668450fb6fbc97abc2ba2dd",
            "47e4e77522334207a838c13d47002fe1",
            "ca8bf65a5b384ad68d8977dfc7fc724e",
            "521e67381bc244c992a8e91f8b90b964",
            "e0411bdd3fe648b680604c877fd0b8fb",
            "48b5742ff7d04e41bdfc9ecdbd4fbf58",
            "7562cabb7c64473fa0e8242e62f9264a",
            "55fe19b57d514ff98b59a4f10a192bd5",
            "b3fbb6a788e04b27a8da7c61e7c4f185",
            "ac73b6bf3a874d2cbc61e047c844186d",
            "47b822bc1dd44a2ead552274d7c817eb",
            "00096749a98846e1ab393b9364f4b16a",
            "b900222d35eb4c648ba360f42529c112",
            "9bb4b48192994905b3787b9b129cd2fc",
            "9c64ddababc84b51a4fb9dca0b98687f",
            "a416728454c741c587001cd9e9c0ba77",
            "66d21075574e42179b17dcdd721678a7",
            "7c22868ced3b40e3a6f32c0cf7d86522",
            "b4a9f638185f44be83b1dc2d87199693",
            "b4242ab356bf4f6d962aa85aa0484866",
            "53287fb659344b16bc499edfc77e10eb",
            "f986a5815dac4da59c52c71b3bf075b4",
            "54358bc4490c484b8179ace9c34599c1",
            "2ede072bf4b046c285a350e797127b63",
            "ecf357c417964bad931d04b227b0f673",
            "713f09bc20ed481eac191ef1dde46462",
            "dd12c3b4a1cc49a48db17e371dd8ee51",
            "c6485be003df4d14ac1716145103bf83",
            "c381531c50984a1ab07a38f4c45852b5",
            "d941814e38d04063b7c7e8aad1a01483",
            "5b078eacbf92429dbecdcc336270e859",
            "9d138828ee6c47e292c6be3c660607a1",
            "0e7ad0287b2442409e8a9db4150fd636",
            "cc3e550d35ac48e9bacb64e14ceb3567",
            "5cb78e5d55d0455b91faef6975844239",
            "98caa05f1f814c37a65867fe7aa859fa",
            "7eb762984d614b8891f3788ee138d5b2",
            "b47d6d875c0140efb869a7a2a24c45aa",
            "4e5eeddeaa5b448db1bb9bc6e6f7633d"
          ]
        },
        "id": "eDG0oLDaswu9",
        "outputId": "02e4c55b-a3ab-40b1-9afd-c44a1ca68d51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating bm25 retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f327aa1fdca4907a6a44bea202c8b32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bm25 retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.880\n",
            "  Recall@1: 0.830\n",
            "  Recall@5: 0.950\n",
            "  Precision@1: 0.830\n",
            "  Precision@5: 0.216\n",
            "  latency_mean_s: 0.033\n",
            "  latency_median_s: 0.033\n",
            "  Total misses: 5 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 ویکسین کا بنیادی مقصد کیا ہے؟\n",
            "    Positives: ['p0007']\n",
            "    Retrieved top ids: ['p0028', 'p0050', 'p0051', 'p0027', 'p0039']\n",
            "   Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            "    Positives: ['p0020']\n",
            "    Retrieved top ids: ['p0017', 'p0060', 'p0031', 'p0048', 'p0027']\n",
            "   Query: ویکسین سائیڈ ایفیکٹس کی نگرانی کیسے کی جاتی ہے؟\n",
            "    Positives: ['p0039']\n",
            "    Retrieved top ids: ['p0058', 'p0040', 'p0032', 'p0051', 'p0011']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating dense retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8deb66a5eeac40018882b5327b3e066e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dense retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.857\n",
            "  Recall@1: 0.780\n",
            "  Recall@5: 0.940\n",
            "  Precision@1: 0.780\n",
            "  Precision@5: 0.214\n",
            "  latency_mean_s: 0.029\n",
            "  latency_median_s: 0.030\n",
            "  Total misses: 6 / 100. Showing up to 3 misses:\n",
            "   Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            "    Positives: ['p0020']\n",
            "    Retrieved top ids: ['p0005', 'p0035', 'p0043', 'p0057', 'p0022']\n",
            "   Query: ویکسین کی افادیت وقت کے ساتھ کیوں کم ہو سکتی ہے؟\n",
            "    Positives: ['p0032']\n",
            "    Retrieved top ids: ['p0008', 'p0003', 'p0025', 'p0045', 'p0052']\n",
            "   Query: ویکسین کے خلاف جھجک کم کرنے کے عملی طریقے کیا ہیں؟\n",
            "    Positives: ['p0037', 'p0056']\n",
            "    Retrieved top ids: ['p0007', 'p0008', 'p0052', 'p0060', 'p0011']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_interleave retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7562cabb7c64473fa0e8242e62f9264a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_interleave retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.857\n",
            "  Recall@1: 0.780\n",
            "  Recall@5: 0.940\n",
            "  Precision@1: 0.780\n",
            "  Precision@5: 0.214\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 6 / 100. Showing up to 3 misses:\n",
            "   Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            "    Positives: ['p0020']\n",
            "    Retrieved top ids: ['p0005', 'p0035', 'p0043', 'p0057', 'p0022']\n",
            "   Query: ویکسین کی افادیت وقت کے ساتھ کیوں کم ہو سکتی ہے؟\n",
            "    Positives: ['p0032']\n",
            "    Retrieved top ids: ['p0008', 'p0003', 'p0025', 'p0045', 'p0052']\n",
            "   Query: ویکسین کے خلاف جھجک کم کرنے کے عملی طریقے کیا ہیں؟\n",
            "    Positives: ['p0037', 'p0056']\n",
            "    Retrieved top ids: ['p0007', 'p0008', 'p0052', 'p0060', 'p0011']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_score retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c22868ced3b40e3a6f32c0cf7d86522"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_score retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.928\n",
            "  Recall@1: 0.860\n",
            "  Recall@5: 1.000\n",
            "  Precision@1: 0.860\n",
            "  Precision@5: 0.230\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 0 / 100. Showing up to 3 misses:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_rrf retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c381531c50984a1ab07a38f4c45852b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_rrf retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.894\n",
            "  Recall@1: 0.820\n",
            "  Recall@5: 0.990\n",
            "  Precision@1: 0.820\n",
            "  Precision@5: 0.228\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 1 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 ویکسین کا بنیادی مقصد کیا ہے؟\n",
            "    Positives: ['p0007']\n",
            "    Retrieved top ids: ['p0051', 'p0033', 'p0039', 'p0028', 'p0040']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6c: Report top-3 from current fine-tuned dense retriever (no re-init)\n",
        "print(\"Dense top-3 (fine-tuned):\", dense_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ],
      "metadata": {
        "id": "pE9Ao8BDO9ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9b (fixed): Validation diagnostics — Recall@1 and Recall@5 on validation examples for Dense Model only\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Map passage text back to IDs\n",
        "text2pid = {p[\"text\"]: p[\"id\"] for p in passages_min}\n",
        "\n",
        "# Build validation pairs (query, gold passage ID)\n",
        "val_pairs = []\n",
        "for ex in val_examples[:200]:  # cap to 200 for speed; remove cap for full set\n",
        "    q = ex.texts[0]\n",
        "    pos_text = ex.texts[1]\n",
        "    gold_pid = text2pid.get(pos_text)\n",
        "    if gold_pid:\n",
        "        val_pairs.append((q, gold_pid))\n",
        "\n",
        "def recall_at_k_pairs(pairs, k=1, mode=\"dense\"):\n",
        "    hits = 0\n",
        "    for q, gold_pid in tqdm(pairs):\n",
        "        # retrieve(...) returns (pid, text, score) tuples; take the first element as pid\n",
        "        retrieved = [r[0] for r in retrieve(q, k=k, mode=mode)]\n",
        "        if gold_pid in retrieved:\n",
        "            hits += 1\n",
        "    return hits / len(pairs) if pairs else 0.0\n",
        "\n",
        "r1 = recall_at_k_pairs(val_pairs, k=1, mode=\"dense\")\n",
        "r5 = recall_at_k_pairs(val_pairs, k=5, mode=\"dense\")\n",
        "print(f\"Validation Recall@1 (dense): {r1:.3f}\")\n",
        "print(f\"Validation Recall@5 (dense): {r5:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "9908ee0586474e7d877138631a1b7c03",
            "2f051019a1274f72bbbd6487be30c82b",
            "56b9fb59efc543fc8d61ce8e8433bd30",
            "56610cbae5414fdf9fe09b437e56b17e",
            "05631134d28a477fb7fca9bbbb8b67d3",
            "c6cdaa0e077648e2a40899a903dce16d",
            "b023a7e95f9441bc86de65899114f5cf",
            "17303282ef17463bb06eb7ea3d8114ac",
            "34d50f1fb4ad425b82416aed5cea9f6f",
            "4b91c8defda3467389d73ac33dad7d7e",
            "4684aea4698140d6a9a4b91b681ca3d1",
            "61f0ed3664144861bca51a44fe27c6e5",
            "a31b65a8b55a46e3ae22ab22ca38a704",
            "34f299db83ee427a8a78a9f569881cbc",
            "6c69570276894f1c8565d26b33deb42b",
            "8fa2038a57ea468bb038d338e12cb54e",
            "d45d57a7b8cb45f59c16f1ede2d30ad3",
            "b519f3f49b654d0785971fd29f991aac",
            "74f2c7364ff14344a7aabe1bba64535e",
            "fb0affd4260f4365adc97b4bb0918501",
            "2a1a16a6f78845bb9223c16d5c48e9db",
            "ac27ef2243ba40469fb760785d00aa34"
          ]
        },
        "id": "a0mlSvToMGTy",
        "outputId": "fb069717-fb62-4d59-8baf-ab06e4fb6f8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9908ee0586474e7d877138631a1b7c03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61f0ed3664144861bca51a44fe27c6e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Recall@1 (dense): 0.650\n",
            "Validation Recall@5 (dense): 0.970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional Cell:\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedder = SentenceTransformer(\"fine_tuned_sbert_urdu\").to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "68qlwuw-Pxff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional cell - only need to be run if generator model not present as unzipped in drive\n",
        "\n",
        "# 1. Mount Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Paths — update to your actual Drive path\n",
        "DRIVE_ZIP = \"/content/drive/MyDrive/mbart_urdu_covid.zip\"   # path to the zip in Drive\n",
        "UNZIP_TARGET = \"/content/drive/MyDrive/fine_tuned_mbart_urdu\"  # folder to create in Drive\n",
        "\n",
        "# 3. Make sure target folder does not already exist (optional safety)\n",
        "import os, shutil\n",
        "if os.path.exists(UNZIP_TARGET):\n",
        "    print(\"Warning: target folder already exists:\", UNZIP_TARGET)\n",
        "else:\n",
        "    # 4. Copy zip into runtime (optional) and unzip directly into Drive\n",
        "    !unzip -q \"{DRIVE_ZIP}\" -d \"{UNZIP_TARGET}\"\n",
        "    print(\"Unzipped to:\", UNZIP_TARGET)\n",
        "\n",
        "# 5. List files to confirm\n",
        "for root, dirs, files in os.walk(UNZIP_TARGET):\n",
        "    print(root)\n",
        "    print(\"  dirs:\", dirs)\n",
        "    print(\"  files:\", files[:10])\n",
        "    break\n"
      ],
      "metadata": {
        "id": "LY9ZXMpRXjQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGSNY-smGM7j",
        "outputId": "160518e6-4d61-444e-b1e3-f4de12dd5660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MBART generator from: /content/drive/MyDrive/fine_tuned_mbart_urdu/fine_tuned_mbart_urdu\n",
            "✅ MBART loaded on cuda. Tokenizer and model ready for generation.\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Load fine-tuned MBART generator from Drive (or local path)\n",
        "# - Paste this cell into your RAG notebook before the RAG inference cell.\n",
        "# - Assumes you have mounted Google Drive earlier in the notebook:\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive')\n",
        "# - Replace `MBART_ARCHIVE_PATH` with the folder path that contains the saved model\n",
        "#   (the folder should contain config.json, model.safetensors, tokenizer files, etc.)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import MBartForConditionalGeneration, MBart50Tokenizer # Changed to MBart50Tokenizer\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "# Path to the unzipped fine-tuned model folder (update to your Drive path)\n",
        "# Corrected path to the nested directory where the model files are actually located\n",
        "MBART_ARCHIVE_PATH = \"/content/drive/MyDrive/fine_tuned_mbart_urdu/fine_tuned_mbart_urdu\"  # <- Corrected path\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------- Load tokenizer and model ----------\n",
        "print(f\"Loading MBART generator from: {MBART_ARCHIVE_PATH}\")\n",
        "tokenizer = MBart50Tokenizer.from_pretrained(MBART_ARCHIVE_PATH) # Changed to MBart50Tokenizer\n",
        "# Ensure tokenizer uses Urdu language code for both source and target\n",
        "tokenizer.src_lang = \"ur_PK\"\n",
        "tokenizer.tgt_lang = \"ur_PK\"\n",
        "\n",
        "gen_model = MBartForConditionalGeneration.from_pretrained(MBART_ARCHIVE_PATH).to(device)\n",
        "gen_model.eval()\n",
        "\n",
        "# Optional: set forced BOS token to ensure Urdu generation if not already set\n",
        "if \"ur_PK\" in tokenizer.lang_code_to_id:\n",
        "    gen_model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"ur_PK\"]\n",
        "\n",
        "print(f\"✅ MBART loaded on {device}. Tokenizer and model ready for generation.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10b (memory-optimized): Fine-tune MBART generator for RAG-style inputs\n",
        "# - Uses eval_queries.jsonl as training data\n",
        "# - Saves fine-tuned model to OUTPUT_DIR on Drive\n",
        "# - Adjusted for Colab GPU memory limits\n",
        "\n",
        "import os, json, torch, gc\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "# Clear GPU cache before training\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ---------- Config ----------\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/data\")\n",
        "TRAIN_JSONL = DATA_DIR / \"eval_queries.jsonl\"\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/models/mbart_rag_finetuned\")\n",
        "\n",
        "# Smaller lengths to reduce memory\n",
        "MAX_INPUT_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 64\n",
        "BATCH_SIZE = 1              # keep tiny batch size\n",
        "EPOCHS = 2                  # fewer epochs to fit memory\n",
        "LEARNING_RATE = 2e-5\n",
        "GRAD_ACCUM_STEPS = 8        # simulate larger batch via accumulation\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'tokenizer' in globals() and 'gen_model' in globals(), \"tokenizer and gen_model must be loaded (Cell 10)\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Training device:\", device)\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "gen_model.gradient_checkpointing_enable()\n",
        "\n",
        "# ---------- Load training examples ----------\n",
        "train_examples = []\n",
        "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            train_examples.append(json.loads(line))\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "assert len(train_examples) > 0, \"No training examples found in eval_queries.jsonl\"\n",
        "\n",
        "# ---------- Helper to build RAG-style input text ----------\n",
        "def build_rag_input(example, top_k=3, mode=\"hybrid_score\"):\n",
        "    q = example.get(\"query\") or example.get(\"question\") or example.get(\"q\")\n",
        "    hits = retrieve(q, k=top_k, mode=mode)\n",
        "    passages = [f\"[حوالہ] {ptext}\" for _, ptext, _ in hits]\n",
        "    context = \"\\n\\n\".join(passages)\n",
        "    instruction = \"حوالہ شدہ معلومات کی بنیاد پر مختصر اور درست جواب لکھیں۔\"\n",
        "    return f\"سوال: {q}\\n\\nحوالہ شدہ معلومات:\\n{context}\\n\\n{instruction}\"\n",
        "\n",
        "# ---------- Build dataset ----------\n",
        "inputs, targets = [], []\n",
        "for ex in train_examples:\n",
        "    inp = build_rag_input(ex, top_k=3, mode=\"hybrid_score\")\n",
        "    tgt = ex.get(\"gold_answer\") or \"\"\n",
        "    if not tgt:\n",
        "        continue\n",
        "    inputs.append(inp)\n",
        "    targets.append(tgt)\n",
        "\n",
        "assert len(inputs) > 0, \"No valid (input,target) pairs constructed\"\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    model_inputs = tokenizer(batch[\"input\"], max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(batch[\"target\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\", text_target=batch[\"target\"])\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "hf_ds = Dataset.from_dict({\"input\": inputs, \"target\": targets})\n",
        "hf_ds = hf_ds.map(tokenize_batch, batched=True, remove_columns=[\"input\", \"target\"])\n",
        "\n",
        "# ---------- Training arguments ----------\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    predict_with_generate=False,\n",
        "    remove_unused_columns=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=gen_model, label_pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# ---------- Trainer ----------\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=gen_model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting fine-tuning (memory-optimized).\")\n",
        "trainer.train()\n",
        "\n",
        "# ---------- Save fine-tuned model ----------\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "trainer.save_model(str(OUTPUT_DIR))\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "print(f\"Fine-tuned generator saved to {OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "70f48ceedad348c49e30a8d0598e08b9",
            "eef606d1343e455ea5b0a7962edb4496",
            "eb57f949d4744473b9e4e7b84a9e0103",
            "57a536e40f1e4f36b2d8e844b22741cd",
            "bafb411358af40fd94a561e030ce4b45",
            "110f0dbd40cc449a849c4563eb2aa8db",
            "a444657a1f6b41a9959d37d5fecc486e",
            "c2d566c2532d44499037d8f75f85f3ec",
            "6c40bd729dca42cdb94e9c9eb8b1f09c",
            "c21cab86aa7243bda4809a5275658498",
            "4f768fb6fccc4cd3b6186e641031bc9f"
          ]
        },
        "id": "o9KPBjPbvN20",
        "outputId": "760d33a3-e035-4084-9f67-3a68cf2cbd38"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70f48ceedad348c49e30a8d0598e08b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1172588676.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning (memory-optimized).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [26/26 05:56, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 250049}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned generator saved to /content/drive/MyDrive/models/mbart_rag_finetuned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10c: Evaluate fine-tuned generator alone (no retrieval context)\n",
        "# - Loads eval queries and gold answers\n",
        "# - Generates answers directly from the fine-tuned MBART model\n",
        "# - Computes BLEU, chrF, token-level F1, latency\n",
        "# - Provides summary metrics for comparison with RAG and retrievers\n",
        "\n",
        "import time, statistics\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'tokenizer' in globals() and 'gen_model' in globals(), \"Fine-tuned generator must be loaded (Cell 10b)\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "\n",
        "# ---------- Helper: token-level F1 ----------\n",
        "def token_f1(pred, refs):\n",
        "    def toks(s):\n",
        "        return [t for t in s.strip().split() if t]\n",
        "    pred_t = set(toks(pred))\n",
        "    if not refs:\n",
        "        return 0.0\n",
        "    best = 0.0\n",
        "    for r in refs:\n",
        "        ref_t = set(toks(r))\n",
        "        if not ref_t:\n",
        "            continue\n",
        "        tp = len(pred_t & ref_t)\n",
        "        prec = tp / max(1, len(pred_t))\n",
        "        rec = tp / max(1, len(ref_t))\n",
        "        f1 = 0.0 if (prec + rec) == 0 else (2 * prec * rec) / (prec + rec)\n",
        "        best = max(best, f1)\n",
        "    return best\n",
        "\n",
        "# ---------- Build references ----------\n",
        "def get_references_for_item(item):\n",
        "    refs = []\n",
        "    if \"gold_answer\" in item and item[\"gold_answer\"]:\n",
        "        refs = [item[\"gold_answer\"]]\n",
        "    elif \"answers\" in item and item[\"answers\"]:\n",
        "        val = item[\"answers\"]\n",
        "        if isinstance(val, str):\n",
        "            refs = [val]\n",
        "        elif isinstance(val, list):\n",
        "            refs = [x for x in val if isinstance(x, str) and x.strip()]\n",
        "    return [r.strip() for r in refs if r and r.strip()]\n",
        "\n",
        "# ---------- Evaluation loop ----------\n",
        "preds = []\n",
        "refs_all = []\n",
        "f1s = []\n",
        "latencies = []\n",
        "\n",
        "for it in eval_queries:\n",
        "    q = it[\"query\"]\n",
        "    refs = get_references_for_item(it)\n",
        "\n",
        "    # Build simple prompt: question only (no retrieval context)\n",
        "    input_text = f\"سوال: {q}\\n\\nبراہ کرم مختصر اور درست جواب دیں۔\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=256, truncation=True).to(gen_model.device)\n",
        "\n",
        "    t0 = time.time()\n",
        "    outputs = gen_model.generate(**inputs, max_length=64, num_beams=4)\n",
        "    latency = time.time() - t0\n",
        "    latencies.append(latency)\n",
        "\n",
        "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    preds.append(pred)\n",
        "    refs_all.append(refs if refs else [\"\"])\n",
        "    f1s.append(token_f1(pred, refs))\n",
        "\n",
        "# ---------- BLEU / chrF ----------\n",
        "max_refs = max(len(r) for r in refs_all) if refs_all else 1\n",
        "ref_sets = []\n",
        "for i in range(max_refs):\n",
        "    ref_sets.append([ (refs[i] if i < len(refs) else \"\") for refs in refs_all ])\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(preds, ref_sets)\n",
        "chrf = sacrebleu.corpus_chrf(preds, ref_sets)\n",
        "\n",
        "# ---------- Summary ----------\n",
        "summary_gen = {\n",
        "    \"BLEU\": float(bleu.score),\n",
        "    \"chrF\": float(chrf.score),\n",
        "    \"F1_mean\": float(np.mean(f1s)) if f1s else 0.0,\n",
        "    \"F1_median\": float(statistics.median(f1s)) if f1s else 0.0,\n",
        "    \"latency_mean_s\": float(np.mean(latencies)) if latencies else 0.0,\n",
        "    \"latency_median_s\": float(statistics.median(latencies)) if latencies else 0.0,\n",
        "    \"n\": len(eval_queries)\n",
        "}\n",
        "\n",
        "print(\"\\n=== Generator-only Evaluation (no retrieval) ===\")\n",
        "for k,v in summary_gen.items():\n",
        "    print(f\"- {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jedq7xUz7dK",
        "outputId": "b91e9bb4-e587-4316-e5c1-216d68262532"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\n",
            "Caching is incompatible with gradient checkpointing in MBartDecoderLayer. Setting `past_key_values=None`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generator-only Evaluation (no retrieval) ===\n",
            "- BLEU: 0.0238927947466256\n",
            "- chrF: 3.3246001371286993\n",
            "- F1_mean: 0.07335472491045246\n",
            "- F1_median: 0.10818713450292397\n",
            "- latency_mean_s: 2.434983456134796\n",
            "- latency_median_s: 2.2022024393081665\n",
            "- n: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional to clear GPU cache:\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "o9wns6MjwdVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional cell to just check if our imported generative model above performs fine independantly.\n",
        "# Expectation: Just generate a fluent urdu answer since this is just a smoke test.\n",
        "\n",
        "prompt = \"کووڈ-19 کی عام علامات کیا ہیں؟\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    output_ids = gen_model.generate(\n",
        "        **inputs,\n",
        "        max_length=64,\n",
        "        num_beams=4,\n",
        "        length_penalty=1.0\n",
        "    )\n",
        "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "mKPApZ23P6tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LfCZMDk5Ivj_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739,
          "referenced_widgets": [
            "8be8940b76c54d3e8bdbc338892d5d02",
            "ff51fe7e6016467380b55faf52fe0fc5",
            "1e64b677afcb48b29dfeb5a5ac240a29",
            "c05acdb5bc5a413eac0267a78d5b3219",
            "fe511e98b53f4b00b1032c2ce2323e49",
            "01b64c3e5f354c62964997a2a585f54a",
            "784fe708df5d454b9e32a92e47cfdd0b",
            "f90793414ae34eebacbac579cb50c33e",
            "62ec9615088441f3bde4e97570d8c1af",
            "e6ae03da6fd84904b151cdaf4723b820",
            "9748dd545fac4f948f73f9e24e800329",
            "a14e4c34734a445f94219bcaf29e1cca",
            "5e9c0b1a80a74c368ffb3de8085fedca",
            "0393a9ec76d64e6db3d18a9f6ff43a76",
            "7fd3ea3fc92c48a2b81acd97bcf4f2a6",
            "ed04769d488f40658795effa4f774984",
            "21ea658a2e824c779e86c2ec2531ba12",
            "1da82a26e0eb4a6b8b0005dec4c6e1a8",
            "b8c67abc2f8d43e5b033a949503b22fd",
            "c86ef96173be41dd9f014f2277b59feb",
            "9fd319b1f7534ae7af4b70d379d38d96",
            "b091add7eb544d6fa4b8053737e6d974",
            "cc1c990921fa4f32a4ff71bfa2a76fa7",
            "c89bf516e3e84bcf81afa311aeec803d",
            "4f26ff8fcba048f5804f0d28c9c409dd",
            "25a8eff61c594487a9821cdc939d4240",
            "d7097ff677c04b2bb0358d031faf7dd9",
            "4fc7f218dc1f4d8e951fe5c9226f8f0b",
            "7db99d131f014f25ac391ea42635642f",
            "4bb528a4191a48b1821c149bc62c2915",
            "eec65943ebd5432ba842157a73d50c4d",
            "95335374c22247cabb9f1367c3503261",
            "a4d653b311824c6f84022e108dff9aa0",
            "bbd218a45c2b4128ac6a1d67f6154445",
            "4efb82b9cdc244b0a82a881caf07d207",
            "ecf7d5bc665c4c16829f9ea0c3ae862a",
            "7b24bf783c944175a3272be88239bf73",
            "40a7ce9dfc164b91ae39ddceeceb6462",
            "44491cea477b461598e6e5677521d61f",
            "1d79c30d7c2c4765bbe47ad42281648e",
            "556064a72d7944d498eb8e0822e10e14",
            "64f628f75631409fa833177821449a43",
            "18a744359b614ddc97cd28d21095e5a5",
            "30e5d77a9fa14dadb55f83b8646f1b8d",
            "ea55395bcdd84ed792a1faeb06bcec28",
            "b05d959b58bd4ce08a603560bf8304e7",
            "dd871980091d4797accc8148948893ba",
            "6a9d53a49c6b4ad088da7766d9cf6021",
            "498e513a68dc4626a7374e2863463f62",
            "1091c004ea294a1fa97c3d601c36897d",
            "b889c362b80f4f728fd3b933ffa20aa9",
            "7208568f4461429694ba520ce39a26aa",
            "6b8cadd62a464a48ad09cf7bdd7e9914",
            "5289587b44764311a9729b8682cdc494",
            "537911192a0048daa8881eefe5aa212c",
            "8585c33aab2c488db22390e2df422bcd",
            "64ca3a5a1acd4e37bd1764586e6dc8db",
            "ac7a667116bc4b108a86ff260788744c",
            "2852504aaabf49d3a22732469245c4f9",
            "d5b9756af2874cfd874d7c29e0714dba",
            "6a3b76ac60834879b0971481a8931910",
            "378b9ef1adad40b3b744a6a905dc3b8c",
            "21b0b0401019459b90e43e61b197879f",
            "74c5d8a9a01f426eac00da633dbf712b",
            "4351d7f3f45c4bf18ffb9739906f3178",
            "640f3fcd03924c6a9a1ff273d28f1070",
            "c9f76c5c65c54a548e2e55df7f117cf9",
            "d5c3b4398ed04d7fa95ff49a96a16381",
            "7bbeafff617f447aa85e18fafe710cf6",
            "56a1f71bfefd4cfa98f8679a99ee745b",
            "7b1eca1a6b2a4fe2bf3ec0d3cb6ba5f5",
            "7768e46f48e34e92ac62f8807eca6f10",
            "c52a5e372797435a9c950cc36bf2e49b",
            "adc3ab643a394608814c5023c4a850ea",
            "7334f0b8edae453c8f703db7b4498f93",
            "d486248b2bae47cc9e6253499006e789",
            "b18902b57c4b469aaaa71332d69a58c3",
            "82e62cbc5e5d466cba073f98eb687596",
            "d17d25943d254b31ad1f39f0f2a3ca9c",
            "1d1a28f0a5fa41f6b1d7d8a11ac96a7a",
            "8cdb33947e194d9083d6af3151df8458",
            "123c4ff705e14cceb7b44cd793e85ecd",
            "70bf6012dceb4d41a92859391c0024fe",
            "b8df89336fa74965895726309bea4fe4",
            "bcf76466da6f4c7d8173f38695f5ddd2",
            "51a04eb4216243ccb6a85e7896d06601",
            "b8feca218a8c45dbb6b9008875ca219f",
            "255975a25aec430d92d3e94ed76cfb14",
            "eabfffeb90c941ffa5280bdfcddd4c99",
            "9006efbda0254ae79e28aff9854b5b22",
            "a76ef5d8b2d64e5fb5cefa0bfd2c2b4c",
            "f1211ee5fcfd4620bfe61e8500bab19a",
            "110d92e47f7345f887d721fce6dd385c",
            "f763191a4e834092965bc0e0858a2d13",
            "19a84a676107429085cc132930a45217",
            "e44042f042c9499c961953f2e58bd754",
            "4be71d2fa13543828e8aa920860de46b",
            "d1c9ba49e4424e7faea25d63a8b64a79",
            "384b831316414bcfbad1fbef0ab7d1dd"
          ]
        },
        "outputId": "6cc53fdd-e9e2-4e3d-b6f7-6cdafe9ad9cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer from base: facebook/mbart-large-50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8be8940b76c54d3e8bdbc338892d5d02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a14e4c34734a445f94219bcaf29e1cca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc1c990921fa4f32a4ff71bfa2a76fa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbd218a45c2b4128ac6a1d67f6154445"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fine-tuned generator weights from: /content/drive/MyDrive/models/mbart_rag_finetuned\n",
            "Fine-tuned generator loaded. src_lang=ur_PK, forced_bos_token_id=None\n",
            "Loaded _CORPUS_DOCS with 60 items.\n",
            "Loading extractive QA model: deepset/xlm-roberta-base-squad2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea55395bcdd84ed792a1faeb06bcec28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8585c33aab2c488db22390e2df422bcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9f76c5c65c54a548e2e55df7f117cf9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82e62cbc5e5d466cba073f98eb687596"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eabfffeb90c941ffa5280bdfcddd4c99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extractive QA model loaded successfully.\n",
            "DEBUG: Explicitly pre-pending p0001 for symptom query.\n",
            "Answer: عام علامات میں بخار، کھانسی اور سانس لینے میں مشکل شامل ہیں؛۔\n",
            "Used fallback: False Generator failed: False Intent: symptoms\n",
            "\n",
            "--- Debug: Top retrieved (raw) ---\n",
            "1. [p0001] کورونا وائرس مرض 2019 (COVID-19) ایک متعدی بیماری ہے جس کی عام علامات میں بخار، کھانسی اور سانس لینے میں دشواری شامل ہیں۔\n",
            "2. [p0024] بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔\n",
            "3. [p0008] ویکسین کے عام مضر اثرات میں انجیکشن سائٹ پر درد، ہلکا بخار اور تھکن شامل ہو سکتے ہیں جو عموماً چند دنوں میں ختم ہو جاتے ہیں؛ سنگین ضمنی اثرات نایاب ہیں۔\n",
            "4. [p0044] کووڈ-19 کے مریضوں کے لیے ریہیب پروگرامز طویل علامات کے انتظام میں مدد دیتے ہیں، جیسے سانس کی ورزشیں اور توانائی کی بحالی کے پروگرام۔\n",
            "5. [p0021] کووڈ-19 کے بعد بعض افراد میں طویل مدتی علامات (Long COVID) جیسے تھکن، سانس کی تکلیف اور دماغی دھند برقرار رہ سکتی ہیں؛ ریہیب پروگرامز مدد دیتے ہیں۔\n",
            "\n",
            "--- Debug: Filtered context used for generation ---\n",
            "1. [p0001] کورونا وائرس مرض 2019 (COVID-19) ایک متعدی بیماری ہے جس کی عام علامات میں بخار، کھانسی اور سانس لینے میں دشواری شامل ہیں۔\n",
            "2. [p0024] بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔\n",
            "3. [p0008] ویکسین کے عام مضر اثرات میں انجیکشن سائٹ پر درد، ہلکا بخار اور تھکن شامل ہو سکتے ہیں جو عموماً چند دنوں میں ختم ہو جاتے ہیں؛ سنگین ضمنی اثرات نایاب ہیں۔\n",
            "4. [p0044] کووڈ-19 کے مریضوں کے لیے ریہیب پروگرامز طویل علامات کے انتظام میں مدد دیتے ہیں، جیسے سانس کی ورزشیں اور توانائی کی بحالی کے پروگرام۔\n",
            "5. [p0021] کووڈ-19 کے بعد بعض افراد میں طویل مدتی علامات (Long COVID) جیسے تھکن، سانس کی تکلیف اور دماغی دھند برقرار رہ سکتی ہیں؛ ریہیب پروگرامز مدد دیتے ہیں۔\n",
            "\n",
            "--- Debug: Rescued hits (corpus scan) ---\n"
          ]
        }
      ],
      "source": [
        "# Cell 11 (final, fixed): RAG inference — hybrid_score fusion + fine-tuned generator\n",
        "# - Loads tokenizer from base MBART (\"facebook/mbart-large-50\") for robust config\n",
        "# - Loads fine-tuned generator weights from Drive path\n",
        "# - Sets Urdu language codes (src_lang, forced_bos_token_id)\n",
        "# - Uses retrieve(..., mode=\"hybrid_score\") by default\n",
        "# - Strict grounding, intent-aware expansion, corpus rescue, extractive QA fallback\n",
        "\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'retrieve' in globals(), \"retrieve(...) must be defined (Cell 9)\"\n",
        "assert 'pid2text' in globals(), \"pid2text must be loaded\"\n",
        "assert 'passages_min' in globals() and len(passages_min) > 0, \"passages_min must be loaded\"\n",
        "\n",
        "# ---------- Device ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------- Load tokenizer (base) and fine-tuned generator (weights) ----------\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "\n",
        "GEN_PATH = \"/content/drive/MyDrive/models/mbart_rag_finetuned\"\n",
        "BASE_TOKENIZER = \"facebook/mbart-large-50\"\n",
        "print(f\"Loading tokenizer from base: {BASE_TOKENIZER}\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(BASE_TOKENIZER)\n",
        "\n",
        "print(f\"Loading fine-tuned generator weights from: {GEN_PATH}\")\n",
        "gen_model = MBartForConditionalGeneration.from_pretrained(GEN_PATH).to(device)\n",
        "gen_model.eval()\n",
        "\n",
        "# Set Urdu language codes for MBART\n",
        "# Use 'ur_PK' (Urdu, Pakistan) if available; fallback to 'ur_IN' if needed\n",
        "src_lang = \"ur_PK\" if \"ur_PK\" in tokenizer.lang_code_to_id else \"ur_IN\"\n",
        "tokenizer.src_lang = src_lang\n",
        "if hasattr(tokenizer, \"lang_code_to_id\"):\n",
        "    forced_bos_id = tokenizer.lang_code_to_id.get(src_lang)\n",
        "    if forced_bos_id is not None:\n",
        "        # Configure BOS token for generation\n",
        "        if hasattr(gen_model, \"generation_config\"):\n",
        "            gen_model.generation_config.forced_bos_token_id = forced_bos_id\n",
        "        else:\n",
        "            gen_model.config.forced_bos_token_id = forced_bos_id\n",
        "\n",
        "print(f\"Fine-tuned generator loaded. src_lang={src_lang}, forced_bos_token_id={getattr(gen_model.config, 'forced_bos_token_id', None)}\")\n",
        "\n",
        "# ---------- Config ----------\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/data\")\n",
        "CORPUS_JSONL_PATH = DATA_DIR / \"urdu_covid_corpus_clean.jsonl\"\n",
        "DEFAULT_RETRIEVE_K = 50      # larger candidate pool for rescue/rerank\n",
        "FINAL_CONTEXT_K = 5          # passages kept for generation\n",
        "DEFAULT_RETRIEVE_MODE = \"hybrid_score\"  # use true fusion by default\n",
        "\n",
        "SYMPTOM_EXPANSION = \" علامات بخار کھانسی سانس ذائقہ بو تھکن\"\n",
        "SYMPTOM_TOKENS = {t.lower() for t in [\"بخار\",\"کھانسی\",\"سانس\",\"علامات\",\"ذائقہ\",\"بو\",\"تھکن\",\"سانس لینے\",\"سانس پھولنا\",\"گلے\"]}\n",
        "\n",
        "# ---------- Load corpus (keyword rescue; read-only) ----------\n",
        "def load_corpus_jsonl(path: str):\n",
        "    docs = []\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    docs.append(json.loads(line))\n",
        "                except Exception:\n",
        "                    continue\n",
        "    except FileNotFoundError:\n",
        "        docs = []\n",
        "    return docs\n",
        "\n",
        "_CORPUS_DOCS = load_corpus_jsonl(str(CORPUS_JSONL_PATH))\n",
        "print(f\"Loaded _CORPUS_DOCS with {len(_CORPUS_DOCS)} items.\")\n",
        "\n",
        "# ---------- Intent detection ----------\n",
        "INTENT_KEYWORDS = {\n",
        "    \"symptoms\": [\"علامات\", \"بخار\", \"کھانسی\", \"سانس\", \"تھکن\", \"ذائقہ\", \"بو\"],\n",
        "    \"diagnosis\": [\"PCR\", \"RT-PCR\", \"اینٹیجن\", \"سویب\", \"ٹیسٹ\", \"تشخیص\"],\n",
        "    \"prevention\": [\"ماسک\", \"ہاتھ\", \"فاصلے\", \"سینیٹائزر\", \"صفائی\"],\n",
        "    \"vaccination\": [\"ویکسین\", \"ٹیکہ\", \"بوسٹر\", \"ڈوز\"]\n",
        "}\n",
        "\n",
        "def infer_intent(query: str) -> str:\n",
        "    q = query.lower()\n",
        "    if any(k.lower() in q for k in INTENT_KEYWORDS[\"diagnosis\"]): return \"diagnosis\"\n",
        "    if \"علامات\" in q or any(k.lower() in q for k in INTENT_KEYWORDS[\"symptoms\"]): return \"symptoms\"\n",
        "    if any(k.lower() in q for k in INTENT_KEYWORDS[\"vaccination\"]): return \"vaccination\"\n",
        "    if any(k.lower() in q for k in INTENT_KEYWORDS[\"prevention\"]): return \"prevention\"\n",
        "    return \"general\"\n",
        "\n",
        "def expanded_query_for_intent(query: str, intent: str) -> str:\n",
        "    if intent == \"symptoms\":\n",
        "        return query + \" \" + SYMPTOM_EXPANSION\n",
        "    return query\n",
        "\n",
        "# ---------- Retrieval helpers ----------\n",
        "def dedupe_candidates(cands: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Accepts iterable items:\n",
        "      - (pid, text)\n",
        "      - (pid, text, score)\n",
        "      - any sequence with pid at [0] and text at [1]\n",
        "    Returns deduplicated list of (pid, text) preserving first-seen order.\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for item in cands:\n",
        "        try:\n",
        "            pid = item[0]\n",
        "            txt = item[1]\n",
        "        except Exception:\n",
        "            continue\n",
        "        if pid in seen:\n",
        "            continue\n",
        "        seen.add(pid)\n",
        "        out.append((pid, txt))\n",
        "    return out\n",
        "\n",
        "def filter_retrieved_by_intent(retrieved_candidates: List[Tuple[str, str]], intent: str, keep: int) -> List[Tuple[str, str]]:\n",
        "    if intent == \"general\":\n",
        "        return dedupe_candidates(retrieved_candidates)[:keep]\n",
        "    intent_keywords = INTENT_KEYWORDS.get(intent, [])\n",
        "    prioritized, other = [], []\n",
        "    for pid, text in retrieved_candidates:\n",
        "        (prioritized if any(keyword.lower() in text.lower() for keyword in intent_keywords) else other).append((pid, text))\n",
        "    return dedupe_candidates(prioritized + other)[:keep]\n",
        "\n",
        "# ---------- Keyword rescue ----------\n",
        "def corpus_keyword_rescue(docs, tokens, limit=10):\n",
        "    hits = []\n",
        "    for d in docs:\n",
        "        txt = d.get(\"text\", \"\").lower()\n",
        "        if any(tok in txt for tok in tokens):\n",
        "            hits.append((d.get(\"id\"), d.get(\"text\")))\n",
        "            if len(hits) >= limit:\n",
        "                break\n",
        "    return hits\n",
        "\n",
        "# ---------- Grounding utilities ----------\n",
        "INSTRUCTION_TOKENS = [\"براہ کرم\", \"جواب\", \"ہدایات\", \"instruction\", \"Answer:\", \"Response:\", \"حوالہ شدہ معلومات\"]\n",
        "\n",
        "def strip_instruction_echoes(text: str) -> str:\n",
        "    t = text.strip()\n",
        "    for tok in INSTRUCTION_TOKENS:\n",
        "        t = t.replace(tok, \"\")\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def content_words(s: str):\n",
        "    stop = {\"ہے\", \"ہیں\", \"میں\", \"کی\", \"کے\", \"اور\", \"سے\", \"پر\", \"کہ\", \"ہی\", \"بھی\"}\n",
        "    toks = [w for w in re.split(r\"\\W+\", s) if len(w) >= 3 and w not in stop]\n",
        "    return set(toks)\n",
        "\n",
        "def has_overlap_with_context(ans: str, context_texts):\n",
        "    ans_words = content_words(ans.lower())\n",
        "    ctx_words = set()\n",
        "    for txt in context_texts:\n",
        "        ctx_words |= content_words(txt.lower())\n",
        "    return len(ans_words & ctx_words) >= 2\n",
        "\n",
        "def looks_like_echo(query: str, ans: str) -> bool:\n",
        "    q = re.sub(r\"\\s+\", \" \", query).strip()\n",
        "    a = re.sub(r\"\\s+\", \" \", ans).strip()\n",
        "    shared = os.path.commonprefix([q, a])\n",
        "    long_overlap = len(shared) >= max(8, int(0.3 * len(q)))\n",
        "    starts_like_q = a.startswith(q[: max(12, len(q)//2)])\n",
        "    return long_overlap or starts_like_q\n",
        "\n",
        "def is_vague(ans: str) -> bool:\n",
        "    a = ans.strip()\n",
        "    if len(a) < 12:\n",
        "        return True\n",
        "    hedges = [\"منحصر\", \"ممکن\", \"عام طور\", \"ضروری\", \"اہم\", \"کامیابی\"]\n",
        "    return any(h in a for h in hedges) and len(a.split()) < 16\n",
        "\n",
        "# ---------- Generation helper (fine-tuned MBART) ----------\n",
        "def generate_answer_with_mbart(query, retrieved_passages, max_length=160, num_beams=6, intent=\"general\"):\n",
        "    def short(p, limit=350):\n",
        "        p = \" \".join(p.split())\n",
        "        return (p[:limit] + \"…\") if len(p) > limit else p\n",
        "\n",
        "    context = \"\\n\\n\".join([f\"[حوالہ] {short(p)}\" for _, p in retrieved_passages])\n",
        "\n",
        "    if intent == \"symptoms\":\n",
        "        instruction = (\n",
        "            \"صرف انہی حوالہ شدہ معلومات کی بنیاد پر علامات کی فہرست لکھیں۔ \"\n",
        "            \"اضافی مشورے یا عمومی صحت کی معلومات شامل نہ کریں۔ سوال کو دوبارہ نہ لکھیں۔\"\n",
        "        )\n",
        "    else:\n",
        "        instruction = (\n",
        "            \"صرف انہی حوالہ شدہ معلومات کی بنیاد پر مختصر اور درست جواب لکھیں۔ \"\n",
        "            \"اضافی معلومات شامل نہ کریں اور سوال کو دوبارہ نہ لکھیں۔\"\n",
        "        )\n",
        "\n",
        "    prompt = f\"سوال: {query}\\n\\nحوالہ شدہ معلومات:\\n{context}\\n\\n{instruction}\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = gen_model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            min_length=8,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=1.0,\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=False,\n",
        "            early_stopping=True\n",
        "        )\n",
        "    ans = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    ans = strip_instruction_echoes(ans)\n",
        "    if ans and not ans.endswith(\"۔\"):\n",
        "        ans += \"۔\"\n",
        "    return ans, prompt\n",
        "\n",
        "# ---------- Extractive QA fallback (multi-passage) ----------\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "qa_model_name = \"deepset/xlm-roberta-base-squad2\"\n",
        "print(f\"Loading extractive QA model: {qa_model_name}\")\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name).to(device)\n",
        "qa_model.eval()\n",
        "print(\"Extractive QA model loaded successfully.\")\n",
        "\n",
        "def extractive_answer_multi(query, passages, top_k=3, min_span_len=3, intent=\"general\"):\n",
        "    best_ans, best_score = \"\", float(\"-inf\")\n",
        "    for passage in passages[:top_k]:\n",
        "        inputs = qa_tokenizer(query, passage, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = qa_model(**inputs)\n",
        "        start_logits = outputs.start_logits[0]\n",
        "        end_logits = outputs.end_logits[0]\n",
        "        score = float(torch.max(start_logits).item() + torch.max(end_logits).item())\n",
        "        start = int(torch.argmax(start_logits).item())\n",
        "        end = int(torch.argmax(end_logits).item()) + 1\n",
        "        tokens = inputs[\"input_ids\"][0][start:end]\n",
        "        ans = qa_tokenizer.decode(tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        if not ans or len(ans.split()) < min_span_len:\n",
        "            continue\n",
        "        if intent == \"symptoms\":\n",
        "            lower = ans.lower()\n",
        "            if not any(tok in lower for tok in SYMPTOM_TOKENS):\n",
        "                continue\n",
        "\n",
        "        if score > best_score:\n",
        "            best_ans, best_score = ans, score\n",
        "    return best_ans.strip()\n",
        "\n",
        "# ---------- RAG wrapper ----------\n",
        "def rag_answer(query, k=5, mode=DEFAULT_RETRIEVE_MODE, metadata_filters=None, max_length=160, num_beams=6, debug=True):\n",
        "    t0 = time.time()\n",
        "    intent = infer_intent(query)\n",
        "    q_for_retrieval = expanded_query_for_intent(query, intent)\n",
        "\n",
        "    # Start with candidates; prepend p0001 for symptoms if available\n",
        "    initial_candidates = []\n",
        "    if intent == \"symptoms\" and ('pid2text' in globals()) and ('p0001' in pid2text):\n",
        "        p0001_text = pid2text['p0001']\n",
        "        initial_candidates.append(('p0001', p0001_text))\n",
        "        if debug:\n",
        "            print(\"DEBUG: Explicitly pre-pending p0001 for symptom query.\")\n",
        "\n",
        "    # Retrieve via fusion retriever\n",
        "    retrieve_k = DEFAULT_RETRIEVE_K if intent == \"symptoms\" else max(k, DEFAULT_RETRIEVE_K // 5)\n",
        "    retrieved = retrieve(q_for_retrieval, k=retrieve_k, mode=mode, **(metadata_filters or {}))\n",
        "    initial_candidates.extend(retrieved)\n",
        "\n",
        "    # Deduplicate and intent-filter\n",
        "    retrieved_raw = dedupe_candidates(initial_candidates)\n",
        "    retrieved_filtered = filter_retrieved_by_intent(retrieved_raw, intent, keep=FINAL_CONTEXT_K)\n",
        "\n",
        "    # Corpus rescue (symptoms only) if filtered lacks strong symptom signals\n",
        "    rescued_hits = []\n",
        "    if intent == \"symptoms\":\n",
        "        has_signal = any(any(tok in txt.lower() for tok in SYMPTOM_TOKENS) for _, txt in retrieved_filtered)\n",
        "        if not has_signal:\n",
        "            pool_hits = [(pid, txt) for pid, txt in retrieved_raw if any(tok in txt.lower() for tok in SYMPTOM_TOKENS)]\n",
        "            if pool_hits:\n",
        "                retrieved_filtered = dedupe_candidates(pool_hits + retrieved_filtered)[:FINAL_CONTEXT_K]\n",
        "            else:\n",
        "                rescued_hits = corpus_keyword_rescue(_CORPUS_DOCS, SYMPTOM_TOKENS, limit=10)\n",
        "                if rescued_hits:\n",
        "                    retrieved_filtered = dedupe_candidates(rescued_hits + retrieved_filtered)[:FINAL_CONTEXT_K]\n",
        "\n",
        "    # Empty context → safe return; no hallucinations\n",
        "    if not retrieved_filtered:\n",
        "        latency = time.time() - t0\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": \"سیاق میں متعلقہ معلومات دستیاب نہیں۔\",\n",
        "            \"provenance\": [],\n",
        "            \"latency\": latency,\n",
        "            \"used_fallback\": True,\n",
        "            \"generator_failed\": True,\n",
        "            \"intent\": intent,\n",
        "            \"debug\": {\n",
        "                \"retrieved_top\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in retrieved_raw[:k]],\n",
        "                \"retrieved_filtered\": [],\n",
        "                \"rescued_hits\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in rescued_hits],\n",
        "                \"prompt_context_preview\": \"\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Try generator\n",
        "    gen_ans = \"\"\n",
        "    generator_failed = False\n",
        "    used_fallback = False\n",
        "    try:\n",
        "        gen_ans, prompt_used = generate_answer_with_mbart(\n",
        "            query, retrieved_filtered, max_length=max_length, num_beams=num_beams, intent=intent\n",
        "        )\n",
        "    except Exception:\n",
        "        gen_ans = \"\"\n",
        "        generator_failed = True\n",
        "        prompt_used = \"\"\n",
        "\n",
        "    # Grounding heuristics\n",
        "    context_texts = [p for _, p in retrieved_filtered]\n",
        "    if (not gen_ans) or looks_like_echo(query, gen_ans) or is_vague(gen_ans) or not has_overlap_with_context(gen_ans, context_texts):\n",
        "        generator_failed = True\n",
        "\n",
        "    # Fallback to extractive QA\n",
        "    if generator_failed:\n",
        "        used_fallback = True\n",
        "        def rephrase_from_question(q, a): return a\n",
        "        span = extractive_answer_multi(query, context_texts, top_k=min(3, len(context_texts)), intent=intent)\n",
        "        final_ans = rephrase_from_question(query, span) if span else \"کوئی جواب نہیں ملا۔\"\n",
        "    else:\n",
        "        final_ans = gen_ans\n",
        "\n",
        "    latency = time.time() - t0\n",
        "\n",
        "    # Provenance\n",
        "    if 'corpus_clean' in globals():\n",
        "        meta_map = {p[\"id\"]: p for p in corpus_clean}\n",
        "    else:\n",
        "        meta_map = {p[\"id\"]: p for p in passages_min}\n",
        "    provenance = []\n",
        "    for pid, text in retrieved_filtered:\n",
        "        meta = meta_map.get(pid, {})\n",
        "        provenance.append({\"id\": pid, \"source\": meta.get(\"source\"), \"retrieved_at\": meta.get(\"retrieved_at\")})\n",
        "\n",
        "    # Debug\n",
        "    debug_block = None\n",
        "    if debug:\n",
        "        debug_block = {\n",
        "            \"retrieved_top\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in retrieved_raw[:min(10, len(retrieved_raw))]],\n",
        "            \"retrieved_filtered\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in retrieved_filtered],\n",
        "            \"rescued_hits\": [{\"id\": pid, \"text\": txt[:300]} for pid, txt in rescued_hits],\n",
        "            \"prompt_context_preview\": context_texts[0][:400] if context_texts else \"\",\n",
        "            \"prompt_used\": prompt_used\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": final_ans,\n",
        "        \"provenance\": provenance,\n",
        "        \"latency\": latency,\n",
        "        \"used_fallback\": used_fallback,\n",
        "        \"generator_failed\": generator_failed,\n",
        "        \"intent\": intent,\n",
        "        \"debug\": debug_block\n",
        "    }\n",
        "\n",
        "# ---------- Quick smoke test ----------\n",
        "res = rag_answer(\"کووڈ-19 کی عام علامات کیا ہیں؟\", k=5, mode=DEFAULT_RETRIEVE_MODE, debug=True)\n",
        "print(\"Answer:\", res[\"answer\"])\n",
        "print(\"Used fallback:\", res[\"used_fallback\"], \"Generator failed:\", res[\"generator_failed\"], \"Intent:\", res[\"intent\"])\n",
        "if res[\"debug\"]:\n",
        "    print(\"\\n--- Debug: Top retrieved (raw) ---\")\n",
        "    for i, it in enumerate(res[\"debug\"][\"retrieved_top\"][:5], 1):\n",
        "        print(f\"{i}. [{it['id']}] {it['text']}\")\n",
        "    print(\"\\n--- Debug: Filtered context used for generation ---\")\n",
        "    for i, it in enumerate(res[\"debug\"][\"retrieved_filtered\"][:5], 1):\n",
        "        print(f\"{i}. [{it['id']}] {it['text']}\")\n",
        "    print(\"\\n--- Debug: Rescued hits (corpus scan) ---\")\n",
        "    for i, it in enumerate(res[\"debug\"][\"rescued_hits\"][:5], 1):\n",
        "        print(f\"{i}. [{it['id']}] {it['text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy cell so that notebook doesn`t reset\n",
        "print(\"Notebook don`t reset!!\")"
      ],
      "metadata": {
        "id": "2CJLtHqe8CCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick smoke-run: 10 eval items to sanity-check Cell 12\n",
        "sample_eval = eval_items[:10]  # eval_items is defined inside Cell 12; if not, use eval_queries[:10]\n",
        "print(\"Running quick RAG eval on 10 items (hybrid) to estimate runtime...\")\n",
        "_ = rag_generation_metrics(sample_eval, mode=\"hybrid\", k=5, max_length=120, num_beams=4)\n",
        "print(\"Quick run complete.\")\n"
      ],
      "metadata": {
        "id": "BZOGCC8KY4KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Comprehensive evaluation — retrieval, RAG generation, ablations, human sampling\n",
        "# - Evaluates bm25, dense, hybrid_interleave, hybrid_score, hybrid_rrf\n",
        "# - Computes Retrieval: Recall@1, Recall@5, MRR\n",
        "# - Computes Generation (RAG): BLEU, chrF, token F1, latency, fallback rates\n",
        "# - Ablation: disable extractive QA fallback; compare retrieval modes\n",
        "# - Exports summary JSON and human-eval samples\n",
        "\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import statistics\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'retrieve' in globals(), \"retrieve(...) must be defined (Cell 9)\"\n",
        "assert 'rag_answer' in globals(), \"rag_answer(...) must be defined (Cell 11)\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "assert isinstance(eval_queries, list) and len(eval_queries) > 0, \"eval_queries must be a non-empty list\"\n",
        "\n",
        "# ---------- Config ----------\n",
        "EVAL_ITEMS = eval_queries_val if 'eval_queries_val' in globals() else eval_queries\n",
        "MODES = [\"bm25\", \"dense\", \"hybrid_interleave\", \"hybrid_score\", \"hybrid_rrf\"]\n",
        "K_RETRIEVAL = 5\n",
        "RAG_K = 5\n",
        "RAG_MAX_LEN = 160\n",
        "RAG_BEAMS = 6\n",
        "\n",
        "# Output paths\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/eval_outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SUMMARY_JSON = OUT_DIR / \"rag_eval_summary.json\"\n",
        "PER_QUERY_JSON = OUT_DIR / \"rag_eval_per_query.jsonl\"\n",
        "HUMAN_SAMPLES_JSON = OUT_DIR / \"rag_human_samples.jsonl\"\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def get_references_for_item(item):\n",
        "    refs = []\n",
        "    if \"gold_answer\" in item and item[\"gold_answer\"]:\n",
        "        refs = [item[\"gold_answer\"]]\n",
        "    elif \"answers\" in item and item[\"answers\"]:\n",
        "        val = item[\"answers\"]\n",
        "        if isinstance(val, str):\n",
        "            refs = [val]\n",
        "        elif isinstance(val, list):\n",
        "            refs = [x for x in val if isinstance(x, str) and x.strip()]\n",
        "    return [r.strip() for r in refs if r and r.strip()]\n",
        "\n",
        "def token_f1(pred, refs):\n",
        "    def toks(s):\n",
        "        return [t for t in s.strip().split() if t]\n",
        "    pred_t = set(toks(pred))\n",
        "    if not refs:\n",
        "        return 0.0\n",
        "    best = 0.0\n",
        "    for r in refs:\n",
        "        ref_t = set(toks(r))\n",
        "        if not ref_t:\n",
        "            continue\n",
        "        tp = len(pred_t & ref_t)\n",
        "        prec = tp / max(1, len(pred_t))\n",
        "        rec = tp / max(1, len(ref_t))\n",
        "        f1 = 0.0 if (prec + rec) == 0 else (2 * prec * rec) / (prec + rec)\n",
        "        best = max(best, f1)\n",
        "    return best\n",
        "\n",
        "# ---------- Retrieval evaluation ----------\n",
        "def retrieval_metrics(eval_items, mode=\"hybrid_score\", k=K_RETRIEVAL):\n",
        "    rr_list = []\n",
        "    r1 = 0\n",
        "    r5 = 0\n",
        "    latencies = []\n",
        "    for it in eval_items:\n",
        "        q = it.get(\"query\") or it.get(\"question\") or it.get(\"q\") or \"\"\n",
        "        pos_ids = set(map(str, it.get(\"positive_ids\", [])))\n",
        "        t0 = time.time()\n",
        "        hits = retrieve(q, k=k, mode=mode)\n",
        "        latencies.append(time.time() - t0)\n",
        "        retrieved_ids = [r[0] for r in hits]\n",
        "        # MRR\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in pos_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "        # Recall@1 / Recall@5\n",
        "        r1 += 1 if (len(retrieved_ids) >= 1 and retrieved_ids[0] in pos_ids) else 0\n",
        "        r5 += 1 if any(pid in pos_ids for pid in retrieved_ids[:5]) else 0\n",
        "    n = len(eval_items)\n",
        "    return {\n",
        "        \"MRR\": float(np.mean(rr_list)) if rr_list else 0.0,\n",
        "        \"Recall@1\": r1 / n if n else 0.0,\n",
        "        \"Recall@5\": r5 / n if n else 0.0,\n",
        "        \"latency_mean_s\": float(np.mean(latencies)) if latencies else 0.0,\n",
        "        \"latency_median_s\": float(statistics.median(latencies)) if latencies else 0.0,\n",
        "        \"n\": n\n",
        "    }\n",
        "\n",
        "# ---------- SacreBLEU setup ----------\n",
        "try:\n",
        "    import sacrebleu\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"sacrebleu\"], check=True)\n",
        "    import sacrebleu\n",
        "\n",
        "# ---------- RAG generation evaluation ----------\n",
        "def rag_generation_metrics(eval_items, mode=\"hybrid_score\", k=RAG_K, max_length=RAG_MAX_LEN, num_beams=RAG_BEAMS, allow_fallback=True):\n",
        "    preds = []\n",
        "    refs_all = []\n",
        "    f1s = []\n",
        "    latencies = []\n",
        "    fallback_count = 0\n",
        "    gen_fail_count = 0\n",
        "    per_query = []\n",
        "\n",
        "    for it in eval_items:\n",
        "        q = it[\"query\"]\n",
        "        refs = get_references_for_item(it)\n",
        "        res = rag_answer(q, k=k, mode=mode, max_length=max_length, num_beams=num_beams, debug=False)\n",
        "        # ablation: disable fallback (use generator answer only; if generator_failed, treat as empty)\n",
        "        if not allow_fallback and res.get(\"generator_failed\"):\n",
        "            pred = \"\"\n",
        "            used_fallback = False\n",
        "            generator_failed = True\n",
        "        else:\n",
        "            pred = res.get(\"answer\", \"\").strip()\n",
        "            used_fallback = res.get(\"used_fallback\", False)\n",
        "            generator_failed = res.get(\"generator_failed\", False)\n",
        "\n",
        "        preds.append(pred)\n",
        "        refs_all.append(refs if refs else [\"\"])\n",
        "        f1s.append(token_f1(pred, refs))\n",
        "        latencies.append(res.get(\"latency\", 0.0))\n",
        "        fallback_count += 1 if used_fallback else 0\n",
        "        gen_fail_count += 1 if generator_failed else 0\n",
        "\n",
        "        per_query.append({\n",
        "            \"query\": q,\n",
        "            \"intent\": res.get(\"intent\"),\n",
        "            \"pred\": pred,\n",
        "            \"refs\": refs,\n",
        "            \"used_fallback\": used_fallback,\n",
        "            \"generator_failed\": generator_failed,\n",
        "            \"latency_s\": res.get(\"latency\", 0.0),\n",
        "            \"provenance\": res.get(\"provenance\", [])\n",
        "        })\n",
        "\n",
        "    # SacreBLEU / chrF formatting: transpose references to list-of-reference-sets\n",
        "    max_refs = max(len(r) for r in refs_all) if refs_all else 1\n",
        "    ref_sets = []\n",
        "    for i in range(max_refs):\n",
        "        ref_sets.append([ (refs[i] if i < len(refs) else \"\") for refs in refs_all ])\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(preds, ref_sets)\n",
        "    chrf = sacrebleu.corpus_chrf(preds, ref_sets)\n",
        "\n",
        "    summary = {\n",
        "        \"BLEU\": float(bleu.score),\n",
        "        \"chrF\": float(chrf.score),\n",
        "        \"F1_mean\": float(np.mean(f1s)) if f1s else 0.0,\n",
        "        \"F1_median\": float(statistics.median(f1s)) if f1s else 0.0,\n",
        "        \"latency_mean_s\": float(np.mean(latencies)) if latencies else 0.0,\n",
        "        \"latency_median_s\": float(statistics.median(latencies)) if latencies else 0.0,\n",
        "        \"fallback_rate\": fallback_count / len(eval_items) if eval_items else 0.0,\n",
        "        \"generator_fail_rate\": gen_fail_count / len(eval_items) if eval_items else 0.0,\n",
        "        \"n\": len(eval_items)\n",
        "    }\n",
        "    return summary, per_query\n",
        "\n",
        "# ---------- Human factuality sampling export ----------\n",
        "def export_human_samples(per_query_records, sample_size=25, path=HUMAN_SAMPLES_JSON):\n",
        "    # Select diverse samples: prioritize no-fallback, then fallback, mix intents\n",
        "    # Simple strategy: take first N no-fallback, then fill with fallback cases\n",
        "    no_fb = [r for r in per_query_records if not r[\"used_fallback\"]]\n",
        "    fb = [r for r in per_query_records if r[\"used_fallback\"]]\n",
        "    sample = (no_fb[:sample_size//2]) + (fb[:sample_size - len(no_fb[:sample_size//2])])\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in sample:\n",
        "            payload = {\n",
        "                \"query\": r[\"query\"],\n",
        "                \"intent\": r.get(\"intent\"),\n",
        "                \"prediction\": r[\"pred\"],\n",
        "                \"references\": r[\"refs\"],\n",
        "                \"provenance\": r.get(\"provenance\", []),\n",
        "                \"human_judgment\": {\n",
        "                    \"factual_consistency\": \"TBD_true/partial/false\",\n",
        "                    \"helpfulness\": \"TBD_1-5\",\n",
        "                    \"notes\": \"\"\n",
        "                }\n",
        "            }\n",
        "            f.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n",
        "    return len(sample)\n",
        "\n",
        "# ---------- Run retrieval evaluation across modes ----------\n",
        "retrieval_results = {}\n",
        "print(\"=== Retrieval Quality (Recall@1 / Recall@5 / MRR) ===\")\n",
        "for m in MODES:\n",
        "    metrics = retrieval_metrics(EVAL_ITEMS, mode=m, k=K_RETRIEVAL)\n",
        "    retrieval_results[m] = metrics\n",
        "    print(f\"- {m}: Recall@1={metrics['Recall@1']:.3f} | Recall@5={metrics['Recall@5']:.3f} | MRR={metrics['MRR']:.3f}\")\n",
        "\n",
        "# ---------- RAG generation & end-to-end evaluation (by mode) ----------\n",
        "rag_results_by_mode = {}\n",
        "per_query_all = {}\n",
        "\n",
        "print(\"\\n=== RAG Generation & End-to-End (with fallback) ===\")\n",
        "for m in MODES:\n",
        "    summary, per_query = rag_generation_metrics(EVAL_ITEMS, mode=m, k=RAG_K, max_length=RAG_MAX_LEN, num_beams=RAG_BEAMS, allow_fallback=True)\n",
        "    rag_results_by_mode[m] = summary\n",
        "    per_query_all[f\"{m}_with_fb\"] = per_query\n",
        "    print(f\"- {m}: BLEU={summary['BLEU']:.2f}, chrF={summary['chrF']:.2f}, F1_mean={summary['F1_mean']:.3f}, latency_mean_s={summary['latency_mean_s']:.3f}, fallback_rate={summary['fallback_rate']:.2f}\")\n",
        "\n",
        "print(\"\\n=== Ablation: RAG without extractive QA fallback (generator-only on retrieved context) ===\")\n",
        "rag_results_no_fb = {}\n",
        "for m in MODES:\n",
        "    summary, per_query = rag_generation_metrics(EVAL_ITEMS, mode=m, k=RAG_K, max_length=RAG_MAX_LEN, num_beams=RAG_BEAMS, allow_fallback=False)\n",
        "    rag_results_no_fb[m] = summary\n",
        "    per_query_all[f\"{m}_no_fb\"] = per_query\n",
        "    print(f\"- {m}: BLEU={summary['BLEU']:.2f}, chrF={summary['chrF']:.2f}, F1_mean={summary['F1_mean']:.3f}, latency_mean_s={summary['latency_mean_s']:.3f}, generator_fail_rate={summary['generator_fail_rate']:.2f}\")\n",
        "\n",
        "# ---------- Compact summary and export ----------\n",
        "summary = {\n",
        "    \"retrieval\": retrieval_results,\n",
        "    \"rag_with_fallback\": rag_results_by_mode,\n",
        "    \"rag_without_fallback\": rag_results_no_fb,\n",
        "    \"config\": {\n",
        "        \"modes\": MODES,\n",
        "        \"retrieval_k\": K_RETRIEVAL,\n",
        "        \"rag_k\": RAG_K,\n",
        "        \"rag_max_length\": RAG_MAX_LEN,\n",
        "        \"rag_num_beams\": RAG_BEAMS,\n",
        "        \"n_eval\": len(EVAL_ITEMS)\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(SUMMARY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(PER_QUERY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    for key, records in per_query_all.items():\n",
        "        for r in records:\n",
        "            r_out = dict(r)\n",
        "            r_out[\"mode\"] = key\n",
        "            f.write(json.dumps(r_out, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# ---------- Human factuality sampling ----------\n",
        "sample_count = export_human_samples(per_query_all.get(\"hybrid_score_with_fb\", []), sample_size=25, path=HUMAN_SAMPLES_JSON)\n",
        "\n",
        "print(\"\\n=== Summary (saved) ===\")\n",
        "print(f\"- Summary JSON: {SUMMARY_JSON}\")\n",
        "print(f\"- Per-query logs: {PER_QUERY_JSON}\")\n",
        "print(f\"- Human samples for factuality (n={sample_count}): {HUMAN_SAMPLES_JSON}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZGQpO0zBBoJ",
        "outputId": "98a77ae2-df76-4448-9fc7-c256e863250e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Retrieval Quality (Recall@1 / Recall@5 / MRR) ===\n",
            "- bm25: Recall@1=0.830 | Recall@5=0.950 | MRR=0.880\n",
            "- dense: Recall@1=0.780 | Recall@5=0.940 | MRR=0.857\n",
            "- hybrid_interleave: Recall@1=0.780 | Recall@5=0.940 | MRR=0.857\n",
            "- hybrid_score: Recall@1=0.860 | Recall@5=1.000 | MRR=0.928\n",
            "- hybrid_rrf: Recall@1=0.820 | Recall@5=0.990 | MRR=0.894\n",
            "\n",
            "=== RAG Generation & End-to-End (with fallback) ===\n",
            "- bm25: BLEU=12.48, chrF=28.41, F1_mean=0.278, latency_mean_s=0.907, fallback_rate=0.29\n",
            "- dense: BLEU=14.48, chrF=30.60, F1_mean=0.313, latency_mean_s=0.626, fallback_rate=0.32\n",
            "- hybrid_interleave: BLEU=14.48, chrF=30.60, F1_mean=0.313, latency_mean_s=0.616, fallback_rate=0.32\n",
            "- hybrid_score: BLEU=14.75, chrF=31.82, F1_mean=0.328, latency_mean_s=0.629, fallback_rate=0.27\n",
            "- hybrid_rrf: BLEU=14.59, chrF=31.26, F1_mean=0.320, latency_mean_s=0.623, fallback_rate=0.29\n",
            "\n",
            "=== Ablation: RAG without extractive QA fallback (generator-only on retrieved context) ===\n",
            "- bm25: BLEU=11.26, chrF=26.12, F1_mean=0.249, latency_mean_s=0.637, generator_fail_rate=0.29\n",
            "- dense: BLEU=12.60, chrF=27.81, F1_mean=0.279, latency_mean_s=0.616, generator_fail_rate=0.32\n",
            "- hybrid_interleave: BLEU=12.60, chrF=27.81, F1_mean=0.279, latency_mean_s=0.623, generator_fail_rate=0.32\n",
            "- hybrid_score: BLEU=12.98, chrF=28.95, F1_mean=0.293, latency_mean_s=0.604, generator_fail_rate=0.27\n",
            "- hybrid_rrf: BLEU=12.77, chrF=28.36, F1_mean=0.283, latency_mean_s=0.626, generator_fail_rate=0.29\n",
            "\n",
            "=== Summary (saved) ===\n",
            "- Summary JSON: /content/drive/MyDrive/eval_outputs/rag_eval_summary.json\n",
            "- Per-query logs: /content/drive/MyDrive/eval_outputs/rag_eval_per_query.jsonl\n",
            "- Human samples for factuality (n=25): /content/drive/MyDrive/eval_outputs/rag_human_samples.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that bm25 index object and corpus mapping exist\n",
        "print(\"bm25_retrieve exists:\", 'bm25_retrieve' in globals())\n",
        "if 'bm25_index' in globals():\n",
        "    try:\n",
        "        print(\"bm25_index type:\", type(bm25_index))\n",
        "        # If your BM25 implementation exposes doc count:\n",
        "        print(\"bm25_index doc count (if available):\", getattr(bm25_index, 'doc_count', 'unknown'))\n",
        "    except Exception as e:\n",
        "        print(\"bm25_index introspect error:\", e)\n",
        "else:\n",
        "    print(\"bm25_index not found in globals.\")\n"
      ],
      "metadata": {
        "id": "Y9mGYjctVWm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "if 'bm25_retrieve' in globals():\n",
        "    print(inspect.getsource(bm25_retrieve))\n",
        "else:\n",
        "    print(\"bm25_retrieve not defined in this session.\")\n"
      ],
      "metadata": {
        "id": "R-_NuwrVVaCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_qs = [eval_queries[0][\"query\"], eval_queries[1][\"query\"], \"کووڈ-19 کی عام علامات کیا ہیں؟\"]\n",
        "for q in sample_qs:\n",
        "    print(\"\\nQuery:\", q)\n",
        "    try:\n",
        "        hits = bm25_retrieve(q, k=10)  # adjust if your wrapper signature differs\n",
        "        for i, (pid, score, txt) in enumerate(hits[:10], 1):\n",
        "            print(f\"{i}. {pid} score={score} text_preview={txt[:120]}\")\n",
        "    except Exception as e:\n",
        "        print(\"bm25_retrieve call error:\", e)\n"
      ],
      "metadata": {
        "id": "Hx8vabVqVedt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If your BM25 corpus mapping is named bm25_corpus or similar, inspect keys\n",
        "if 'bm25_corpus' in globals():\n",
        "    sample_keys = list(bm25_corpus.keys())[:10]\n",
        "    print(\"bm25_corpus sample keys:\", sample_keys)\n",
        "else:\n",
        "    print(\"No bm25_corpus variable found; check your BM25 build step.\")\n",
        "# Check overlap with pid2text\n",
        "if 'pid2text' in globals() and 'bm25_corpus' in globals():\n",
        "    overlap = set(pid2text.keys()) & set(bm25_corpus.keys())\n",
        "    print(\"Overlap count between pid2text and bm25_corpus:\", len(overlap))\n"
      ],
      "metadata": {
        "id": "2RyITB0ZViQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = eval_queries[0][\"query\"]\n",
        "print(\"Dense top-5:\", [pid for pid,_,_ in dense_retrieve(q, k=5)])\n",
        "print(\"BM25 top-5:\", [pid for pid,_,_ in bm25_retrieve(q, k=5)])\n"
      ],
      "metadata": {
        "id": "mFpFXSk8VlvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Full evaluation run (may take long). Use max_examples to limit.\n",
        "results = evaluate_rag(eval_queries, k=5, mode=\"hybrid\", max_examples=100)  # set None to run all\n",
        "print(\"Results:\", results)\n"
      ],
      "metadata": {
        "id": "C-7YyeTQBKJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Ablation experiments: compare retrieval modes and cross-lingual fallback\n",
        "modes = [\"bm25\", \"dense\", \"hybrid\"]\n",
        "ablation_results = {}\n",
        "for mode in modes:\n",
        "    print(\"Evaluating mode:\", mode)\n",
        "    ablation_results[mode] = evaluate_rag(eval_queries, k=5, mode=mode, max_examples=100)\n",
        "print(\"Ablation summary:\", ablation_results)\n"
      ],
      "metadata": {
        "id": "M2oPkfA9BSba"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UWlbuPkYZc0sXKeM2L5lHP0wSGcZhhCp",
      "authorship_tag": "ABX9TyMUzUFfhxyhXmHQVfiQL1sU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
}
}
    
