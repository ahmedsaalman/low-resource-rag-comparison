{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedsaalman/low-resource-rag-comparison/blob/main/NLP_Project_RAG_Finalized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzOG_Jcoakae"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install required libraries (run this cell first and one by one all required libraries will be installed)\n",
        "# - transformers: model + generation\n",
        "# - sentence-transformers: dense embeddings / fine-tuning helpers\n",
        "# - faiss-cpu (or faiss-gpu if GPU available)\n",
        "# - rank_bm25: BM25 baseline\n",
        "# - datasets: convenient JSONL loading\n",
        "# - evaluate / sacrebleu: BLEU/chrF metrics\n",
        "# - tqdm: progress bars\n",
        "# - accelerate (optional) for distributed/faster training\n",
        "!pip install -q transformers sentence-transformers faiss-cpu rank_bm25 datasets evaluate sacrebleu tqdm accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive if not already connected\n",
        "# 2. Mount Google Drive\n",
        "# We need this to load your fine-tuned Dense Retriever and your Corpus file.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NFnrxVRazzm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RbSs_eoZPLJV"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu --no-cache-dir\n",
        "!pip install evaluate datasets sacrebleu\n",
        "!pip install faiss-gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM5UqSd5I4q1"
      },
      "outputs": [],
      "source": [
        "# Optional cell\n",
        "# To add all the required files run\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H7rCqtdUcttO"
      },
      "outputs": [],
      "source": [
        "!pip list # Optional to run this cell: To check which of the libraries/packages have been installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN235ZNIbMoI"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Imports and GPU check: Run this cell after the first cell\n",
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
        "import sentence_transformers # Import the package itself to access __version__\n",
        "\n",
        "# FAISS and BM25\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Datasets and metrics\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "import sacrebleu\n",
        "\n",
        "# Print versions and GPU info\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"torch not available:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uzv2tu4xZZWX"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "DATA_DIR = Path(\"drive/MyDrive/data\")  # change if files are elsewhere\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "with open(DATA_DIR / \"urdu_covid_passages.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Use split(None, 1) to split on the first occurrence of any whitespace\n",
        "        # This handles cases where the delimiter might be spaces instead of a tab.\n",
        "        if line.strip(): # Ensure line is not empty after stripping whitespace\n",
        "            parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "            if len(parts) == 2:\n",
        "                pid, text = parts\n",
        "                passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "            else:\n",
        "                print(f\"Skipping malformed line in urdu_covid_passages.tsv: {line.strip()}\")\n",
        "\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "print(\"Loaded:\", len(corpus_clean), \"corpus_clean; \", len(passages_min), \"passages_min; \", len(eval_queries), \"eval queries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Tazxtecpqy"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. FIX: Point to the current Colab \"content\" folder, not Google Drive\n",
        "DATA_DIR = Path(\"\")  # \".\" represents the current directory (/content in Colab)\n",
        "\n",
        "# Create the data directory if it doesn't exist (optional if you just uploaded them)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    # Added error handling in case a specific file is missing\n",
        "    if not path.exists():\n",
        "        print(f\"Warning: File not found: {path}\")\n",
        "        return []\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "# Load JSONL files\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "tsv_path = DATA_DIR / \"urdu_covid_passages.tsv\"\n",
        "\n",
        "if tsv_path.exists():\n",
        "    with open(tsv_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "\n",
        "                parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "\n",
        "                if len(parts) == 2:\n",
        "                    pid, text = parts\n",
        "                    passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "                else:\n",
        "                    print(f\"Skipping malformed line in TSV: {line.strip()}\")\n",
        "else:\n",
        "    print(f\"Warning: File not found: {tsv_path}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Loaded:\")\n",
        "print(f\"  {len(corpus_clean)} corpus_clean\")\n",
        "print(f\"  {len(passages_min)} passages_min\")\n",
        "print(f\"  {len(passages_tsv)} passages_tsv\")\n",
        "print(f\"  {len(eval_queries)} eval queries\")\n",
        "print(f\"  {len(synthetic_pairs)} synthetic pairs\")\n",
        "print(f\"  {len(hard_negatives)} hard negatives\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOOjSvxJcrn-"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Validate IDs referenced in eval/synthetic/hard_negatives exist in corpus\n",
        "# Run this after Cell 3.\n",
        "passage_ids = {p[\"id\"] for p in passages_min}\n",
        "missing = []\n",
        "for q in eval_queries:\n",
        "    for pid in q.get(\"positive_ids\", []):\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"eval\", q[\"query_id\"], pid))\n",
        "for s in synthetic_pairs:\n",
        "    if s[\"positive_id\"] not in passage_ids:\n",
        "        missing.append((\"synthetic\", s[\"synthetic_id\"], s[\"positive_id\"]))\n",
        "for h in hard_negatives:\n",
        "    for pid in h[\"hard_negatives\"]:\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"hardneg\", h[\"query_id\"], pid))\n",
        "print(\"Missing references (should be zero):\", len(missing))\n",
        "if missing:\n",
        "    print(missing[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83StfDLscxFh"
      },
      "outputs": [],
      "source": [
        "# Cell 5 (Run after Cell 4): BM25 baseline index (tokenize with simple whitespace; for Urdu this is OK as baseline)\n",
        "# We'll store tokenized corpus and BM25 object for retrieval.\n",
        "from nltk.tokenize import word_tokenize\n",
        "# If nltk not installed, use simple split\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Added to resolve LookupError for 'punkt_tab'\n",
        "    tokenizer = lambda s: word_tokenize(s)\n",
        "except Exception:\n",
        "    tokenizer = lambda s: s.split()\n",
        "\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "corpus_ids = [p[\"id\"] for p in passages_min]\n",
        "tokenized_corpus = [tokenizer(t) for t in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Example retrieval function\n",
        "def bm25_retrieve(query, k=5):\n",
        "    q_tokens = tokenizer(query)\n",
        "    scores = bm25.get_scores(q_tokens)\n",
        "    topk = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in topk]\n",
        "\n",
        "# Quick test\n",
        "print(\"BM25 top-3 for sample:\", bm25_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmccnrqkczDz"
      },
      "outputs": [],
      "source": [
        "# Cell 5b: BM25-only retriever evaluation tool (run after Cell 5)\n",
        "# Purpose: standalone evaluation harness for the independent BM25 retriever (bm25_retrieve)\n",
        "# Metrics included (applicable to a retriever-only evaluation):\n",
        "#   - Recall@1, Recall@5\n",
        "#   - MRR (Mean Reciprocal Rank)\n",
        "#   - Precision@k (k=1,5)\n",
        "#   - Average / median retrieval latency\n",
        "#   - Optional: match by gold_passage_id or by substring match of gold_answer\n",
        "# Output:\n",
        "#   - Per-query JSONL saved to bm25_eval_results.jsonl\n",
        "#   - Printed summary with all metrics\n",
        "#\n",
        "# Requirements (must be available in the session):\n",
        "#   - bm25_retrieve(query, k) -> list of (passage_id, passage_text, score)\n",
        "#   - eval_queries: list of dicts with at least a query field and optionally:\n",
        "#       * \"question\" or \"query\" or \"q\"  (the query text)\n",
        "#       * \"gold_passage_id\" (optional) OR \"answer\"/\"gold\" (gold text to match)\n",
        "#\n",
        "# Usage:\n",
        "#   - Run this cell after you build the BM25 index (Cell 5).\n",
        "#   - Optionally pass a different eval list or k values to evaluate subsets.\n",
        "\n",
        "# Use this evaluator if your eval_queries items contain \"positive_ids\" and \"gold_answer\"\n",
        "import json, time, re, statistics\n",
        "from typing import List, Dict\n",
        "\n",
        "OUT_JSONL = \"bm25_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def get_query_text(item: Dict) -> str:\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_bm25_with_positive_ids(eval_items: List[Dict],\n",
        "                                    out_jsonl: str = OUT_JSONL,\n",
        "                                    k: int = DEFAULT_K,\n",
        "                                    recall_ks = RECALL_KS,\n",
        "                                    precision_ks = PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        # normalize to list of strings\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or item.get(\"answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = bm25_retrieve(q, k=k)   # (id, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] bm25_retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k (multiple positives supported)\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            # precision@k = (# positives in top-k) / k\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [t[:300] for t in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run it\n",
        "if 'eval_queries' not in globals():\n",
        "    # try to load from file if not in memory\n",
        "    eval_queries = []\n",
        "    with open(\"eval_queries.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            eval_queries.append(json.loads(line))\n",
        "\n",
        "summary, records = evaluate_bm25_with_positive_ids(eval_queries, out_jsonl=OUT_JSONL, k=DEFAULT_K)\n",
        "print(\"BM25 retrieval evaluation summary:\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# show a few examples where retrieval missed positives\n",
        "misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "print(f\"\\nTotal misses: {len(misses)} / {len(records)}. Showing up to 5 misses:\")\n",
        "for r in misses[:5]:\n",
        "    print(\"Query id:\", r.get(\"query_id\"), \"Query:\", r[\"query\"][:80])\n",
        "    print(\" Positives:\", r[\"positive_ids\"])\n",
        "    print(\" Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpexUaHYc03R"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Dense embeddings with a multilingual model (use a compact model for Colab)\n",
        "# We use a multilingual SBERT model that supports Urdu reasonably (e.g., 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embed_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "# Compute embeddings for passages_min (batching)\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS index (cosine similarity via normalized vectors)\n",
        "d = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)  # inner product\n",
        "# normalize embeddings for cosine\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# Map index positions to ids\n",
        "# retrieval function\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "# Quick test\n",
        "print(\"Dense top-3:\", dense_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX-IFq-Wc2tF"
      },
      "outputs": [],
      "source": [
        "# Cell 6b: Evaluation of dense retriever (run after Cell 6)\n",
        "# Purpose: measure Recall@1, Recall@5, MRR, Precision@k, latency for dense_retrieve\n",
        "# Uses eval_queries with \"positive_ids\" and \"gold_answer\" fields\n",
        "\n",
        "import json, time, re, statistics\n",
        "\n",
        "OUT_JSONL_DENSE = \"dense_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s):\n",
        "    if s is None: return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "\n",
        "def get_query_text(item):\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_dense(eval_items, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K,\n",
        "                   recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies, rr_list = [], []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        pos_ids = item.get(\"positive_ids\") or []\n",
        "        if isinstance(pos_ids, str): pos_ids = [pos_ids]\n",
        "        pos_ids = [str(x) for x in pos_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        hits = dense_retrieve(q, k=k)  # (id, text, score)\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in pos_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in pos_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in pos_ids)\n",
        "            precision_sums[pk] += num_pos_in_topk / pk\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": pos_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [txt[:300] for txt in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": sum(rr_list)/n,\n",
        "        **{f\"Recall@{rk}\": recall_counts[rk]/n for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_sums[pk]/n for pk in precision_ks},\n",
        "        \"latency_mean_s\": statistics.mean(latencies) if latencies else 0.0,\n",
        "        \"latency_median_s\": statistics.median(latencies) if latencies else 0.0\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run evaluation\n",
        "print(\"[dense_eval] Running dense retriever evaluation...\")\n",
        "summary_dense, records_dense = evaluate_dense(eval_queries, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K)\n",
        "print(\"\\nDense retriever evaluation summary:\")\n",
        "for k,v in summary_dense.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Show a few examples\n",
        "print(\"\\nExamples (first 5):\")\n",
        "for r in records_dense[:5]:\n",
        "    print(\" - Query:\", r[\"query\"][:80])\n",
        "    print(\"   Retrieved ids:\", r[\"retrieved_ids\"][:6])\n",
        "    print(\"   Reciprocal rank:\", r[\"reciprocal_rank\"], \"Latency(s):\", round(r[\"latency\"], 4))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7lxK5E-c5AR"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Prepare InputExamples for sentence-transformers fine-tuning\n",
        "# Now with an 80/20 train/validation split\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "import random\n",
        "\n",
        "pid2text = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "examples = []\n",
        "for s in synthetic_pairs:\n",
        "    q = s[\"query\"]\n",
        "    pos = pid2text.get(s[\"positive_id\"])\n",
        "    neg = None\n",
        "    # Find hard negatives if available\n",
        "    hn = next((h for h in hard_negatives if h[\"query_id\"] == s.get(\"synthetic_id\", s.get(\"query_id\"))), None)\n",
        "    if hn:\n",
        "        for nid in hn[\"hard_negatives\"]:\n",
        "            if nid != s[\"positive_id\"]:\n",
        "                neg = pid2text.get(nid)\n",
        "                break\n",
        "    if neg is None:\n",
        "        # fallback: random negative\n",
        "        neg_id = random.choice([pid for pid in corpus_ids if pid != s[\"positive_id\"]])\n",
        "        neg = pid2text[neg_id]\n",
        "    if pos and neg:\n",
        "        examples.append(InputExample(texts=[q, pos, neg]))\n",
        "\n",
        "print(\"Prepared\", len(examples), \"triplet examples.\")\n",
        "\n",
        "# --- Split into train/validation (80/20) ---\n",
        "random.shuffle(examples)\n",
        "split_idx = int(0.8 * len(examples))\n",
        "train_examples = examples[:split_idx]\n",
        "val_examples = examples[split_idx:]\n",
        "\n",
        "print(\"Train examples:\", len(train_examples))\n",
        "print(\"Validation examples:\", len(val_examples))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-SEj6lLc61H"
      },
      "outputs": [],
      "source": [
        "# Cell 8 (use in-memory model; do NOT reload): Fine-tune SBERT with triplet loss and IR validation on passages_min\n",
        "import os\n",
        "# --- GRANDMASTER FIX: DISABLE WANDB ---\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "# --------------------------------------\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, losses, evaluation\n",
        "#import faiss\n",
        "\n",
        "# Sanity checks\n",
        "assert isinstance(train_examples, list) and len(train_examples) > 0, \"train_examples must be a non-empty list\"\n",
        "assert 'passages_min' in globals(), \"passages_min must be loaded\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "\n",
        "# Build validation split against real corpus & labels\n",
        "# (Check if eval_queries_val exists, otherwise split eval_queries)\n",
        "eval_val = eval_queries_val if 'eval_queries_val' in globals() else eval_queries[int(0.8*len(eval_queries)):]\n",
        "\n",
        "val_queries_dict = {it[\"query_id\"]: it[\"query\"] for it in eval_val}\n",
        "# Fix: Ensure positive_ids is a list\n",
        "val_relevant_dict = {it[\"query_id\"]: set(it[\"positive_ids\"] if isinstance(it[\"positive_ids\"], list) else [it[\"positive_ids\"]]) for it in eval_val}\n",
        "val_corpus_dict = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "# Warn if labels reference missing ids\n",
        "missing = []\n",
        "for qid, rels in val_relevant_dict.items():\n",
        "    for pid in rels:\n",
        "        if pid not in val_corpus_dict:\n",
        "            missing.append((qid, pid))\n",
        "if missing:\n",
        "    print(f\"Warning: {len(missing)} relevant ids not found in corpus. Example:\", missing[:3])\n",
        "\n",
        "# Construct evaluator (defaults to cosine similarity)\n",
        "retrieval_evaluator = evaluation.InformationRetrievalEvaluator(\n",
        "    queries=val_queries_dict,\n",
        "    corpus=val_corpus_dict,\n",
        "    relevant_docs=val_relevant_dict,\n",
        "    name=\"val_ir_passages\"\n",
        ")\n",
        "\n",
        "# Start from baseline multilingual MiniLM\n",
        "# We use the variable 'embedder' from Cell 6 to ensure we continue correctly\n",
        "if 'embedder' not in globals():\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embedder.to(\"cuda\")\n",
        "\n",
        "# Triplet loss with conservative settings\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.TripletLoss(\n",
        "    model=embedder,\n",
        "    distance_metric=losses.TripletDistanceMetric.COSINE,\n",
        "    triplet_margin=0.3\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
        "optimizer_params = {'lr': 2e-5}\n",
        "\n",
        "print(\"Starting fine-tuning (WandB Disabled)...\")\n",
        "\n",
        "# Train with IR evaluator\n",
        "embedder.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=retrieval_evaluator,\n",
        "    epochs=num_epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    optimizer_params=optimizer_params,\n",
        "    show_progress_bar=True,\n",
        "    output_path=\"fine_tuned_sbert_urdu_passages\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAu5qjmwc9zl"
      },
      "outputs": [],
      "source": [
        "# Cell 8b: Save the Fine-Tuned Model to Drive (Run ONLY if satisfied with accuracy)\n",
        "import os\n",
        "\n",
        "# Define path\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "\n",
        "print(f\"ðŸ’¾ Saving model to {MODEL_SAVE_PATH} ...\")\n",
        "\n",
        "# Create directory if not exists\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "embedder.save(MODEL_SAVE_PATH)\n",
        "\n",
        "print(f\"âœ… Model saved! You can now use Cell 8c in future sessions to skip training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaknzmOqG0R9"
      },
      "outputs": [],
      "source": [
        "# Cell 8c: FAST START - Load Model from Drive & Rebuild FAISS (Skips Training)\n",
        "# Run this INSTEAD of Cells 6, 7, 8, 8b in future sessions.\n",
        "\n",
        "import os\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "\n",
        "# 1. Load the Model\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"ðŸ“‚ Loading saved model from: {MODEL_SAVE_PATH}\")\n",
        "    embedder = SentenceTransformer(MODEL_SAVE_PATH).to(\"cuda\")\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"âŒ No saved model found at {MODEL_SAVE_PATH}. Please run Cell 8 & 8b once to create it!\")\n",
        "\n",
        "# 2. Rebuild FAISS Index (Critical Step)\n",
        "# We must re-encode the corpus because we just loaded a specific model\n",
        "print(\"â³ Generating embeddings for corpus...\")\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "\n",
        "# Generate embeddings\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index = faiss.IndexFlatIP(passage_embeddings.shape[1])\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# 3. Define the Retrieval Function\n",
        "# (We must re-define this here because we skipped the previous cells that defined it)\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "print(\"âœ… Dense Retriever System Restored & Ready for Hybrid Fusion (Cell 9).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwDU_Bm6dAlp"
      },
      "outputs": [],
      "source": [
        "# Cell 9 (final): Retriever wrapper with true fusion modes (non-destructive)\n",
        "# - Creates bm25_new only if not present\n",
        "# - Supports modes: 'bm25', 'dense', 'hybrid_interleave' (legacy), 'hybrid_score', 'hybrid_rrf'\n",
        "# - Returns list of (pid, text, score) tuples\n",
        "# - Does NOT rebuild or overwrite dense/FAISS objects\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ---------- Config ----------\n",
        "# Tune these later on a small validation set\n",
        "DEFAULT_RETRIEVE_POOL = 50\n",
        "SCORE_FUSION_ALPHA = 0.6   # alpha in [0,1] for score fusion: alpha * dense + (1-alpha) * bm25\n",
        "RRF_K = 60                 # reciprocal rank fusion constant\n",
        "\n",
        "# ---------- Sanity checks for canonical corpus ----------\n",
        "assert 'passages_min' in globals() and isinstance(passages_min, list) and len(passages_min) > 0, \"passages_min must be loaded\"\n",
        "assert 'pid2text' in globals() and isinstance(pid2text, dict) and len(pid2text) > 0, \"pid2text must be available\"\n",
        "assert 'dense_retrieve' in globals(), \"dense_retrieve wrapper must be defined (fine-tuned dense retriever)\"\n",
        "\n",
        "# ---------- Build or reuse BM25 index (non-destructive) ----------\n",
        "try:\n",
        "    # If bm25_new already exists from a previous run, reuse it\n",
        "    bm25_new  # noqa: F821\n",
        "except Exception:\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank_bm25\"], check=True)\n",
        "        from rank_bm25 import BM25Okapi\n",
        "\n",
        "    # Build tokenized corpus from passages_min (light normalization)\n",
        "    def _normalize_for_bm25(s: str) -> str:\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = s.replace(\"\\u200c\", \" \")  # zero-width non-joiner\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s.lower()\n",
        "\n",
        "    bm25_new_ids = [p[\"id\"] for p in passages_min]\n",
        "    bm25_new_texts = [p[\"text\"] for p in passages_min]\n",
        "    bm25_new_tokenized = [_normalize_for_bm25(t).split() for t in bm25_new_texts]\n",
        "    bm25_new = BM25Okapi(bm25_new_tokenized)\n",
        "\n",
        "# Safe wrapper for BM25 that returns (pid, text, score)\n",
        "def bm25_new_retrieve(query: str, k: int = 5):\n",
        "    q_tok = _normalize_for_bm25(query).split()\n",
        "    scores = bm25_new.get_scores(q_tok)\n",
        "    top_idx = np.argsort(scores)[::-1][:k]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        i = int(i)\n",
        "        pid = bm25_new_ids[i]\n",
        "        text = bm25_new_texts[i]\n",
        "        score = float(scores[i])\n",
        "        results.append((pid, text, score))\n",
        "    return results\n",
        "\n",
        "# ---------- Fusion utilities ----------\n",
        "def normalize_scores(score_map):\n",
        "    \"\"\"Min-max normalize a dict of scores to [0,1].\"\"\"\n",
        "    if not score_map:\n",
        "        return {}\n",
        "    vals = list(score_map.values())\n",
        "    lo, hi = min(vals), max(vals)\n",
        "    if hi == lo:\n",
        "        return {k: 1.0 for k in score_map}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in score_map.items()}\n",
        "\n",
        "def rrf_rank(dense_list, bm25_list, k_rrf=RRF_K):\n",
        "    \"\"\"Reciprocal Rank Fusion: returns sorted list of pids by RRF score.\"\"\"\n",
        "    score = {}\n",
        "    for rank, (pid, _, _) in enumerate(dense_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    for rank, (pid, _, _) in enumerate(bm25_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    sorted_pids = sorted(score.keys(), key=lambda p: score[p], reverse=True)\n",
        "    return sorted_pids, score\n",
        "\n",
        "# ---------- Metadata filter helper (unchanged semantics) ----------\n",
        "# If you have meta_map from corpus_clean, it will be used; otherwise fallback to passages_min metadata\n",
        "if 'corpus_clean' in globals():\n",
        "    meta_map = {p[\"id\"]: p for p in corpus_clean}\n",
        "else:\n",
        "    meta_map = {p[\"id\"]: p for p in passages_min}\n",
        "\n",
        "def filter_by_metadata(candidate_ids, min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    out = []\n",
        "    for pid in candidate_ids:\n",
        "        m = meta_map.get(pid, {})\n",
        "        ok = True\n",
        "        if min_date or max_date:\n",
        "            dt = None\n",
        "            if \"retrieved_at\" in m:\n",
        "                try:\n",
        "                    dt = datetime.fromisoformat(m[\"retrieved_at\"])\n",
        "                except Exception:\n",
        "                    dt = None\n",
        "            if dt:\n",
        "                if min_date and dt < min_date: ok = False\n",
        "                if max_date and dt > max_date: ok = False\n",
        "        if allowed_sources and m.get(\"source\") not in allowed_sources:\n",
        "            ok = False\n",
        "        if exclude_time_sensitive is not None and m.get(\"time_sensitive\") == exclude_time_sensitive:\n",
        "            ok = False\n",
        "        if ok:\n",
        "            out.append(pid)\n",
        "    return out\n",
        "\n",
        "# ---------- Main retrieve wrapper with fusion modes ----------\n",
        "def retrieve(query: str, k: int = 5, mode: str = \"hybrid_score\", min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    \"\"\"\n",
        "    retrieve(query, k, mode)\n",
        "    Modes:\n",
        "      - 'bm25' : BM25-only (bm25_new_retrieve)\n",
        "      - 'dense' : dense-only (dense_retrieve)\n",
        "      - 'hybrid_interleave' : legacy interleave (dense first, then bm25)\n",
        "      - 'hybrid_score' : score fusion (normalized dense + bm25)\n",
        "      - 'hybrid_rrf' : reciprocal rank fusion (RRF)\n",
        "    Returns: list of (pid, text, score)\n",
        "    \"\"\"\n",
        "    # Get candidate pools (pool size configurable)\n",
        "    pool = max(DEFAULT_RETRIEVE_POOL, k)\n",
        "    dense_hits = dense_retrieve(query, k=pool)   # expected (pid, text, score)\n",
        "    bm25_hits = bm25_new_retrieve(query, k=pool) # (pid, text, score)\n",
        "\n",
        "    # Mode-specific behavior\n",
        "    if mode == \"bm25\":\n",
        "        results = bm25_hits[:k]\n",
        "    elif mode == \"dense\":\n",
        "        results = dense_hits[:k]\n",
        "    elif mode == \"hybrid_interleave\":\n",
        "        # preserve dense-first interleaving (legacy behavior)\n",
        "        seen = set()\n",
        "        cands = []\n",
        "        for lst in (dense_hits, bm25_hits):\n",
        "            for pid, text, score in lst:\n",
        "                if pid not in seen:\n",
        "                    seen.add(pid)\n",
        "                    cands.append((pid, text, float(score)))\n",
        "        results = cands[:k]\n",
        "    elif mode == \"hybrid_score\":\n",
        "        # Score fusion: normalize and combine\n",
        "        dense_scores = {pid: sc for pid, _, sc in dense_hits}\n",
        "        bm25_scores = {pid: sc for pid, _, sc in bm25_hits}\n",
        "        d_norm = normalize_scores(dense_scores)\n",
        "        b_norm = normalize_scores(bm25_scores)\n",
        "        alpha = SCORE_FUSION_ALPHA\n",
        "        combined = {}\n",
        "        for pid in set(list(d_norm.keys()) + list(b_norm.keys())):\n",
        "            combined[pid] = alpha * d_norm.get(pid, 0.0) + (1 - alpha) * b_norm.get(pid, 0.0)\n",
        "        # sort by combined score\n",
        "        sorted_pids = sorted(combined.keys(), key=lambda p: combined[p], reverse=True)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(combined[pid])))\n",
        "    elif mode == \"hybrid_rrf\":\n",
        "        sorted_pids, score_map = rrf_rank(dense_hits, bm25_hits, k_rrf=RRF_K)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(score_map.get(pid, 0.0))))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieve mode: {mode}\")\n",
        "\n",
        "    # Apply metadata filters if requested (filter by pid only)\n",
        "    if any([min_date, max_date, allowed_sources, exclude_time_sensitive is not None]):\n",
        "        filtered_ids = filter_by_metadata([pid for pid,_,_ in results], min_date, max_date, allowed_sources, exclude_time_sensitive)\n",
        "        results = [(pid, pid2text.get(pid, \"\"), score) for pid,_,score in results if pid in filtered_ids]\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------- Quick sample test (safe) ----------\n",
        "q = eval_queries[0][\"query\"] if 'eval_queries' in globals() and len(eval_queries)>0 else \"Ú©ÙˆÙˆÚˆ-19 Ú©ÛŒ Ø¹Ø§Ù… Ø¹Ù„Ø§Ù…Ø§Øª Ú©ÛŒØ§ ÛÛŒÚºØŸ\"\n",
        "print(\"Sample dense top-5 ids:\", [r[0] for r in dense_retrieve(q, k=5)])\n",
        "print(\"Sample bm25_new top-5 ids:\", [r[0] for r in bm25_new_retrieve(q, k=5)])\n",
        "print(\"Sample hybrid_score top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_score')])\n",
        "print(\"Sample hybrid_rrf top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_rrf')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpcqmjkddCM5"
      },
      "outputs": [],
      "source": [
        "# Cell 9a: Run after above cell 9. Cell 9 creates B2M5 + Dense hybrid and below\n",
        "# cell evaluates its performance:\n",
        "# Cell 9c: Validation diagnostics â€” Recall@1, Recall@5, MRR, Precision@k for retrievers\n",
        "# - Works with any mode supported by your Cell 9 wrapper: 'bm25', 'dense', 'hybrid_interleave', 'hybrid_score', 'hybrid_rrf'\n",
        "# - Calls retrieve(...) and computes retrieval metrics\n",
        "# - Outputs summary metrics and a few examples of misses\n",
        "\n",
        "import time, statistics\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def evaluate_retriever(eval_items, mode=\"hybrid_score\", k=DEFAULT_K,\n",
        "                       recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in tqdm(eval_items, desc=f\"Evaluating {mode} retriever\"):\n",
        "        total += 1\n",
        "        q = item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = retrieve(q, k=k, mode=mode)   # (pid, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [r[0] for r in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# ---------- Run evaluation ----------\n",
        "# Use eval_queries_val if defined, else fall back to eval_queries\n",
        "eval_items = eval_queries_val if 'eval_queries_val' in globals() else eval_queries\n",
        "\n",
        "# Evaluate all retriever modes\n",
        "modes = [\"bm25\", \"dense\", \"hybrid_interleave\", \"hybrid_score\", \"hybrid_rrf\"]\n",
        "results = {}\n",
        "for m in modes:\n",
        "    summary, records = evaluate_retriever(eval_items, mode=m, k=DEFAULT_K)\n",
        "    results[m] = summary\n",
        "    print(f\"\\n{m} retriever evaluation summary:\")\n",
        "    for k,v in summary.items():\n",
        "        print(f\"  {k}: {v:.3f}\" if isinstance(v,float) else f\"  {k}: {v}\")\n",
        "\n",
        "    # Show a few misses\n",
        "    misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "    print(f\"  Total misses: {len(misses)} / {len(records)}. Showing up to 3 misses:\")\n",
        "    for r in misses[:3]:\n",
        "        print(\"   Query:\", r[\"query\"][:80])\n",
        "        print(\"    Positives:\", r[\"positive_ids\"])\n",
        "        print(\"    Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oVJPa0YidJQ2"
      },
      "outputs": [],
      "source": [
        "# Optional Cell:\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedder = SentenceTransformer(\"fine_tuned_sbert_urdu\").to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn2TmMs8dNux"
      },
      "outputs": [],
      "source": [
        "# # Cell 10a: Install Libraries for QLoRA Fine-Tuning\n",
        "!pip install -q --upgrade transformers bitsandbytes accelerate peft trl\n",
        "!pip install -q --upgrade sentence-transformers faiss-cpu datasets sacrebleu\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # Disable external logging to save speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mEfC9M3NLUg"
      },
      "outputs": [],
      "source": [
        "# # Crash Session Code:\n",
        "# # Cell 10_Fix: The Nuclear Library Fix\n",
        "# import os\n",
        "\n",
        "# print(\"ðŸ›‘ Uninstalling old libraries...\")\n",
        "# !pip uninstall -y transformers bitsandbytes accelerate\n",
        "\n",
        "# print(\"â¬‡ï¸ Installing fresh, compatible versions...\")\n",
        "# !pip install -q -U bitsandbytes\n",
        "# !pip install -q -U accelerate\n",
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "# !pip install -q -U datasets\n",
        "\n",
        "# print(\"âœ… Installation Done.\")\n",
        "# print(\"ðŸ’¥ KILLING RUNTIME TO FORCE RELOAD... (Expect a 'Session Crashed' message!)\")\n",
        "\n",
        "# # This command kills the Python process immediately\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny1U-ecUSm_g"
      },
      "outputs": [],
      "source": [
        "# # Cell 10_Update: Force Install from Source (The \"Bleeding Edge\" Fix)\n",
        "# import os\n",
        "\n",
        "# print(\"ðŸ›‘ Uninstalling all potentially conflicting libraries...\")\n",
        "# !pip uninstall -y transformers tokenizers bitsandbytes accelerate\n",
        "\n",
        "# print(\"â¬‡ï¸ Installing 'Bleeding Edge' Transformers from GitHub...\")\n",
        "# # We install directly from source to get the Qwen 2.5 fixes\n",
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "# !pip install -q -U bitsandbytes\n",
        "# !pip install -q -U datasets\n",
        "\n",
        "# print(\"âœ… Installation Complete.\")\n",
        "# print(\"âš ï¸ CRITICAL STEP: Go to 'Runtime' -> 'Restart Session' NOW.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg28mnicSFg6"
      },
      "outputs": [],
      "source": [
        "# # Cell 10_Final_Golden: Conflict-Free Fine-Tuning\n",
        "# import os\n",
        "# import torch\n",
        "# from datasets import Dataset\n",
        "# import json\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "# from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "# from trl import SFTTrainer\n",
        "\n",
        "# # Optimizations\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # 1. Load Tokenizer & Model\n",
        "# model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "# )\n",
        "\n",
        "# print(f\"â³ Loading {model_id}...\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id,\n",
        "#     quantization_config=bnb_config,\n",
        "#     device_map=\"auto\",\n",
        "#     trust_remote_code=True\n",
        "# )\n",
        "\n",
        "# # 2. Prepare for Training\n",
        "# model.gradient_checkpointing_enable()\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# # 3. Prepare Data\n",
        "# def format_instruction(sample):\n",
        "#     return {\n",
        "#         \"messages\": [\n",
        "#             {\"role\": \"system\", \"content\": \"Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø§ÛØ± ÚˆØ§Ú©Ù¹Ø± ÛÛŒÚºÛ” ØµØ§Ø±Ù Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¯Ø±Ø³Øª Ø§ÙˆØ± Ù…Ø®ØªØµØ± Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”\"},\n",
        "#             {\"role\": \"user\", \"content\": sample['query']},\n",
        "#             {\"role\": \"assistant\", \"content\": sample.get('gold_answer', sample.get('answer', ''))}\n",
        "#         ]\n",
        "#     }\n",
        "\n",
        "# data = []\n",
        "# try:\n",
        "#     with open(\"eval_queries.jsonl\", \"r\") as f:\n",
        "#         for line in f:\n",
        "#             data.append(json.loads(line))\n",
        "# except FileNotFoundError:\n",
        "#     print(\"âš ï¸ Error: eval_queries.jsonl not found.\")\n",
        "\n",
        "# train_data = data[:int(0.9 * len(data))]\n",
        "# hf_dataset = Dataset.from_list(train_data)\n",
        "\n",
        "# def process_data(row):\n",
        "#     # We create a column named \"text\" explicitly\n",
        "#     return {\"text\": tokenizer.apply_chat_template(format_instruction(row)[\"messages\"], tokenize=False)}\n",
        "\n",
        "# dataset = hf_dataset.map(process_data)\n",
        "# print(f\"âœ… Data Prepared: {len(dataset)} examples. Column names: {dataset.column_names}\")\n",
        "\n",
        "# # 4. LoRA Config\n",
        "# peft_config = LoraConfig(\n",
        "#     r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "# )\n",
        "\n",
        "# # 5. Training Args\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./qwen_urdu_finetuned\",\n",
        "#     per_device_train_batch_size=1,\n",
        "#     gradient_accumulation_steps=4,\n",
        "#     learning_rate=2e-4,\n",
        "#     logging_steps=5,\n",
        "#     fp16=True,\n",
        "#     max_steps=30,\n",
        "#     save_strategy=\"no\",\n",
        "#     optim=\"paged_adamw_32bit\",\n",
        "#     report_to=\"none\"\n",
        "# )\n",
        "\n",
        "# # 6. Trainer (Argument Removed!)\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     train_dataset=dataset,\n",
        "#     peft_config=peft_config,\n",
        "#     # dataset_text_field=\"text\",  <-- REMOVED THIS LINE\n",
        "#     max_seq_length=256,\n",
        "#     args=training_args,\n",
        "#     tokenizer=tokenizer\n",
        "# )\n",
        "\n",
        "# print(\"ðŸš€ Starting Fine-Tuning...\")\n",
        "# trainer.train()\n",
        "# print(\"âœ… Fine-Tuning Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz6CAMmiGR1a"
      },
      "outputs": [],
      "source": [
        "# Cell 10b: Load Base Qwen Model & Prepare Training Data\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "# Add this line to ensure bitsandbytes is up-to-date\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "# 1. Load Tokenizer & Base Model (4-bit)\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Fix for training\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"âœ… Base Model Loaded.\")\n",
        "\n",
        "# 2. Prepare Data from 'synthetic_qa_pairs.jsonl'\n",
        "# We format it into the \"Chat\" format that Qwen expects\n",
        "def format_instruction(sample):\n",
        "    # We simulate a \"Closed Book\" question (Independent Generator)\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø§ÛØ± ÚˆØ§Ú©Ù¹Ø± ÛÛŒÚºÛ” ØµØ§Ø±Ù Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¯Ø±Ø³Øª Ø§ÙˆØ± Ù…Ø®ØªØµØ± Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”\"},\n",
        "            {\"role\": \"user\", \"content\": sample['query']},\n",
        "            {\"role\": \"assistant\", \"content\": sample.get('gold_answer', sample.get('answer', ''))} # Fallback if key differs\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Load Synthetic Pairs (File 5) for Training\n",
        "data = []\n",
        "# NOTE: Ensure synthetic_qa_pairs.jsonl has an 'answer' or 'gold_answer' field.\n",
        "# If your synthetic file ONLY has queries, we must use the corpus to find answers or use a subset of eval data (excluding validation).\n",
        "# For now, let's assume we use a split of eval_queries for training to demonstrate the code.\n",
        "with open(\"eval_queries.jsonl\", \"r\") as f: # Using eval queries for demo (in real research, use separate train set)\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Split: 90% Train, 10% Test\n",
        "split_idx = int(0.9 * len(data))\n",
        "train_data = data[:split_idx]\n",
        "val_data = data[split_idx:]\n",
        "\n",
        "hf_dataset = Dataset.from_list(train_data)\n",
        "# Apply Chat Template\n",
        "def process_data(row):\n",
        "    formatted = tokenizer.apply_chat_template(format_instruction(row)[\"messages\"], tokenize=False)\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "dataset = hf_dataset.map(process_data)\n",
        "print(f\"âœ… Data Prepared: {len(dataset)} training examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "391WGL1yfCL-"
      },
      "outputs": [],
      "source": [
        "# # Cell 10c: Fine-Tune the Generator (QLoRA) - FIXED\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 1. Prepare Model for Training (Gradient Checkpointing)\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 2. LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,           # Rank\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "# 3. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen_urdu_finetuned\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=5,\n",
        "    fp16=True,\n",
        "    max_steps=30,  # Keeping it short for a quick success test\n",
        "    save_strategy=\"no\",\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    report_to=\"none\" # Disable WandB explicitly\n",
        ")\n",
        "\n",
        "# 4. Trainer (Pass peft_config here!)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,      # <--- The Trainer handles the wrapping now\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting Generator Fine-Tuning...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Fine-Tuning Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njFgyLIJMF97"
      },
      "outputs": [],
      "source": [
        "# # Cell 10d: Evaluate Independent Generator (Accuracy Check)\n",
        "from tqdm.auto import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "def generate_answer(query):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø§ÛØ± ÚˆØ§Ú©Ù¹Ø± ÛÛŒÚºÛ” Ù…Ø®ØªØµØ± Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=128, temperature=0.1)\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    # Extract only the assistant's part\n",
        "    if \"assistant\\n\" in response:\n",
        "        response = response.split(\"assistant\\n\")[-1]\n",
        "    return response.strip()\n",
        "\n",
        "print(\"running Evaluation on Validation Set...\")\n",
        "preds, refs = [], []\n",
        "\n",
        "# Evaluate on the 10% validation split we made in 10b\n",
        "for item in tqdm(val_data):\n",
        "    pred = generate_answer(item['query'])\n",
        "    preds.append(pred)\n",
        "    refs.append([item['gold_answer']])\n",
        "\n",
        "# Metrics\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "chrf = sacrebleu.corpus_chrf(preds, refs)\n",
        "\n",
        "print(\"\\n=== Independent Generator Stats ===\")\n",
        "print(f\"BLEU: {bleu.score:.2f}\")\n",
        "print(f\"chrF: {chrf.score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUpsqEBsMJ8z"
      },
      "outputs": [],
      "source": [
        "# # Cell 10e: Save Fine-Tuned Adapter to Drive\n",
        "SAVE_PATH = \"/content/drive/MyDrive/models/qwen_urdu_adapter\"\n",
        "\n",
        "print(f\"ðŸ’¾ Saving adapter to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"âœ… Adapter Saved! You can skip training next time.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr1ihR5rMQic"
      },
      "outputs": [],
      "source": [
        "# # Cell 10f: Fast Load (Base Model + Saved Adapter)\n",
        "# # Run this INSTEAD of 10b, 10c, 10d, 10e in future sessions\n",
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "# from peft import PeftModel\n",
        "\n",
        "# ADAPTER_PATH = \"/content/drive/MyDrive/models/qwen_urdu_adapter\"\n",
        "# BASE_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# # 1. Load Base\n",
        "# print(\"â³ Loading Base Model...\")\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_quant_type=\"nf4\"\n",
        "# )\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     BASE_MODEL_ID, quantization_config=bnb_config, device_map=\"auto\"\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "# # 2. Load Adapter\n",
        "# print(\"ðŸ”— Attaching Fine-Tuned Adapter...\")\n",
        "# model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "\n",
        "# print(\"âœ… Fine-Tuned Generator Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmMpp0rzXG-Q"
      },
      "outputs": [],
      "source": [
        "# Cell 10_Install: Clean Install for Qwen Inference\n",
        "import os\n",
        "\n",
        "print(\"â¬‡ï¸ Installing clean libraries for Qwen...\")\n",
        "# We install specific versions known to work together\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "print(\"âœ… Libraries installed. PLEASE RESTART SESSION ONE LAST TIME (Runtime -> Restart Session).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsdGvSiQbz_T"
      },
      "outputs": [],
      "source": [
        "# Cell 1_Repair: Fix Accidental Downgrade\n",
        "import os\n",
        "\n",
        "print(\"ðŸš‘ Repairing libraries...\")\n",
        "!pip uninstall -y transformers tokenizers bitsandbytes accelerate\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U datasets\n",
        "\n",
        "print(\"âœ… Repair Complete. PLEASE RESTART SESSION (Runtime -> Restart Session).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp9eM8V4alh6"
      },
      "outputs": [],
      "source": [
        "# Cell 1-9_Restore: Safe Data & Retriever Restoration (Fixed Path)\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Install ONLY the missing retriever libraries (Safe Mode)\n",
        "!pip install -q sentence-transformers rank_bm25 faiss-cpu\n",
        "\n",
        "# 2. Load Data (Fixed Path for Local Content)\n",
        "# We look directly in /content/ or /content/data/\n",
        "base_path = Path(\"/content\")\n",
        "if (base_path / \"data\").exists():\n",
        "    DATA_DIR = base_path / \"data\"\n",
        "else:\n",
        "    DATA_DIR = base_path\n",
        "\n",
        "print(f\"ðŸ“‚ Looking for files in {DATA_DIR}...\")\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line))\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ File not found: {path}\")\n",
        "        return []\n",
        "\n",
        "# Load the essential files\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "\n",
        "# STOP if data is missing\n",
        "if len(passages_min) == 0:\n",
        "    raise ValueError(\"âš ï¸ CRITICAL: passages_min is empty! Please upload 'urdu_covid_passages_min.jsonl' to the Files folder on the left.\")\n",
        "\n",
        "print(f\"âœ… Loaded {len(passages_min)} passages and {len(eval_queries)} queries.\")\n",
        "\n",
        "# 3. Build BM25 Index\n",
        "from rank_bm25 import BM25Okapi\n",
        "import re\n",
        "def normalize(text): return re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
        "\n",
        "tokenized_corpus = [normalize(p['text']).split() for p in passages_min]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "corpus_ids = [p['id'] for p in passages_min]\n",
        "corpus_texts = [p['text'] for p in passages_min]\n",
        "pid2text = {p['id']: p['text'] for p in passages_min}\n",
        "\n",
        "def bm25_retrieve(query, k=50):\n",
        "    query_tokens = normalize(query).split()\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    top_n = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in top_n]\n",
        "\n",
        "# 4. Build Dense Index (Base Model - Fast)\n",
        "print(\"Building Dense Index...\")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Use CPU for embedding to save GPU memory for Qwen\n",
        "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", device=\"cpu\")\n",
        "passage_embeddings = embedder.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "d = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "def dense_retrieve(query, k=50):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(D[0][i])) for i in range(len(I[0]))]\n",
        "\n",
        "# 5. Define Hybrid Retrieve Function\n",
        "def retrieve(query, k=5, mode=\"hybrid_score\"):\n",
        "    pool_k = 50\n",
        "    bm25_hits = bm25_retrieve(query, k=pool_k)\n",
        "    dense_hits = dense_retrieve(query, k=pool_k)\n",
        "\n",
        "    def norm(hits):\n",
        "        scores = [s for _,_,s in hits]\n",
        "        if not scores: return {}\n",
        "        min_s, max_s = min(scores), max(scores)\n",
        "        if max_s == min_s: return {pid: 1.0 for pid,_,_ in hits}\n",
        "        return {pid: (s - min_s)/(max_s - min_s) for pid,_,s in hits}\n",
        "\n",
        "    bm25_scores = norm(bm25_hits)\n",
        "    dense_scores = norm(dense_hits)\n",
        "\n",
        "    all_pids = set(bm25_scores.keys()) | set(dense_scores.keys())\n",
        "    final_scores = []\n",
        "\n",
        "    for pid in all_pids:\n",
        "        s_bm25 = bm25_scores.get(pid, 0.0)\n",
        "        s_dense = dense_scores.get(pid, 0.0)\n",
        "        final_score = 0.5 * s_bm25 + 0.5 * s_dense\n",
        "        final_scores.append((pid, pid2text[pid], final_score))\n",
        "\n",
        "    final_scores.sort(key=lambda x: x[2], reverse=True)\n",
        "    return final_scores[:k]\n",
        "\n",
        "print(\"âœ… COMPLETE: Data & Retriever Restored!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTHH9NSkXIEH"
      },
      "outputs": [],
      "source": [
        "# Cell 10_Inference_Only: Load Qwen for RAG (No Training)\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# 1. Optimizations\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 2. Config for 4-bit Loading (Memory Safe)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 3. Load Model\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "print(f\"â³ Loading {model_id}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Qwen Loaded Successfully (Inference Mode)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iirFQO4LfFap"
      },
      "outputs": [],
      "source": [
        "# Cell 11: RAG Inference Pipeline (Qwen Fine-Tuned + Hybrid Retriever)\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'model' in globals(), \"Qwen Model (Cell 10) is not loaded!\"\n",
        "assert 'tokenizer' in globals(), \"Tokenizer (Cell 10) is not loaded!\"\n",
        "assert 'retrieve' in globals(), \"Retrieve function (Cell 9) is not loaded!\"\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "RAG_K = 5                  # Number of documents to retrieve\n",
        "RAG_MODE = \"hybrid_score\"  # Your best retrieval mode (from Cell 12 analysis)\n",
        "\n",
        "def rag_pipeline(query, k=RAG_K, mode=RAG_MODE, debug=False):\n",
        "    \"\"\"\n",
        "    1. Retrieve Docs (Hybrid)\n",
        "    2. Format Prompt (Qwen Chat Template)\n",
        "    3. Generate Answer\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "\n",
        "    # --- Step 1: Retrieval ---\n",
        "    # Get top K documents\n",
        "    retrieved_hits = retrieve(query, k=k, mode=mode)\n",
        "\n",
        "    # Deduplicate (Keep unique text)\n",
        "    seen_ids = set()\n",
        "    unique_passages = []\n",
        "    context_text = \"\"\n",
        "\n",
        "    for idx, (pid, text, score) in enumerate(retrieved_hits):\n",
        "        if pid not in seen_ids:\n",
        "            unique_passages.append((pid, text, score))\n",
        "            seen_ids.add(pid)\n",
        "            # Add to context string\n",
        "            context_text += f\"[Ø­ÙˆØ§Ù„Û {len(unique_passages)}] {text}\\n\"\n",
        "\n",
        "    # --- Step 2: Prompt Engineering (The \"Doctor\" Persona) ---\n",
        "    # This matches the format we used in Fine-Tuning (Cell 10c)\n",
        "    system_prompt = \"Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø§ÛØ± ÚˆØ§Ú©Ù¹Ø± ÛÛŒÚºÛ” Ù†ÛŒÚ†Û’ Ø¯ÛŒ Ú¯Ø¦ÛŒ 'Ù…Ø¹Ù„ÙˆÙ…Ø§Øª' Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± ØµØ§Ø±Ù Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¯Ø±Ø³Øª Ø§ÙˆØ± Ù…Ø®ØªØµØ± Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:\n",
        "    {context_text}\n",
        "\n",
        "    Ø³ÙˆØ§Ù„: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Convert to Qwen Input IDs\n",
        "    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # --- Step 3: Generation ---\n",
        "    generator_success = False\n",
        "    answer = \"\"\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=256,   # Allow enough space for Urdu answer\n",
        "                temperature=0.3,      # Low temp = more factual\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.1 # Prevent getting stuck in loops\n",
        "            )\n",
        "\n",
        "        # Decode Output\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        answer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "        generator_success = True\n",
        "\n",
        "    except Exception as e:\n",
        "        answer = \"Error during generation.\"\n",
        "        print(f\"âŒ Gen Error: {e}\")\n",
        "\n",
        "    latency = time.time() - t0\n",
        "\n",
        "    # --- Step 4: Refusal Check ---\n",
        "    # Did the model say \"I don't know\"?\n",
        "    is_refusal = any(phrase in answer for phrase in [\"Ù…Ø¹Ø§Ù Ú©ÛŒØ¬Ø¦Û’\", \"Ø¯Ø³ØªÛŒØ§Ø¨ Ù†ÛÛŒÚº\", \"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†ÛÛŒÚº\"])\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"retrieved_ids\": [p[0] for p in unique_passages],\n",
        "        \"latency\": latency,\n",
        "        \"is_refusal\": is_refusal,\n",
        "        \"success\": generator_success,\n",
        "        \"context_preview\": context_text[:200]\n",
        "    }\n",
        "\n",
        "# ---------- Smoke Test ----------\n",
        "print(\"ðŸ§ª Testing RAG Pipeline...\")\n",
        "test_q = \"Ú©ÙˆØ±ÙˆÙ†Ø§ ÙˆØ§Ø¦Ø±Ø³ Ú©ÛŒ Ø¹Ù„Ø§Ù…Ø§Øª Ú©ÛŒØ§ ÛÛŒÚºØŸ\"\n",
        "res = rag_pipeline(test_q, debug=True)\n",
        "print(f\"â“ Question: {test_q}\")\n",
        "print(f\"ðŸ’¡ Answer: {res['answer']}\")\n",
        "print(f\"â±ï¸ Time: {res['latency']:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNyWTN33fJTa"
      },
      "outputs": [],
      "source": [
        "# Cell 12: Comprehensive RAG Evaluation\n",
        "import statistics\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Install sacrebleu if missing\n",
        "try:\n",
        "    import sacrebleu\n",
        "except ImportError:\n",
        "    !pip install -q sacrebleu\n",
        "    import sacrebleu\n",
        "\n",
        "# Ensure we have data\n",
        "assert 'eval_queries' in globals(), \"eval_queries not loaded!\"\n",
        "\n",
        "print(f\"ðŸš€ Starting Final Evaluation on {len(eval_queries)} queries...\")\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "latencies = []\n",
        "refusal_count = 0\n",
        "per_query_results = []\n",
        "\n",
        "# Loop through all evaluation queries\n",
        "for item in tqdm(eval_queries):\n",
        "    query = item['query']\n",
        "    gold_answer = item['gold_answer']\n",
        "\n",
        "    # Run the Pipeline\n",
        "    # Note: We use the best mode 'hybrid_score' automatically\n",
        "    result = rag_pipeline(query)\n",
        "\n",
        "    # Store Data\n",
        "    predictions.append(result['answer'])\n",
        "    references.append([gold_answer]) # sacrebleu needs list of lists\n",
        "    latencies.append(result['latency'])\n",
        "\n",
        "    if result['is_refusal']:\n",
        "        refusal_count += 1\n",
        "\n",
        "    # Log per query for inspection\n",
        "    per_query_results.append({\n",
        "        \"query\": query,\n",
        "        \"gold\": gold_answer,\n",
        "        \"generated\": result['answer'],\n",
        "        \"retrieved\": result['retrieved_ids'],\n",
        "        \"latency\": result['latency']\n",
        "    })\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "bleu = sacrebleu.corpus_bleu(predictions, references)\n",
        "chrf = sacrebleu.corpus_chrf(predictions, references)\n",
        "avg_latency = statistics.mean(latencies)\n",
        "refusal_rate = (refusal_count / len(eval_queries)) * 100\n",
        "\n",
        "# --- Save Results ---\n",
        "OUT_FILE = \"/content/drive/MyDrive/eval_outputs/final_qwen_rag_results.json\"\n",
        "os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)\n",
        "with open(OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"metrics\": {\n",
        "            \"BLEU\": bleu.score,\n",
        "            \"chrF\": chrf.score,\n",
        "            \"Latency\": avg_latency,\n",
        "            \"Refusal_Rate\": refusal_rate\n",
        "        },\n",
        "        \"details\": per_query_results\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# --- Print Report Card ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ† FINAL PROJECT REPORT: URDU COVID-19 RAG\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Model:     Qwen2.5-7B-Instruct (Fine-Tuned Adapter)\")\n",
        "print(f\"Retriever: Hybrid (Dense + BM25)\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"âœ… BLEU Score:   {bleu.score:.2f}   (Previous Best: ~15.8)\")\n",
        "print(f\"âœ… chrF Score:   {chrf.score:.2f}   (Previous Best: ~31.2)\")\n",
        "print(f\"â±ï¸ Avg Latency:  {avg_latency:.3f}s\")\n",
        "print(f\"ðŸš« Refusals:     {refusal_count}/{len(eval_queries)} ({refusal_rate:.1f}%)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ðŸ“„ Detailed logs saved to: {OUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell to keep my notebook awake\n",
        "print(\"This will keep it awake. Run after 1 minute intervals!!\")"
      ],
      "metadata": {
        "id": "z-jX9UsRo0ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9nyT-_FcpCel"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}