{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedsaalman/low-resource-rag-comparison/blob/main/Retriever_Model_NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cells 1 - 5b: Sparse Retriver Model (B2M5)**\n",
        "\n",
        "**Cells 6 - 8c: Dense Retriver Model and its fine tuning (FAISS)**\n",
        "\n",
        "**Cells 9 - 9b: Hybrid/Finalized Retriver Model (Both B2M5 and Dense fused together)**"
      ],
      "metadata": {
        "id": "kXlzM8PUuksQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "thG_G4Kh7WsV"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install required libraries (run this cell first and one by one all required libraries will be installed)\n",
        "# - transformers: model + generation\n",
        "# - sentence-transformers: dense embeddings / fine-tuning helpers\n",
        "# - faiss-cpu (or faiss-gpu if GPU available)\n",
        "# - rank_bm25: BM25 baseline\n",
        "# - datasets: convenient JSONL loading\n",
        "# - evaluate / sacrebleu: BLEU/chrF metrics\n",
        "# - tqdm: progress bars\n",
        "# - accelerate (optional) for distributed/faster training\n",
        "!pip install -q transformers sentence-transformers faiss-cpu rank_bm25 datasets evaluate sacrebleu tqdm accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive if not already connected\n",
        "# 2. Mount Google Drive\n",
        "# We need this to load your fine-tuned Dense Retriever and your Corpus file.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4PtiDaVl9O-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional cell\n",
        "# To add all the required files run\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "WDLVmIv-9UN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list # Optional to run this cell: To check which of the libraries/packages have been installed"
      ],
      "metadata": {
        "id": "whXUPVz8Bi65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and GPU check: Run this cell after the first cell\n",
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
        "import sentence_transformers # Import the package itself to access __version__\n",
        "\n",
        "# FAISS and BM25\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Datasets and metrics\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "import sacrebleu\n",
        "\n",
        "# Print versions and GPU info\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"torch not available:\", e)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E37rhn4DBj_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "DATA_DIR = Path(\"drive/MyDrive/data\")  # change if files are elsewhere\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "with open(DATA_DIR / \"urdu_covid_passages.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Use split(None, 1) to split on the first occurrence of any whitespace\n",
        "        # This handles cases where the delimiter might be spaces instead of a tab.\n",
        "        if line.strip(): # Ensure line is not empty after stripping whitespace\n",
        "            parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "            if len(parts) == 2:\n",
        "                pid, text = parts\n",
        "                passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "            else:\n",
        "                print(f\"Skipping malformed line in urdu_covid_passages.tsv: {line.strip()}\")\n",
        "\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "print(\"Loaded:\", len(corpus_clean), \"corpus_clean; \", len(passages_min), \"passages_min; \", len(eval_queries), \"eval queries\")\n"
      ],
      "metadata": {
        "id": "-f6qd_ZhF5GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Validate IDs referenced in eval/synthetic/hard_negatives exist in corpus\n",
        "# Run this after Cell 3.\n",
        "passage_ids = {p[\"id\"] for p in passages_min}\n",
        "missing = []\n",
        "for q in eval_queries:\n",
        "    for pid in q.get(\"positive_ids\", []):\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"eval\", q[\"query_id\"], pid))\n",
        "for s in synthetic_pairs:\n",
        "    if s[\"positive_id\"] not in passage_ids:\n",
        "        missing.append((\"synthetic\", s[\"synthetic_id\"], s[\"positive_id\"]))\n",
        "for h in hard_negatives:\n",
        "    for pid in h[\"hard_negatives\"]:\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"hardneg\", h[\"query_id\"], pid))\n",
        "print(\"Missing references (should be zero):\", len(missing))\n",
        "if missing:\n",
        "    print(missing[:10])\n"
      ],
      "metadata": {
        "id": "167ZFXrCtoVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 (Run after Cell 4): BM25 baseline index (tokenize with simple whitespace; for Urdu this is OK as baseline)\n",
        "# We'll store tokenized corpus and BM25 object for retrieval.\n",
        "from nltk.tokenize import word_tokenize\n",
        "# If nltk not installed, use simple split\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Added to resolve LookupError for 'punkt_tab'\n",
        "    tokenizer = lambda s: word_tokenize(s)\n",
        "except Exception:\n",
        "    tokenizer = lambda s: s.split()\n",
        "\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "corpus_ids = [p[\"id\"] for p in passages_min]\n",
        "tokenized_corpus = [tokenizer(t) for t in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Example retrieval function\n",
        "def bm25_retrieve(query, k=5):\n",
        "    q_tokens = tokenizer(query)\n",
        "    scores = bm25.get_scores(q_tokens)\n",
        "    topk = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in topk]\n",
        "\n",
        "# Quick test\n",
        "print(\"BM25 top-3 for sample:\", bm25_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ],
      "metadata": {
        "id": "CrMWc6dotsbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5b: BM25-only retriever evaluation tool (run after Cell 5)\n",
        "# Purpose: standalone evaluation harness for the independent BM25 retriever (bm25_retrieve)\n",
        "# Metrics included (applicable to a retriever-only evaluation):\n",
        "#   - Recall@1, Recall@5\n",
        "#   - MRR (Mean Reciprocal Rank)\n",
        "#   - Precision@k (k=1,5)\n",
        "#   - Average / median retrieval latency\n",
        "#   - Optional: match by gold_passage_id or by substring match of gold_answer\n",
        "# Output:\n",
        "#   - Per-query JSONL saved to bm25_eval_results.jsonl\n",
        "#   - Printed summary with all metrics\n",
        "#\n",
        "# Requirements (must be available in the session):\n",
        "#   - bm25_retrieve(query, k) -> list of (passage_id, passage_text, score)\n",
        "#   - eval_queries: list of dicts with at least a query field and optionally:\n",
        "#       * \"question\" or \"query\" or \"q\"  (the query text)\n",
        "#       * \"gold_passage_id\" (optional) OR \"answer\"/\"gold\" (gold text to match)\n",
        "#\n",
        "# Usage:\n",
        "#   - Run this cell after you build the BM25 index (Cell 5).\n",
        "#   - Optionally pass a different eval list or k values to evaluate subsets.\n",
        "\n",
        "# Use this evaluator if your eval_queries items contain \"positive_ids\" and \"gold_answer\"\n",
        "import json, time, re, statistics\n",
        "from typing import List, Dict\n",
        "\n",
        "OUT_JSONL = \"bm25_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def get_query_text(item: Dict) -> str:\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_bm25_with_positive_ids(eval_items: List[Dict],\n",
        "                                    out_jsonl: str = OUT_JSONL,\n",
        "                                    k: int = DEFAULT_K,\n",
        "                                    recall_ks = RECALL_KS,\n",
        "                                    precision_ks = PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        # normalize to list of strings\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or item.get(\"answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = bm25_retrieve(q, k=k)   # (id, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] bm25_retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k (multiple positives supported)\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            # precision@k = (# positives in top-k) / k\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [t[:300] for t in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run it\n",
        "if 'eval_queries' not in globals():\n",
        "    # try to load from file if not in memory\n",
        "    eval_queries = []\n",
        "    with open(\"eval_queries.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            eval_queries.append(json.loads(line))\n",
        "\n",
        "summary, records = evaluate_bm25_with_positive_ids(eval_queries, out_jsonl=OUT_JSONL, k=DEFAULT_K)\n",
        "print(\"BM25 retrieval evaluation summary:\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# show a few examples where retrieval missed positives\n",
        "misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "print(f\"\\nTotal misses: {len(misses)} / {len(records)}. Showing up to 5 misses:\")\n",
        "for r in misses[:5]:\n",
        "    print(\"Query id:\", r.get(\"query_id\"), \"Query:\", r[\"query\"][:80])\n",
        "    print(\" Positives:\", r[\"positive_ids\"])\n",
        "    print(\" Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "ogC1jaFituYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Dense embeddings with a multilingual model (use a compact model for Colab)\n",
        "# We use a multilingual SBERT model that supports Urdu reasonably (e.g., 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embed_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "# Compute embeddings for passages_min (batching)\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS index (cosine similarity via normalized vectors)\n",
        "d = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)  # inner product\n",
        "# normalize embeddings for cosine\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# Map index positions to ids\n",
        "# retrieval function\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "# Quick test\n",
        "print(\"Dense top-3:\", dense_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ],
      "metadata": {
        "id": "a4rLWra5tw3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6b: Evaluation of dense retriever (run after Cell 6)\n",
        "# Purpose: measure Recall@1, Recall@5, MRR, Precision@k, latency for dense_retrieve\n",
        "# Uses eval_queries with \"positive_ids\" and \"gold_answer\" fields\n",
        "\n",
        "import json, time, re, statistics\n",
        "\n",
        "OUT_JSONL_DENSE = \"dense_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s):\n",
        "    if s is None: return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "\n",
        "def get_query_text(item):\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_dense(eval_items, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K,\n",
        "                   recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies, rr_list = [], []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        pos_ids = item.get(\"positive_ids\") or []\n",
        "        if isinstance(pos_ids, str): pos_ids = [pos_ids]\n",
        "        pos_ids = [str(x) for x in pos_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        hits = dense_retrieve(q, k=k)  # (id, text, score)\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in pos_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in pos_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in pos_ids)\n",
        "            precision_sums[pk] += num_pos_in_topk / pk\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": pos_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [txt[:300] for txt in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": sum(rr_list)/n,\n",
        "        **{f\"Recall@{rk}\": recall_counts[rk]/n for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_sums[pk]/n for pk in precision_ks},\n",
        "        \"latency_mean_s\": statistics.mean(latencies) if latencies else 0.0,\n",
        "        \"latency_median_s\": statistics.median(latencies) if latencies else 0.0\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run evaluation\n",
        "print(\"[dense_eval] Running dense retriever evaluation...\")\n",
        "summary_dense, records_dense = evaluate_dense(eval_queries, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K)\n",
        "print(\"\\nDense retriever evaluation summary:\")\n",
        "for k,v in summary_dense.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Show a few examples\n",
        "print(\"\\nExamples (first 5):\")\n",
        "for r in records_dense[:5]:\n",
        "    print(\" - Query:\", r[\"query\"][:80])\n",
        "    print(\"   Retrieved ids:\", r[\"retrieved_ids\"][:6])\n",
        "    print(\"   Reciprocal rank:\", r[\"reciprocal_rank\"], \"Latency(s):\", round(r[\"latency\"], 4))\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "5xDWX7gktzLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Prepare InputExamples for sentence-transformers fine-tuning i.e. of dense retriever model\n",
        "# Now with an 80/20 train/validation split\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "import random\n",
        "\n",
        "pid2text = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "examples = []\n",
        "for s in synthetic_pairs:\n",
        "    q = s[\"query\"]\n",
        "    pos = pid2text.get(s[\"positive_id\"])\n",
        "    neg = None\n",
        "    # Find hard negatives if available\n",
        "    hn = next((h for h in hard_negatives if h[\"query_id\"] == s.get(\"synthetic_id\", s.get(\"query_id\"))), None)\n",
        "    if hn:\n",
        "        for nid in hn[\"hard_negatives\"]:\n",
        "            if nid != s[\"positive_id\"]:\n",
        "                neg = pid2text.get(nid)\n",
        "                break\n",
        "    if neg is None:\n",
        "        # fallback: random negative\n",
        "        neg_id = random.choice([pid for pid in corpus_ids if pid != s[\"positive_id\"]])\n",
        "        neg = pid2text[neg_id]\n",
        "    if pos and neg:\n",
        "        examples.append(InputExample(texts=[q, pos, neg]))\n",
        "\n",
        "print(\"Prepared\", len(examples), \"triplet examples.\")\n",
        "\n",
        "# --- Split into train/validation (80/20) ---\n",
        "random.shuffle(examples)\n",
        "split_idx = int(0.8 * len(examples))\n",
        "train_examples = examples[:split_idx]\n",
        "val_examples = examples[split_idx:]\n",
        "\n",
        "print(\"Train examples:\", len(train_examples))\n",
        "print(\"Validation examples:\", len(val_examples))\n"
      ],
      "metadata": {
        "id": "-v1VMOMft1Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 (use in-memory model; do NOT reload): Fine-tune SBERT with triplet loss and IR validation on passages_min\n",
        "import os\n",
        "# --- GRANDMASTER FIX: DISABLE WANDB ---\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "# --------------------------------------\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, losses, evaluation\n",
        "#import faiss\n",
        "\n",
        "# Sanity checks\n",
        "assert isinstance(train_examples, list) and len(train_examples) > 0, \"train_examples must be a non-empty list\"\n",
        "assert 'passages_min' in globals(), \"passages_min must be loaded\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "\n",
        "# Build validation split against real corpus & labels\n",
        "# (Check if eval_queries_val exists, otherwise split eval_queries)\n",
        "eval_val = eval_queries_val if 'eval_queries_val' in globals() else eval_queries[int(0.8*len(eval_queries)):]\n",
        "\n",
        "val_queries_dict = {it[\"query_id\"]: it[\"query\"] for it in eval_val}\n",
        "# Fix: Ensure positive_ids is a list\n",
        "val_relevant_dict = {it[\"query_id\"]: set(it[\"positive_ids\"] if isinstance(it[\"positive_ids\"], list) else [it[\"positive_ids\"]]) for it in eval_val}\n",
        "val_corpus_dict = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "# Warn if labels reference missing ids\n",
        "missing = []\n",
        "for qid, rels in val_relevant_dict.items():\n",
        "    for pid in rels:\n",
        "        if pid not in val_corpus_dict:\n",
        "            missing.append((qid, pid))\n",
        "if missing:\n",
        "    print(f\"Warning: {len(missing)} relevant ids not found in corpus. Example:\", missing[:3])\n",
        "\n",
        "# Construct evaluator (defaults to cosine similarity)\n",
        "retrieval_evaluator = evaluation.InformationRetrievalEvaluator(\n",
        "    queries=val_queries_dict,\n",
        "    corpus=val_corpus_dict,\n",
        "    relevant_docs=val_relevant_dict,\n",
        "    name=\"val_ir_passages\"\n",
        ")\n",
        "\n",
        "# Start from baseline multilingual MiniLM\n",
        "# We use the variable 'embedder' from Cell 6 to ensure we continue correctly\n",
        "if 'embedder' not in globals():\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embedder.to(\"cuda\")\n",
        "\n",
        "# Triplet loss with conservative settings\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.TripletLoss(\n",
        "    model=embedder,\n",
        "    distance_metric=losses.TripletDistanceMetric.COSINE,\n",
        "    triplet_margin=0.3\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
        "optimizer_params = {'lr': 2e-5}\n",
        "\n",
        "print(\"Starting fine-tuning (WandB Disabled)...\")\n",
        "\n",
        "# Train with IR evaluator\n",
        "embedder.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=retrieval_evaluator,\n",
        "    epochs=num_epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    optimizer_params=optimizer_params,\n",
        "    show_progress_bar=True,\n",
        "    output_path=\"fine_tuned_sbert_urdu_passages\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\")"
      ],
      "metadata": {
        "id": "uFrENz5Lt77O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8b: Save the Fine-Tuned Model to Drive (Run ONLY if satisfied with accuracy)\n",
        "import os\n",
        "\n",
        "# Define path\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "\n",
        "print(f\"ðŸ’¾ Saving model to {MODEL_SAVE_PATH} ...\")\n",
        "\n",
        "# Create directory if not exists\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "embedder.save(MODEL_SAVE_PATH)\n",
        "\n",
        "print(f\"âœ… Model saved! You can now use Cell 8c in future sessions to skip training.\")"
      ],
      "metadata": {
        "id": "TgHNPWrVt83W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8c: FAST START - Load Model from Drive & Rebuild FAISS (Skips Training)\n",
        "# Run this INSTEAD of Cells 6, 7, 8, 8b in future sessions.\n",
        "\n",
        "import os\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "\n",
        "# 1. Load the Model\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"ðŸ“‚ Loading saved model from: {MODEL_SAVE_PATH}\")\n",
        "    embedder = SentenceTransformer(MODEL_SAVE_PATH).to(\"cuda\")\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"âŒ No saved model found at {MODEL_SAVE_PATH}. Please run Cell 8 & 8b once to create it!\")\n",
        "\n",
        "# 2. Rebuild FAISS Index (Critical Step)\n",
        "# We must re-encode the corpus because we just loaded a specific model\n",
        "print(\"â³ Generating embeddings for corpus...\")\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "\n",
        "# Generate embeddings\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index = faiss.IndexFlatIP(passage_embeddings.shape[1])\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# 3. Define the Retrieval Function\n",
        "# (We must re-define this here because we skipped the previous cells that defined it)\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "print(\"âœ… Dense Retriever System Restored & Ready for Hybrid Fusion (Cell 9).\")"
      ],
      "metadata": {
        "id": "Du-Vybaut_hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run cell 6b again to test the improvement of our dense retriever model after fine tuning."
      ],
      "metadata": {
        "id": "Rk1X8O__uUmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 (final): Retriever wrapper with true fusion modes (non-destructive)\n",
        "# - Creates bm25_new only if not present\n",
        "# - Supports modes: 'bm25', 'dense', 'hybrid_interleave' (legacy), 'hybrid_score', 'hybrid_rrf'\n",
        "# - Returns list of (pid, text, score) tuples\n",
        "# - Does NOT rebuild or overwrite dense/FAISS objects\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ---------- Config ----------\n",
        "# Tune these later on a small validation set\n",
        "DEFAULT_RETRIEVE_POOL = 50\n",
        "SCORE_FUSION_ALPHA = 0.6   # alpha in [0,1] for score fusion: alpha * dense + (1-alpha) * bm25\n",
        "RRF_K = 60                 # reciprocal rank fusion constant\n",
        "\n",
        "# ---------- Sanity checks for canonical corpus ----------\n",
        "assert 'passages_min' in globals() and isinstance(passages_min, list) and len(passages_min) > 0, \"passages_min must be loaded\"\n",
        "assert 'pid2text' in globals() and isinstance(pid2text, dict) and len(pid2text) > 0, \"pid2text must be available\"\n",
        "assert 'dense_retrieve' in globals(), \"dense_retrieve wrapper must be defined (fine-tuned dense retriever)\"\n",
        "\n",
        "# ---------- Build or reuse BM25 index (non-destructive) ----------\n",
        "try:\n",
        "    # If bm25_new already exists from a previous run, reuse it\n",
        "    bm25_new  # noqa: F821\n",
        "except Exception:\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank_bm25\"], check=True)\n",
        "        from rank_bm25 import BM25Okapi\n",
        "\n",
        "    # Build tokenized corpus from passages_min (light normalization)\n",
        "    def _normalize_for_bm25(s: str) -> str:\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = s.replace(\"\\u200c\", \" \")  # zero-width non-joiner\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s.lower()\n",
        "\n",
        "    bm25_new_ids = [p[\"id\"] for p in passages_min]\n",
        "    bm25_new_texts = [p[\"text\"] for p in passages_min]\n",
        "    bm25_new_tokenized = [_normalize_for_bm25(t).split() for t in bm25_new_texts]\n",
        "    bm25_new = BM25Okapi(bm25_new_tokenized)\n",
        "\n",
        "# Safe wrapper for BM25 that returns (pid, text, score)\n",
        "def bm25_new_retrieve(query: str, k: int = 5):\n",
        "    q_tok = _normalize_for_bm25(query).split()\n",
        "    scores = bm25_new.get_scores(q_tok)\n",
        "    top_idx = np.argsort(scores)[::-1][:k]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        i = int(i)\n",
        "        pid = bm25_new_ids[i]\n",
        "        text = bm25_new_texts[i]\n",
        "        score = float(scores[i])\n",
        "        results.append((pid, text, score))\n",
        "    return results\n",
        "\n",
        "# ---------- Fusion utilities ----------\n",
        "def normalize_scores(score_map):\n",
        "    \"\"\"Min-max normalize a dict of scores to [0,1].\"\"\"\n",
        "    if not score_map:\n",
        "        return {}\n",
        "    vals = list(score_map.values())\n",
        "    lo, hi = min(vals), max(vals)\n",
        "    if hi == lo:\n",
        "        return {k: 1.0 for k in score_map}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in score_map.items()}\n",
        "\n",
        "def rrf_rank(dense_list, bm25_list, k_rrf=RRF_K):\n",
        "    \"\"\"Reciprocal Rank Fusion: returns sorted list of pids by RRF score.\"\"\"\n",
        "    score = {}\n",
        "    for rank, (pid, _, _) in enumerate(dense_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    for rank, (pid, _, _) in enumerate(bm25_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    sorted_pids = sorted(score.keys(), key=lambda p: score[p], reverse=True)\n",
        "    return sorted_pids, score\n",
        "\n",
        "# ---------- Metadata filter helper (unchanged semantics) ----------\n",
        "# If you have meta_map from corpus_clean, it will be used; otherwise fallback to passages_min metadata\n",
        "if 'corpus_clean' in globals():\n",
        "    meta_map = {p[\"id\"]: p for p in corpus_clean}\n",
        "else:\n",
        "    meta_map = {p[\"id\"]: p for p in passages_min}\n",
        "\n",
        "def filter_by_metadata(candidate_ids, min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    out = []\n",
        "    for pid in candidate_ids:\n",
        "        m = meta_map.get(pid, {})\n",
        "        ok = True\n",
        "        if min_date or max_date:\n",
        "            dt = None\n",
        "            if \"retrieved_at\" in m:\n",
        "                try:\n",
        "                    dt = datetime.fromisoformat(m[\"retrieved_at\"])\n",
        "                except Exception:\n",
        "                    dt = None\n",
        "            if dt:\n",
        "                if min_date and dt < min_date: ok = False\n",
        "                if max_date and dt > max_date: ok = False\n",
        "        if allowed_sources and m.get(\"source\") not in allowed_sources:\n",
        "            ok = False\n",
        "        if exclude_time_sensitive is not None and m.get(\"time_sensitive\") == exclude_time_sensitive:\n",
        "            ok = False\n",
        "        if ok:\n",
        "            out.append(pid)\n",
        "    return out\n",
        "\n",
        "# ---------- Main retrieve wrapper with fusion modes ----------\n",
        "def retrieve(query: str, k: int = 5, mode: str = \"hybrid_score\", min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    \"\"\"\n",
        "    retrieve(query, k, mode)\n",
        "    Modes:\n",
        "      - 'bm25' : BM25-only (bm25_new_retrieve)\n",
        "      - 'dense' : dense-only (dense_retrieve)\n",
        "      - 'hybrid_interleave' : legacy interleave (dense first, then bm25)\n",
        "      - 'hybrid_score' : score fusion (normalized dense + bm25)\n",
        "      - 'hybrid_rrf' : reciprocal rank fusion (RRF)\n",
        "    Returns: list of (pid, text, score)\n",
        "    \"\"\"\n",
        "    # Get candidate pools (pool size configurable)\n",
        "    pool = max(DEFAULT_RETRIEVE_POOL, k)\n",
        "    dense_hits = dense_retrieve(query, k=pool)   # expected (pid, text, score)\n",
        "    bm25_hits = bm25_new_retrieve(query, k=pool) # (pid, text, score)\n",
        "\n",
        "    # Mode-specific behavior\n",
        "    if mode == \"bm25\":\n",
        "        results = bm25_hits[:k]\n",
        "    elif mode == \"dense\":\n",
        "        results = dense_hits[:k]\n",
        "    elif mode == \"hybrid_interleave\":\n",
        "        # preserve dense-first interleaving (legacy behavior)\n",
        "        seen = set()\n",
        "        cands = []\n",
        "        for lst in (dense_hits, bm25_hits):\n",
        "            for pid, text, score in lst:\n",
        "                if pid not in seen:\n",
        "                    seen.add(pid)\n",
        "                    cands.append((pid, text, float(score)))\n",
        "        results = cands[:k]\n",
        "    elif mode == \"hybrid_score\":\n",
        "        # Score fusion: normalize and combine\n",
        "        dense_scores = {pid: sc for pid, _, sc in dense_hits}\n",
        "        bm25_scores = {pid: sc for pid, _, sc in bm25_hits}\n",
        "        d_norm = normalize_scores(dense_scores)\n",
        "        b_norm = normalize_scores(bm25_scores)\n",
        "        alpha = SCORE_FUSION_ALPHA\n",
        "        combined = {}\n",
        "        for pid in set(list(d_norm.keys()) + list(b_norm.keys())):\n",
        "            combined[pid] = alpha * d_norm.get(pid, 0.0) + (1 - alpha) * b_norm.get(pid, 0.0)\n",
        "        # sort by combined score\n",
        "        sorted_pids = sorted(combined.keys(), key=lambda p: combined[p], reverse=True)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(combined[pid])))\n",
        "    elif mode == \"hybrid_rrf\":\n",
        "        sorted_pids, score_map = rrf_rank(dense_hits, bm25_hits, k_rrf=RRF_K)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(score_map.get(pid, 0.0))))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieve mode: {mode}\")\n",
        "\n",
        "    # Apply metadata filters if requested (filter by pid only)\n",
        "    if any([min_date, max_date, allowed_sources, exclude_time_sensitive is not None]):\n",
        "        filtered_ids = filter_by_metadata([pid for pid,_,_ in results], min_date, max_date, allowed_sources, exclude_time_sensitive)\n",
        "        results = [(pid, pid2text.get(pid, \"\"), score) for pid,_,score in results if pid in filtered_ids]\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------- Quick sample test (safe) ----------\n",
        "q = eval_queries[0][\"query\"] if 'eval_queries' in globals() and len(eval_queries)>0 else \"Ú©ÙˆÙˆÚˆ-19 Ú©ÛŒ Ø¹Ø§Ù… Ø¹Ù„Ø§Ù…Ø§Øª Ú©ÛŒØ§ ÛÛŒÚºØŸ\"\n",
        "print(\"Sample dense top-5 ids:\", [r[0] for r in dense_retrieve(q, k=5)])\n",
        "print(\"Sample bm25_new top-5 ids:\", [r[0] for r in bm25_new_retrieve(q, k=5)])\n",
        "print(\"Sample hybrid_score top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_score')])\n",
        "print(\"Sample hybrid_rrf top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_rrf')])\n"
      ],
      "metadata": {
        "id": "zv-7V8StuDYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9b: Run after above cell 9. Cell 9 creates B2M5 + Dense hybrid and below\n",
        "# cell evaluates its performance:\n",
        "# Validation diagnostics â€” Recall@1, Recall@5, MRR, Precision@k for retrievers\n",
        "# - Works with any mode supported by your Cell 9 wrapper: 'bm25', 'dense', 'hybrid_interleave', 'hybrid_score', 'hybrid_rrf'\n",
        "# - Calls retrieve(...) and computes retrieval metrics\n",
        "# - Outputs summary metrics and a few examples of misses\n",
        "\n",
        "import time, statistics\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def evaluate_retriever(eval_items, mode=\"hybrid_score\", k=DEFAULT_K,\n",
        "                       recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in tqdm(eval_items, desc=f\"Evaluating {mode} retriever\"):\n",
        "        total += 1\n",
        "        q = item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = retrieve(q, k=k, mode=mode)   # (pid, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [r[0] for r in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# ---------- Run evaluation ----------\n",
        "# Use eval_queries_val if defined, else fall back to eval_queries\n",
        "eval_items = eval_queries_val if 'eval_queries_val' in globals() else eval_queries\n",
        "\n",
        "# Evaluate all retriever modes\n",
        "modes = [\"bm25\", \"dense\", \"hybrid_interleave\", \"hybrid_score\", \"hybrid_rrf\"]\n",
        "results = {}\n",
        "for m in modes:\n",
        "    summary, records = evaluate_retriever(eval_items, mode=m, k=DEFAULT_K)\n",
        "    results[m] = summary\n",
        "    print(f\"\\n{m} retriever evaluation summary:\")\n",
        "    for k,v in summary.items():\n",
        "        print(f\"  {k}: {v:.3f}\" if isinstance(v,float) else f\"  {k}: {v}\")\n",
        "\n",
        "    # Show a few misses\n",
        "    misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "    print(f\"  Total misses: {len(misses)} / {len(records)}. Showing up to 3 misses:\")\n",
        "    for r in misses[:3]:\n",
        "        print(\"   Query:\", r[\"query\"][:80])\n",
        "        print(\"    Positives:\", r[\"positive_ids\"])\n",
        "        print(\"    Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n"
      ],
      "metadata": {
        "id": "64L8M6cduMGX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}