##  Model Evaluation & Error Analysis

**Performance Summary:**
The generator model (Facebook mBART) currently achieves a **BLEU score of 4.56**, which is below the target threshold for usable translations. The **ROUGE-L score of 1.58** indicates a significant lack of structural coherence in the generated responses.


MODEL PERFORMANCE REPORT
ğŸ”¹ BLEU Score:   4.56  (Higher is better, >15 is decent for Urdu)
ğŸ”¹ chrF Score:   23.66  (Best metric for Urdu, aim for >40)
ğŸ”¹ ROUGE-L:      1.58  (Sentence structure match)
ğŸ”¹ METEOR:       20.04  (Synonym/Meaning match)


---  Qualitative Analysis (First 3 Samples) ---

Question: Ú©ÙˆÙˆÚˆ-19 Ú©ÛŒ Ø¹Ø§Ù… Ø¹Ù„Ø§Ù…Ø§Øª Ú©ÛŒØ§ ÛÛŒÚºØŸ
Gold Ans: Ø¹Ø§Ù… Ø¹Ù„Ø§Ù…Ø§Øª Ù…ÛŒÚº Ø¨Ø®Ø§Ø±ØŒ Ú©Ú¾Ø§Ù†Ø³ÛŒ Ø§ÙˆØ± Ø³Ø§Ù†Ø³ Ù„ÛŒÙ†Û’ Ù…ÛŒÚº Ø¯Ø´ÙˆØ§Ø±ÛŒ Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ”
Model Ans: Ú©ÙˆÙˆÚˆ-19 Ú©Û’ Ø¹Ø§Ù… Ø¹Ù„Ø§Ù…Ø§Øª Ù…ÛŒÚº rhinitisØŒ ÙˆØ§Ø¦Ø±Ø³ Ø§ÙˆØ± ÙˆØ§Ø¦Ø±Ø³ Ú©ÛŒ Ø¹Ù„Ø§Ù…Ø§Øª Ø´Ø§Ù…Ù„ ÛÛŒÚºØŒ Ø¬Ù† Ú©Û’ Ù„ÛŒÛ’ rhinitis ÙˆØ§Ø¦Ø±Ø³ Ú©ÛŒ ØªØ´Ø®ÛŒØµ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’Û”
--------------------------------------------------
Question: Ú©ÙˆÙˆÚˆ-19 Ú©ÛŒ ØªØ´Ø®ÛŒØµ Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆÙ† Ø³Ø§ Ù¹ÛŒØ³Ù¹ Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’ØŸ
Gold Ans: ØªØ´Ø®ÛŒØµ Ú©Û’ Ù„ÛŒÛ’ Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± rRT-PCR Ø³ÙˆÛŒØ¨ Ù¹ÛŒØ³Ù¹ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªÛ’ ÛÛŒÚºÛ”
Model Ans: Ú©ÙˆÙˆÚˆ-19 Ú©Û’ ØªØ´Ø®ÛŒØµ Ú©Û’ Ù„ÛŒÛ’ Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± Ù¹ÛŒØ³Ù¹ Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªÛ’ ÛÛŒÚº Ù…Ú¯Ø± ÛØ± Ù…Ø±ÛŒØ¶ Ú©Û’ Ù„ÛŒÛ’ Ø®Ø§Øµ Ù¹ÛŒØ³Ù¹ Ø¶Ø±ÙˆØ±ÛŒ ÛÛŒÚºÛ”
--------------------------------------------------
Question: ÛØ§ØªÚ¾ÙˆÚº Ú©ÛŒ ØµÙØ§Ø¦ÛŒ ÙˆØ¨Ø§ Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ú©ÛŒÙˆÚº Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’ØŸ
Gold Ans: ØµØ§Ø¨Ù† Ø§ÙˆØ± Ù¾Ø§Ù†ÛŒ Ø³Û’ Ú©Ù… Ø§Ø² Ú©Ù… 20 Ø³ÛŒÚ©Ù†Úˆ ØªÚ© ÛØ§ØªÚ¾ Ø¯Ú¾ÙˆÙ†Û’ Ø³Û’ ÙˆØ§Ø¦Ø±Ø³ Ú©Û’ Ù¾Ú¾ÛŒÙ„Ø§Ø¤ Ú©Ø§ Ø®Ø·Ø±Û Ú©Ù… ÛÙˆØªØ§ ÛÛ’Û”
Model Ans: ÙˆØ¨Ø§ Ú©Û’ Ø¯ÙˆØ±Ø§Ù† ÛØ§ØªÚ¾ÙˆÚº Ú©ÛŒ ØµÙØ§Ø¦ÛŒ Ø§ÙˆØ± ØµÙØ§Ø¦ÛŒ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’ ØªØ§Ú©Û Ø¬Ù„Ø¯ÛŒ Ù¾ÛÙ†Ú† Ø³Ú©Û’Û”
--------------------------------------------------

**Primary Issues Identified:**

**Training Loss:** Dropped rapidly from `3.64` to `0.07` (near-perfect memorization).
***Validation Loss:** Diverged significantly, increasing from `3.28` (Epoch 1) to `3.75` (Epoch 4).
***Conclusion:** The model stopped learning useful patterns after **Epoch 1**. The subsequent epochs (2-4) only degraded the model's generalization capabilities.

1.  **Severe Overfitting:**
    * **Observation:** The training loss decreased rapidly to **0.073** (Epoch 4), while the validation loss *increased* from **3.28** (Epoch 1) to **3.74** (Epoch 4).
    * **Analysis:** The model has "memorized" the training data but fails to generalize to unseen data. This is common when fine-tuning large models like mBART on smaller datasets without sufficient regularization.

2.  **Degenerate Repetition (Hallucination):**
    * **Observation:** Qualitative analysis shows the model entering repetition loops (e.g., repeating *"rhinitis virus rhinitis"*).
    * **Analysis:** This "repetition penalty" issue occurs when the decoder gets stuck in a high-probability loop. It suggests the need for `repetition_penalty` in the generation parameters or Early Stopping to prevent the model from degrading into these loops.


Attempted Fixes:

training_args = Seq2SeqTrainingArguments(
    # ... your other args ...
    num_train_epochs=5,           # Keep this, but let EarlyStopping cut it short
    load_best_model_at_end=True,  # CRITICAL: Revert to the best epoch (Epoch 1)
    evaluation_strategy="epoch",
    save_strategy="epoch",
    weight_decay=0.01,            # Helps prevent overfitting
    metric_for_best_model="eval_loss",
    
    # GENERATION ARGUMENTS (Fixes the "rhinitis" loop)
    generation_config=GenerationConfig(
        max_new_tokens=128,
        repetition_penalty=1.2,   # Penalizes repeating words
        no_repeat_ngram_size=3,   # Prevents 3-word phrase repeats
    )
)

Model still provides poor accuracy.

**Conclusion:**
Either switch to a different API mBart is good for translation but it isnt recognizing the propper context.
The model requires regularization (higher weight decay, dropout) and "Early Stopping" to prevent overfitting. We must also adjust the decoding strategy (beam search parameters) to fix the repetition loops.
